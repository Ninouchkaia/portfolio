
./CV/02_Postdoc_CONICET/ANKYRIN_MODULARITY/clans.py
# clans.py
import logging
from pathlib import Path
from typing import Dict

from .config import PFAM_CLANS_FILE, PFAM_FREQUENCIES_FILE, PFAM_CLAN_FREQ_FILE
from .io import ensure_dir

logger = logging.getLogger(__name__)


def load_family_to_clan_mapping(clans_file: Path = PFAM_CLANS_FILE) -> Dict[str, str]:
    """
    Construit un dict PFAM-family → clan à partir de Pfam-A.clans.txt.
    Reste fidèle à ton parsing (colonnes 4 et 2). :contentReference[oaicite:1]{index=1}
    """
    mapping: Dict[str, str] = {}
    with clans_file.open("r") as fh:
        header = next(fh, None)
        for raw in fh:
            cols = raw.rstrip("
").split("	")
            if len(cols) < 4:
                continue
            clan = cols[1]
            family = cols[3]
            if clan != r"\N":
                mapping[family] = clan
    logger.info("Loaded %d PFAM family→clan mappings from %s", len(mapping), clans_file)
    return mapping


def aggregate_pfam_frequencies_by_clan(
    pfam_freq_file: Path = PFAM_FREQUENCIES_FILE,
    clans_file: Path = PFAM_CLANS_FILE,
    output_file: Path = PFAM_CLAN_FREQ_FILE,
) -> Path:
    """
    Reprend le comportement de adapt_frequencies_pfam_to_clans.py :
    - lit FrequenciesPfam_mapped.txt
    - regroupe par clan
    - écrit FrequenciesPfam_mapped_CLANS.txt :contentReference[oaicite:2]{index=2}
    """
    family_to_clan = load_family_to_clan_mapping(clans_file)
    clan_counts: Dict[str, int] = {}

    with pfam_freq_file.open("r") as fh:
        header = next(fh, None)
        for raw in fh:
            cols = raw.rstrip("
").split("	")
            if len(cols) < 2:
                continue
            domain = cols[1]
            try:
                count = int(cols[-1])
            except ValueError:
                logger.warning("Skipping line with non-int count: %s", raw.strip())
                continue

            clan = family_to_clan.get(domain, domain)
            clan_counts[clan] = clan_counts.get(clan, 0) + count

    ensure_dir(output_file.parent)
    with output_file.open("w") as out:
        out.write("Pfam_clan	count_in_swissprot
")
        for clan, total in sorted(clan_counts.items()):
            out.write(f"{clan}	{total}
")

    logger.info("Wrote %d clan counts to %s", len(clan_counts), output_file)
    return output_file


def annotate_domains_with_clans(
    input_file: Path,
    clans_file: Path = PFAM_CLANS_FILE,
    output_file: Path | None = None,
) -> Path:
    """
    Equivalent à rename_domains_to_clans.py :
    ajoute une colonne CLAN à une table contenant un domaine PFAM en première colonne. :contentReference[oaicite:3]{index=3}
    """
    family_to_clan = load_family_to_clan_mapping(clans_file)
    if output_file is None:
        output_file = input_file.with_name(f"{input_file.stem}_with_clans.txt")

    ensure_dir(output_file.parent)

    with input_file.open("r") as fh, output_file.open("w") as out:
        lines = fh.readlines()
        if not lines:
            return output_file

        header_cols = lines[0].rstrip("
").split("	")
        out.write(f"{header_cols[0]}	CLAN	" + "	".join(header_cols[1:]) + "
")

        for raw in lines[1:]:
            cols = raw.rstrip("
").split("	")
            if not cols:
                continue
            domain_name = cols[0]
            clan_name = family_to_clan.get(domain_name, "NULL")
            out.write(f"{domain_name}	{clan_name}	" + "	".join(cols[1:]) + "
")

    logger.info("Annotated %s → %s with clan column", input_file, output_file)
    return output_file

./CV/02_Postdoc_CONICET/ANKYRIN_MODULARITY/config.py
# config.py
from pathlib import Path

# Racine du projet
PROJECT_ROOT = Path(__file__).resolve().parent

# ------------------------
# STRUCTURE DES DONNÉES
# ------------------------
DATA_DIR = PROJECT_ROOT / "data"
RAW_DIR = DATA_DIR / "raw"
INTERMEDIATE_DIR = DATA_DIR / "intermediate"
RESULTS_DIR = DATA_DIR / "results"

# Sous-dossiers PFAM 
PFAM_QUERY_DIR = INTERMEDIATE_DIR / "pfam_queries"
PFAM_HOMOLOG_DIR = INTERMEDIATE_DIR / "pfam_homologs"

# Sous-dossiers de résultats FASTA
SEQUENCES_DIR = RESULTS_DIR / "sequences"
DOMAIN_FASTA_DIR = SEQUENCES_DIR / "domains"
INDEP_FASTA_DIR = SEQUENCES_DIR / "independent"

# ------------------------
# FICHIERS D'ENTRÉE
# ------------------------
PFAM_CLANS_FILE = RAW_DIR / "Pfam-A.clans.txt"
PFAM_FREQUENCIES_FILE = RAW_DIR / "FrequenciesPfam_mapped.txt"

# Multifasta principaux
ANK_FASTA = RAW_DIR / "Ank_uniref50_string_mapped_522.fasta"
BD_FASTA = RAW_DIR / "binding_partners_2038.fasta"

# ------------------------
# FICHIERS DE SORTIE
# ------------------------
PFAM_CLAN_FREQ_FILE = INTERMEDIATE_DIR / "FrequenciesPfam_mapped_CLANS.txt"

# Paramètres par défaut
DEFAULT_MAX_HOMOLOGS = 400
DEFAULT_FILENAME_PATTERN = (
    "pfam_domains_in_{unp}_MaxHomologs_{N}_{kind}.txt"
)

./CV/02_Postdoc_CONICET/ANKYRIN_MODULARITY/conservation.py
# conservation.py
import logging
from pathlib import Path
from collections import Counter, defaultdict
from typing import Dict, List, Tuple

from .config import INTERMEDIATE_DIR
from .io import ensure_dir, read_uniprot_ids_from_fasta

logger = logging.getLogger(__name__)


def _load_domain_occurrences(path: Path) -> List[Tuple[str, int, int]]:
    """
    Lit un fichier PFAM simplifié:
    uniprot_id 	 start 	 end 	 domain_id
    (ce format correspond aux pfam_domains_in_*_query.txt/homolog.txt utilisés en legacy).
    """
    occurrences: List[Tuple[str, int, int]] = []
    with path.open("r") as fh:
        for raw in fh:
            cols = raw.rstrip("
").split("	")
            if len(cols) < 4:
                continue
            unp = cols[0]
            start = int(cols[1])
            end = int(cols[2])
            domain = cols[3]
            occurrences.append((domain, start, end))
    return occurrences


def compute_domain_conservation_for_fasta(
    fasta_path: Path,
    num_homologs: int,
    base_dir: Path,
) -> Tuple[Path, Path]:
    """
    Equivalent structuré à check_domain_conservation_BD_2015.py pour un multifasta donné.
    - lit les uniprot IDs dans fasta_path
    - pour chaque uniprot, lit les fichiers query/homolog
    - calcule la conservation de chaque domaine
    - écrit:
      * conservation_state_...txt
      * domain_conservation_percentages_...txt

    On supppose des chemins du type:
    {basename}_MaxHomologs_MAX{N}_queries_pfam_output/pfam_domains_in_{UNP}_MaxHomologs_{N}_query.txt
    et mêmes patterns pour homologs. :contentReference[oaicite:5]{index=5}
    """
    basename = fasta_path.stem  # ex: binding_partners_2038
    out_state = INTERMEDIATE_DIR / f"conservation_state_{basename}_{num_homologs}.txt"
    out_percent = INTERMEDIATE_DIR / f"domain_conservation_percentages_{basename}_{num_homologs}.txt"

    if out_percent.exists():
        logger.info("Conservation already computed for %s (N=%d)", basename, num_homologs)
        return out_state, out_percent

    ensure_dir(INTERMEDIATE_DIR)

    uniprot_ids = read_uniprot_ids_from_fasta(fasta_path)
    logger.info("Computing conservation for %d proteins in %s (N=%d)", len(uniprot_ids), fasta_path, num_homologs)

    # (domaine → [nb_conservé, nb_non_conservé])
    conservation_counts: Dict[str, List[int]] = defaultdict(lambda: [0, 0])

    with out_state.open("w") as state_out:
        state_out.write("uniprot_id	domain_id	is_conserved
")

        for unp in uniprot_ids:
            query_file = base_dir / f"{basename}_MaxHomologs_MAX{num_homologs}_queries_pfam_output" / \
                f"pfam_domains_in_{unp}_MaxHomologs_{num_homologs}_query.txt"
            homol_file = base_dir / f"{basename}_MaxHomologs_MAX{num_homologs}_homologs_pfam_output" / \
                f"pfam_domains_in_{unp}_MaxHomologs_{num_homologs}_homolog.txt"

            if not query_file.exists() or not homol_file.exists():
                logger.warning("Missing PFAM files for %s: %s or %s", unp, query_file, homol_file)
                continue

            # Collecte des domaines chez le query
            query_domains = _load_domain_occurrences(query_file)
            # Collecte des domaines homologues
            homolog_domains = _load_domain_occurrences(homol_file)
            homolog_counter = Counter([d for d, _, _ in homolog_domains])

            for domain_id, _, _ in query_domains:
                conserved = domain_id in homolog_counter
                state_out.write(f"{unp}	{domain_id}	{int(conserved)}
")
                if conserved:
                    conservation_counts[domain_id][0] += 1
                else:
                    conservation_counts[domain_id][1] += 1

    # Calcul des pourcentages
    with out_percent.open("w") as perc_out:
        perc_out.write("domain_name	conservation_percentage
")
        for domain, (n_conserved, n_not) in sorted(conservation_counts.items()):
            total = n_conserved + n_not
            if total == 0:
                continue
            perc = n_conserved / total
            perc_out.write(f"{domain}	{perc:.6f}
")

    logger.info("Wrote conservation state to %s and percentages to %s", out_state, out_percent)
    return out_state, out_percent

./CV/02_Postdoc_CONICET/ANKYRIN_MODULARITY/elm.py
# elm.py
import logging
from pathlib import Path
from collections import Counter, defaultdict
from typing import Dict, List, Tuple

from .io import ensure_dir

logger = logging.getLogger(__name__)


# ------------------------------------------------
# 1. Lecture des tables ELM (SLiMs) prédites
# ------------------------------------------------
def read_elm_table(path: Path) -> List[Tuple[str, str]]:
    """
    Lit un fichier ELM (SLiM predictions), supposé contenir :
        uniprot_id 	 elm_name
    """
    els: List[Tuple[str, str]] = []
    with path.open("r") as fh:
        header = next(fh, None)
        for raw in fh:
            cols = raw.rstrip("
").split("	")
            if len(cols) < 2:
                continue
            els.append((cols[0], cols[1]))
    logger.info("Read %d ELM predictions from %s", len(els), path)
    return els


# ------------------------------------------------
# 2. Comptage par ELM
# ------------------------------------------------
def count_elms(elm_tuples: List[Tuple[str, str]]) -> Dict[str, int]:
    """
    Compte le nombre d'occurrences de chaque ELM.
    """
    counter = Counter([elm for _, elm in elm_tuples])
    logger.info("Counted %d distinct ELMs", len(counter))
    return dict(counter)


def write_elm_counts(counts: Dict[str, int], output_file: Path) -> Path:
    ensure_dir(output_file.parent)
    with output_file.open("w") as out:
        out.write("elm_name	count
")
        for elm, cnt in sorted(counts.items()):
            out.write(f"{elm}	{cnt}
")
    logger.info("Wrote ELM counts → %s", output_file)
    return output_file


# ------------------------------------------------
# 3. Enrichissement ELM (log(obs/exp))
# ------------------------------------------------
def compute_elm_enrichment(
    elm_counts_subfamily: Dict[str, int],
    elm_counts_background: Dict[str, int],
    num_subfamily_proteins: int,
    num_background_proteins: int,
    output_file: Path,
) -> Path:
    """
    Equivalent de domain enrichment, mais pour les ELMs.
    """
    ensure_dir(output_file.parent)

    with output_file.open("w") as out:
        out.write("elm_name	count_subfamily	count_background	expected	log_obs_exp
")

        for elm, sub_count in sorted(elm_counts_subfamily.items()):
            bg_count = elm_counts_background.get(elm, 0)
            expected = bg_count * num_subfamily_proteins / num_background_proteins

            if expected > 0 and sub_count > 0:
                log_obs_exp = math.log(sub_count / expected)
            else:
                log_obs_exp = 0.0

            out.write(
                f"{elm}	{sub_count}	{bg_count}	{expected:.4f}	{log_obs_exp:.6f}
"
            )

    logger.info("Computed ELM enrichment → %s", output_file)
    return output_file


# ------------------------------------------------
# 4. Co-occurrence ELM ↔ Domain dans interacting pairs
# ------------------------------------------------
def compute_elm_domain_cooccurrences(
    elm_table: List[Tuple[str, str]],
    domain_table: List[Tuple[str, str]],
    output_file: Path,
) -> Path:
    """
    Approche inspirée du draft initial (figure 2) :
    - ELM dans un partenaire
    - domaine dans son interacteur
    - comptage des couples (ELM, domain)

    elm_table: [(unp, elm)]
    domain_table: [(unp, domain)]
    """
    ensure_dir(output_file.parent)

    elms_by_unp = defaultdict(list)
    for unp, elm in elm_table:
        elms_by_unp[unp].append(elm)

    domains_by_unp = defaultdict(list)
    for unp, dom in domain_table:
        domains_by_unp[unp].append(dom)

    pairs_count = Counter()

    all_unp = set(elms_by_unp.keys()) | set(domains_by_unp.keys())

    for unp in all_unp:
        for elm in elms_by_unp.get(unp, []):
            for dom in domains_by_unp.get(unp, []):
                pairs_count[(elm, dom)] += 1

    with output_file.open("w") as out:
        out.write("elm	domain	count
")
        for (elm, dom), cnt in pairs_count.items():
            out.write(f"{elm}	{dom}	{cnt}
")

    logger.info("Computed %d ELM-domain cooccurrences → %s", len(pairs_count), output_file)
    return output_file

./CV/02_Postdoc_CONICET/ANKYRIN_MODULARITY/enrichment.py
# enrichment.py
import logging
import math
from pathlib import Path
from typing import Dict

from .config import INTERMEDIATE_DIR, RESULTS_DIR
from .io import ensure_dir, read_simple_two_column_int_table

logger = logging.getLogger(__name__)


def compute_domain_enrichment_from_counts(
    domain_counts_file: Path,
    pfam_counts_file: Path,
    num_proteins_subfamily: int,
    num_proteins_swissprot: int,
    output_file: Path,
) -> Path:
    """
    Version généralisée du bloc qui écrit Pfam_domains_in_BD_2038.txt :contentReference[oaicite:8]{index=8}

    domain_counts_file : table "domain_name <tab> # in subfamily"
    pfam_counts_file   : table "domain_name <tab> # in Uniprot"
    """
    if output_file.exists():
        logger.info("Enrichment file already exists: %s", output_file)
        return output_file

    ensure_dir(output_file.parent)

    sub_counts = read_simple_two_column_int_table(domain_counts_file)
    pfam_counts = read_simple_two_column_int_table(pfam_counts_file)

    with output_file.open("w") as out:
        out.write("domain_name	count_subfamily	count_uniprot	expected_count	log_obs_exp
")
        for domain_name, sub_count in sorted(sub_counts.items()):
            if domain_name not in pfam_counts:
                logger.debug("Domain %s not found in pfam_counts, skipping", domain_name)
                continue
            unp_count = pfam_counts[domain_name]
            exp_count = unp_count * num_proteins_subfamily / num_proteins_swissprot
            if exp_count == 0:
                continue
            obs_exp = sub_count / exp_count
            log_obs_exp = math.log(obs_exp)
            out.write(
                f"{domain_name}	{sub_count}	{unp_count}	{exp_count:.6f}	{log_obs_exp:.6f}
"
            )

    logger.info("Wrote domain enrichment table to %s", output_file)
    return output_file


def _load_conservation_percentages(path: Path) -> Dict[str, float]:
    """
    Lit domain_conservation_percentages_*.txt.
    """
    d: Dict[str, float] = {}
    with path.open("r") as fh:
        header = next(fh, None)
        for raw in fh:
            cols = raw.rstrip("
").split()
            if len(cols) < 2:
                continue
            try:
                d[cols[0]] = float(cols[1].replace(",", "."))
            except ValueError:
                continue
    return d


def color_enrichment_by_conservation(
    enrichment_zscore_file: Path,
    conservation_percent_file: Path,
    num_homologs: int,
    output_prefix: str,
) -> Path:
    """
    Version factorisée de domain_enrich_conserv_2015.py et _BD_2015.py 

    enrichment_zscore_file :
        - Pfam_domains_in_Ank1234_Zscores.txt
        - ou Pfam_domains_in_BD_2038_Zscores.txt
      colonnes (au moins) : domain_name, ..., log(obs/exp), Zscore

    conservation_percent_file :
        - domain_conservation_percentages_*.txt

    output_prefix :
        - ex: "Ank1234" ou "BD_2038"
    """
    ensure_dir(RESULTS_DIR)
    output_file = RESULTS_DIR / f"Pfam_domains_in_{output_prefix}_MaxHomologs_{num_homologs}_Zscores_color.txt"

    if output_file.exists():
        logger.info("Colored enrichment file already exists: %s", output_file)
        return output_file

    conserv = _load_conservation_percentages(conservation_percent_file)
    if not conserv:
        logger.warning("Empty conservation file: %s", conservation_percent_file)

    avg_conserv = sum(conserv.values()) / len(conserv) if conserv else 0.0
    logger.info(
        "Average domain conservation (N=%d) = %.4f from %s",
        len(conserv),
        avg_conserv,
        conservation_percent_file,
    )

    with enrichment_zscore_file.open("r") as fh, output_file.open("w") as out:
        header = next(fh, None)
        out.write(
            f"domain_name	domain_enrichment_log(obs/exp)	Zscore_enrichment	"
            f"domain_conservation_over_{num_homologs}Homologs	color
"
        )

        for raw in fh:
            cols = raw.rstrip("
").split("	")
            if len(cols) < 6:
                continue
            domain_name = cols[0]
            try:
                domain_enrichment = float(cols[4].replace(",", "."))
                domain_zscore = float(cols[5].replace(",", "."))
            except ValueError:
                continue

            if domain_name not in conserv:
                logger.debug("Domain %s not in conservation dict", domain_name)
                continue

            domain_conservation = conserv[domain_name]
            color = '"red"' if (domain_zscore > 1 and domain_conservation > avg_conserv) else '"black"'
            out.write(
                f"{domain_name}	{domain_enrichment:.6f}	{domain_zscore:.6f}	"
                f"{domain_conservation:.6f}	{color}
"
            )

    logger.info("Wrote colored enrichment table to %s", output_file)
    return output_file

./CV/02_Postdoc_CONICET/ANKYRIN_MODULARITY/io.py
# io.py
import logging
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

from Bio import SeqIO  # type: ignore

logger = logging.getLogger(__name__)


def ensure_dir(path: Path) -> None:
    """Créer un répertoire (et parents) s'il n'existe pas."""
    path.mkdir(parents=True, exist_ok=True)


def read_uniprot_ids_from_fasta(fasta_path: Path) -> List[str]:
    """
    Reproduit le pattern record.id[3:9] pour extraire l'UNP ID
    depuis un multifasta UniProt-style.
    """
    ids: List[str] = []
    with fasta_path.open("r") as handle:
        for record in SeqIO.parse(handle, "fasta"):
            uniprot_id = record.id[3:9]
            ids.append(uniprot_id)
    logger.info("Read %d UniProt IDs from %s", len(ids), fasta_path)
    return ids


def read_pfam_table(path: Path) -> List[Tuple[str, int, int, str]]:
    """
    Lire un fichier PFAM tabulé de type:
    start	end	...	domain_id	...

    Retourne une liste de tuples (start, end, domain_id, full_line_stripped).
    """
    entries: List[Tuple[str, int, int, str]] = []
    with path.open("r") as fh:
        for raw in fh:
            line = raw.rstrip("
").split("	")
            if len(line) < 6:
                continue
            start = int(line[1])
            end = int(line[2])
            domain_id = line[5]
            entries.append((domain_id, start, end, raw.rstrip("
")))
    logger.debug("Read %d PFAM entries from %s", len(entries), path)
    return entries


def read_simple_two_column_int_table(path: Path) -> Dict[str, int]:
    """
    Lire un fichier tabulé simple: key	value
    (pour counts, fréquences, etc.)
    """
    d: Dict[str, int] = {}
    with path.open("r") as fh:
        header = next(fh, None)
        for raw in fh:
            cols = raw.rstrip("
").split("	")
            if len(cols) < 2:
                continue
            key = cols[0]
            try:
                value = int(cols[1])
            except ValueError:
                logger.warning("Skipping line (non-int value): %s", raw.strip())
                continue
            d[key] = value
    logger.info("Read %d entries from %s", len(d), path)
    return d


# --- Extraction FASTA depuis positions domaine/homologue --- #

from Bio.SeqRecord import SeqRecord  # type: ignore
from Bio.Seq import Seq  # type: ignore


# Dans io.py (mise à jour de extract_domain_sequences)

def extract_domain_sequences(
    fasta_path: Path,
    pfam_domain_file: Path,
    output_fasta: Path | None = None,
) -> Path:
    """
    Extraction propre des domaines PFAM.
    Si output_fasta n'est pas donné, on génère un nom propre automatiquement.
    """
    from .config import DOMAIN_FASTA_DIR

    ensure_dir(DOMAIN_FASTA_DIR)

    # Déduire UNIPROT du nom du fichier PFAM
    parts = pfam_domain_file.stem.split("_")
    if len(parts) < 5:
        logger.warning("Nom inattendu pour fichier PFAM: %s", pfam_domain_file)
        uniprot = parts[-1]
    else:
        uniprot = parts[-4]  # conforme à tes scripts

    if output_fasta is None:
        output_fasta = DOMAIN_FASTA_DIR / f"{uniprot}_domains.fasta"

    # Index du FASTA
    fasta_index = SeqIO.to_dict(SeqIO.parse(str(fasta_path), "fasta"))

    entries = read_pfam_table(pfam_domain_file)
    seq_records: List[SeqRecord] = []

    for domain_id, start, end, _ in entries:
        if uniprot not in fasta_index:
            logger.warning("UNIPROT %s absent du FASTA %s", uniprot, fasta_path)
            continue

        subseq = fasta_index[uniprot].seq[start - 1 : end]
        rec_id = f"{uniprot}_{domain_id}_{start}_{end}"
        seq_records.append(SeqRecord(Seq(str(subseq)), id=rec_id, description=""))

    with output_fasta.open("w") as out:
        SeqIO.write(seq_records, out, "fasta")

    logger.info(
        "Extracted %d domain sequences for %s → %s",
        len(seq_records), uniprot, output_fasta,
    )
    return output_fasta



def extract_independent_sequences(
    fasta_path: Path,
    cutoff_start: int,
    cutoff_end: int,
    output_fasta: Path | None = None,
) -> Path:
    """
    Génère un FASTA sans la région [cutoff_start, cutoff_end].
    Nomme automatiquement le fichier dans INDEP_FASTA_DIR.
    """
    from .config import INDEP_FASTA_DIR

    ensure_dir(INDEP_FASTA_DIR)

    fasta_index = SeqIO.to_dict(SeqIO.parse(str(fasta_path), "fasta"))

    seq_records: List[SeqRecord] = []
    for uniprot, rec in fasta_index.items():
        full = rec.seq
        new_seq = full[: cutoff_start - 1] + full[cutoff_end:]

        rec_id = f"{uniprot}_indep_without_{cutoff_start}_{cutoff_end}"
        seq_records.append(SeqRecord(new_seq, id=rec_id, description=""))

    if output_fasta is None:
        output_fasta = INDEP_FASTA_DIR / f"indep_without_{cutoff_start}_{cutoff_end}.fasta"

    with output_fasta.open("w") as fh:
        SeqIO.write(seq_records, fh, "fasta")

    logger.info(
        "Generated independent sequences without [%d:%d] → %s",
        cutoff_start, cutoff_end, output_fasta,
    )
    return output_fasta




def find_pfam_files(uniprot_id: str, num_homologs: int) -> Tuple[Path, Path]:
    """
    Trouve automatiquement les fichiers PFAM query/homolog à partir du pattern :

        pfam_domains_in_{UNIPROT}_MaxHomologs_{N}_{query|homolog}.txt

    dans PFAM_QUERY_DIR et PFAM_HOMOLOG_DIR.

    Retourne (query_file, homolog_file).
    """
    from .config import PFAM_QUERY_DIR, PFAM_HOMOLOG_DIR, DEFAULT_FILENAME_PATTERN

    query_pattern = DEFAULT_FILENAME_PATTERN.format(
        unp=uniprot_id, N=num_homologs, kind="query"
    )
    homolog_pattern = DEFAULT_FILENAME_PATTERN.format(
        unp=uniprot_id, N=num_homologs, kind="homolog"
    )

    query_file = PFAM_QUERY_DIR / query_pattern
    homolog_file = PFAM_HOMOLOG_DIR / homolog_pattern

    if not query_file.exists():
        logger.warning("PFAM query file not found: %s", query_file)
    if not homolog_file.exists():
        logger.warning("PFAM homolog file not found: %s", homolog_file)

    return query_file, homolog_file


def concatenate_pfam_outputs_by_threshold(
    input_dir: Path,
    output_file: Path,
    min_occurrences: int = 1,
) -> Path:
    """
    Concatène tous les fichiers PFAM dans un dossier (ex: pfam_queries/)
    et applique un seuil minimal d'occurrences par domaine.

    Reproduit la logique de concaten_pfam_outputs_BD_2015.py.

    Format attendu des fichiers PFAM :
        uniprot_id 	 start 	 end 	 domain_id

    Sortie : fichier tabulé domaine 	 total_occurrences
    """
    ensure_dir(output_file.parent)

    domain_counts: Dict[str, int] = {}

    for pfam_file in sorted(input_dir.glob("*.txt")):
        with pfam_file.open("r") as fh:
            for raw in fh:
                cols = raw.rstrip("
").split("	")
                if len(cols) < 4:
                    continue
                domain_id = cols[3]
                domain_counts[domain_id] = domain_counts.get(domain_id, 0) + 1

    with output_file.open("w") as out:
        out.write("domain_name	occurrences
")
        for domain, count in sorted(domain_counts.items()):
            if count >= min_occurrences:
                out.write(f"{domain}	{count}
")

    logger.info(
        "Concatenated %d PFAM domain entries → %s (threshold=%d)",
        sum(domain_counts.values()), output_file, min_occurrences
    )
    return output_file

./CV/02_Postdoc_CONICET/ANKYRIN_MODULARITY/pipeline.py
# pipeline.py

import logging
from pathlib import Path

from .config import (
    PROJECT_ROOT,
    RAW_DIR,
    INTERMEDIATE_DIR,
    RESULTS_DIR,
    PFAM_FREQUENCIES_FILE,
    PFAM_CLANS_FILE,
    PFAM_CLAN_FREQ_FILE,
    ANK_FASTA,
    BD_FASTA,
    DEFAULT_MAX_HOMOLOGS,
    PFAM_QUERY_DIR,
    PFAM_HOMOLOG_DIR,
    DOMAIN_FASTA_DIR,
    INDEP_FASTA_DIR,
)

from .io import (
    ensure_dir,
    concatenate_pfam_outputs_by_threshold,
    find_pfam_files,
    extract_domain_sequences,
    extract_independent_sequences,
    read_uniprot_ids_from_fasta,
)

from .clans import aggregate_pfam_frequencies_by_clan
from .conservation import compute_domain_conservation_for_fasta
from .enrichment import color_enrichment_by_conservation

from .elm import (
    read_elm_table,
    count_elms,
    write_elm_counts,
    compute_elm_enrichment,
    compute_elm_domain_cooccurrences,
)

# ---------------------------------------------------------
# ACTIVATION DES ÉTAPES (contrôle manuel)
# ---------------------------------------------------------
RUN_CLAN_AGGREGATION = False
RUN_PFAM_CONCATENATION = False
RUN_DOMAIN_EXTRACTION = False
RUN_INDEP_EXTRACTION = False

RUN_CONSERVATION_ANK = False
RUN_CONSERVATION_BD = False

RUN_ENRICHMENT_ANK = False
RUN_ENRICHMENT_BD = False

RUN_ELM_PROCESSING = False
RUN_ELM_ENRICHMENT = False
RUN_ELM_DOMAIN_ASSOCIATION = False
# ---------------------------------------------------------


def setup_logging() -> None:
    log_file = PROJECT_ROOT / "ankyrin_modularity.log"
    logging.basicConfig(
        filename=str(log_file),
        level=logging.INFO,
        format="%(asctime)s — %(name)s — %(levelname)s — %(message)s",
    )
    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    formatter = logging.Formatter("%(name)s — %(levelname)s — %(message)s")
    console.setFormatter(formatter)
    logging.getLogger().addHandler(console)


def main() -> None:

    setup_logging()
    logger = logging.getLogger(__name__)

    logger.info("=== Starting Ankyrin modularity pipeline ===")

    # Assure la présence des répertoires
    ensure_dir(RAW_DIR)
    ensure_dir(INTERMEDIATE_DIR)
    ensure_dir(RESULTS_DIR)
    ensure_dir(PFAM_QUERY_DIR)
    ensure_dir(PFAM_HOMOLOG_DIR)
    ensure_dir(DOMAIN_FASTA_DIR)
    ensure_dir(INDEP_FASTA_DIR)

    # ---------------------------------------------------------
    # 1) PFAM → CLANS
    # ---------------------------------------------------------
    if RUN_CLAN_AGGREGATION:
        logger.info("Step: PFAM → Clan aggregation")
        aggregate_pfam_frequencies_by_clan(
            pfam_freq_file=PFAM_FREQUENCIES_FILE,
            clans_file=PFAM_CLANS_FILE,
            output_file=PFAM_CLAN_FREQ_FILE,
        )

    # ---------------------------------------------------------
    # 2) PFAM → Concaténation tous fichiers + seuil
    # ---------------------------------------------------------
    if RUN_PFAM_CONCATENATION:
        logger.info("Step: Concatenate PFAM outputs (query files)")
        concatenated = concatenate_pfam_outputs_by_threshold(
            input_dir=PFAM_QUERY_DIR,
            output_file=INTERMEDIATE_DIR / "PFAM_domains_concatenated.txt",
            min_occurrences=1,
        )
        logger.info("Concatenated PFAM file saved to %s", concatenated)

    # ---------------------------------------------------------
    # 3) EXTRACTION DE DOMAINES (FASTA)
    # ---------------------------------------------------------
    if RUN_DOMAIN_EXTRACTION:
        logger.info("Step: Extract domain FASTA for Binding Partners")

        uniprot_list = read_uniprot_ids_from_fasta(BD_FASTA)

        for unp in uniprot_list:
            query_file, homolog_file = find_pfam_files(unp, DEFAULT_MAX_HOMOLOGS)

            if query_file.exists():
                extract_domain_sequences(
                    fasta_path=BD_FASTA,
                    pfam_domain_file=query_file,
                )

    # ---------------------------------------------------------
    # 4) EXTRACTION DE SÉQUENCES INDÉPENDANTES
    # ---------------------------------------------------------
    if RUN_INDEP_EXTRACTION:
        logger.info("Step: Extract independent sequences")
        extract_independent_sequences(
            fasta_path=BD_FASTA,
            cutoff_start=150,
            cutoff_end=350,
        )

    # ---------------------------------------------------------
    # 5) CONSERVATION Ankyrin
    # ---------------------------------------------------------
    if RUN_CONSERVATION_ANK:
        logger.info("Step: Compute conservation for Ankyrin family")
        compute_domain_conservation_for_fasta(
            fasta_path=ANK_FASTA,
            num_homologs=DEFAULT_MAX_HOMOLOGS,
            base_dir=INTERMEDIATE_DIR,
        )

    # ---------------------------------------------------------
    # 6) CONSERVATION Binding Partners
    # ---------------------------------------------------------
    if RUN_CONSERVATION_BD:
        logger.info("Step: Compute conservation for Binding Partners")
        compute_domain_conservation_for_fasta(
            fasta_path=BD_FASTA,
            num_homologs=DEFAULT_MAX_HOMOLOGS,
            base_dir=INTERMEDIATE_DIR,
        )

    # ---------------------------------------------------------
    # 7) ENRICHISSEMENT + COULEUR Ankyrin
    # ---------------------------------------------------------
    if RUN_ENRICHMENT_ANK:
        logger.info("Step: Enrichment + color for Ankyrins")

        conservation_file = (
            INTERMEDIATE_DIR / f"domain_conservation_percentages_{ANK_FASTA.stem}_{DEFAULT_MAX_HOMOLOGS}.txt"
        )
        zscore_file = RAW_DIR / "Pfam_domains_in_Ank1234_Zscores.txt"

        color_enrichment_by_conservation(
            enrichment_zscore_file=zscore_file,
            conservation_percent_file=conservation_file,
            num_homologs=DEFAULT_MAX_HOMOLOGS,
            output_prefix="Ank1234",
        )

    # ---------------------------------------------------------
    # 8) ENRICHISSEMENT + COULEUR Binding Partners
    # ---------------------------------------------------------
    if RUN_ENRICHMENT_BD:
        logger.info("Step: Enrichment + color for Binding Partners")

        conservation_file = (
            INTERMEDIATE_DIR / f"domain_conservation_percentages_{BD_FASTA.stem}_{DEFAULT_MAX_HOMOLOGS}.txt"
        )
        zscore_file = RAW_DIR / "Pfam_domains_in_BD_2038_Zscores.txt"

        color_enrichment_by_conservation(
            enrichment_zscore_file=zscore_file,
            conservation_percent_file=conservation_file,
            num_homologs=DEFAULT_MAX_HOMOLOGS,
            output_prefix="BD_2038",
        )

    # ---------------------------------------------------------
    # 9) SECTION ELM (SLiMs)
    # ---------------------------------------------------------
    if RUN_ELM_PROCESSING:
        logger.info("Step: Counting ELMs")

        elm_file_ank = RAW_DIR / "ELM_predictions_ankyrins.txt"
        elm_file_bd = RAW_DIR / "ELM_predictions_binding_partners.txt"

        ank_elms = read_elm_table(elm_file_ank)
        bd_elms = read_elm_table(elm_file_bd)

        ank_counts = count_elms(ank_elms)
        bd_counts = count_elms(bd_elms)

        write_elm_counts(ank_counts, RESULTS_DIR / "ELM_counts_ankyrins.txt")
        write_elm_counts(bd_counts, RESULTS_DIR / "ELM_counts_binding_partners.txt")

    if RUN_ELM_ENRICHMENT:
        logger.info("Step: ELM enrichment")

        compute_elm_enrichment(
            elm_counts_subfamily=ank_counts,
            elm_counts_background=bd_counts,
            num_subfamily_proteins=len(ank_counts),
            num_background_proteins=len(bd_counts),
            output_file=RESULTS_DIR / "ELM_enrichment_ank_vs_bd.txt",
        )

    if RUN_ELM_DOMAIN_ASSOCIATION:
        logger.info("Step: ELM-domain cooccurrence")

        # Domain table BD format: uniprot	 domain
        domain_table_file = RESULTS_DIR / "domains_binding_partners_for_elm.txt"
        domain_table = []
        with domain_table_file.open() as fh:
            next(fh)
            for raw in fh:
                cols = raw.rstrip("
").split("	")
                if len(cols) >= 2:
                    domain_table.append((cols[0], cols[1]))

        compute_elm_domain_cooccurrences(
            elm_table=bd_elms,
            domain_table=domain_table,
            output_file=RESULTS_DIR / "ELM_domain_cooccurrences.txt",
        )

    # ---------------------------------------------------------
    logger.info("=== Pipeline finished ===")


if __name__ == "__main__":
    main()

./CV/02_Postdoc_CONICET/ANKYRIN_MODULARITY/plots.py


./CV/02_Postdoc_CONICET/paxdb/scripts/analyze_amino_acids.py


./CV/02_Postdoc_CONICET/paxdb/scripts/build_mapping_tables.py


./CV/02_Postdoc_CONICET/paxdb/scripts/coverage_analysis.py


./CV/02_Postdoc_CONICET/paxdb/scripts/extract_ds1_unweighted.py


./CV/02_Postdoc_CONICET/paxdb/scripts/extract_ds2_paxdb_weighted.py


./CV/02_Postdoc_CONICET/paxdb/src/aa_metrics.py
# paxdb/src/aa_metrics.py

from __future__ import annotations

import logging
from pathlib import Path
from typing import Dict, Iterable, Tuple

import numpy as np
import pandas as pd

from .protein import Protein

logger = logging.getLogger(__name__)

STANDARD_AA = list("ACDEFGHIKLMNPQRSTVWY")


def compute_proteome_aa_usage(
    proteins: Iterable[Protein],
    weighted: bool = True,
    allowed_aas: Iterable[str] = STANDARD_AA,
) -> Dict[str, float]:
    """
    Compute proteome-level amino acid relative abundances.

    Parameters
    ----------
    proteins : iterable of Protein
        Proteins with abundance and sequence information.
    weighted : bool
        If True, weight counts by abundance; otherwise each protein
        contributes equally (unweighted composition).
    allowed_aas : iterable of str
        Consider only this set of amino acids (typically 20 standard).

    Returns
    -------
    dict
        Mapping amino-acid -> relative frequency in proteome.
        Frequencies sum to 1.0 over allowed_aas (if any non-zero counts).
    """
    allowed = set(allowed_aas)
    total_counts = {aa: 0.0 for aa in allowed}

    for p in proteins:
        counts = p.weighted_amino_acid_counts() if weighted else p.amino_acid_counts()
        for aa, c in counts.items():
            if aa in allowed:
                total_counts[aa] += float(c)

    total = float(sum(total_counts.values())) or 1.0
    freqs = {aa: c / total for aa, c in total_counts.items()}
    logger.debug(
        "Computed %sweighted AA usage: %s",
        "" if weighted else "un",
        freqs,
    )
    return freqs


def load_amino_acid_costs(path: Path) -> Dict[str, float]:
    """
    Load amino acid synthesis costs (e.g. ATP/time units).

    Parameters
    ----------
    path : Path
        TSV file with columns: 'aa', 'cost_atp'.

    Returns
    -------
    dict
        Mapping single-letter AA -> cost.
    """
    df = pd.read_csv(path, sep="	", comment="#", dtype={"aa": str})
    if "aa" not in df.columns or "cost_atp" not in df.columns:
        raise ValueError(
            f"Cost file {path} must contain columns 'aa' and 'cost_atp'."
        )
    df["cost_atp"] = pd.to_numeric(df["cost_atp"], errors="coerce")
    df = df.dropna(subset=["cost_atp"])
    costs = dict(zip(df["aa"].str.upper(), df["cost_atp"].astype(float)))
    logger.info("Loaded amino acid costs for %d residues from %s", len(costs), path)
    return costs


def aa_vector_from_freqs(
    freqs: Dict[str, float],
    ordered_aas: Iterable[str] = STANDARD_AA,
) -> np.ndarray:
    """
    Convert AA frequency dict into a numeric vector in fixed order.
    """
    aa_list = list(ordered_aas)
    return np.array([float(freqs.get(aa, 0.0)) for aa in aa_list], dtype=float)


def aa_vector_from_costs(
    costs: Dict[str, float],
    ordered_aas: Iterable[str] = STANDARD_AA,
) -> np.ndarray:
    """
    Convert AA cost dict into a numeric vector in fixed order.
    """
    aa_list = list(ordered_aas)
    return np.array([float(costs.get(aa, np.nan)) for aa in aa_list], dtype=float)

./CV/02_Postdoc_CONICET/paxdb/src/abundance_loader.py
# paxdb/src/abundance_loader.py

from __future__ import annotations

import logging
from pathlib import Path
from typing import Optional

import pandas as pd

logger = logging.getLogger(__name__)


def load_abundance_table(
    path: Path,
    id_col: str = "protein_id",
    abundance_col: str = "abundance",
) -> pd.DataFrame:
    """
    Load a PaxDB-like protein abundance table.

    Parameters
    ----------
    path : Path
        TSV/CSV file with protein IDs and abundance values.
    id_col : str
        Name of the column containing protein IDs.
    abundance_col : str
        Name of the column containing abundance scores.

    Returns
    -------
    DataFrame
        Columns: [id_col, abundance_col], filtered to non-null abundances.
    """
    logger.info("Loading abundance table from %s", path)
    df = pd.read_csv(path, sep="	", comment="#", dtype=str)
    if id_col not in df.columns or abundance_col not in df.columns:
        raise ValueError(
            f"Required columns '{id_col}' and '{abundance_col}' not found in {path}"
        )

    df = df[[id_col, abundance_col]].copy()
    df[abundance_col] = pd.to_numeric(df[abundance_col], errors="coerce")
    df = df.dropna(subset=[abundance_col])
    df = df[df[abundance_col] > 0]

    logger.info(
        "Loaded %d proteins with positive abundance from %s", len(df), path.name
    )
    return df


def load_mapping_table(
    path: Optional[Path],
    string_col: str = "string_id",
    uniprot_col: str = "uniprot_id",
) -> Optional[pd.DataFrame]:
    """
    Load an optional STRING→UniProt mapping table.

    Parameters
    ----------
    path : Path or None
        If None or empty, returns None.
    string_col : str
        Column name with STRING / PaxDB protein ids.
    uniprot_col : str
        Column name with UniProt ids.

    Returns
    -------
    DataFrame or None
        If path is provided: DataFrame with [string_col, uniprot_col].
        Otherwise: None.
    """
    if path is None:
        logger.info("No mapping table provided.")
        return None

    if not path.exists():
        raise FileNotFoundError(f"Mapping file not found: {path}")

    logger.info("Loading STRING→UniProt mapping from %s", path)
    df = pd.read_csv(path, sep="	", comment="#", dtype=str)
    if string_col not in df.columns or uniprot_col not in df.columns:
        raise ValueError(
            f"Required columns '{string_col}' and '{uniprot_col}' not found in {path}"
        )

    df = df[[string_col, uniprot_col]].dropna()
    logger.info("Loaded %d mapping rows from %s", len(df), path.name)
    return df


def map_abundances_to_fasta_ids(
    abund_df: pd.DataFrame,
    fasta_ids: set[str],
    mapping_df: Optional[pd.DataFrame] = None,
    abundance_id_col: str = "protein_id",
    abundance_col: str = "abundance",
    string_col: str = "string_id",
    uniprot_col: str = "uniprot_id",
) -> pd.DataFrame:
    """
    Map abundance entries to FASTA sequence identifiers.

    Two cases:
    1. No mapping_df: we assume abundance_df[abundance_id_col] already matches
       FASTA IDs directly; we filter on intersection.
    2. mapping_df provided: we join on STRING/PaxDB IDs and map to UniProt IDs.

    Parameters
    ----------
    abund_df : DataFrame
        Abundance table.
    fasta_ids : set of str
        Identifiers present in the FASTA proteome (seq_id from fasta_parser).
    mapping_df : DataFrame, optional
        Mapping between STRING/PaxDB ids and UniProt ids.
    abundance_id_col : str
    abundance_col : str
    string_col : str
    uniprot_col : str

    Returns
    -------
    DataFrame
        Columns: ['seq_id', 'abundance']
    """
    if mapping_df is None:
        logger.info(
            "Mapping abundances directly to FASTA IDs (no external mapping table)."
        )
        df = abund_df.rename(columns={abundance_id_col: "seq_id"})[["seq_id", abundance_col]]
        df = df[df["seq_id"].isin(fasta_ids)].copy()
        df = df.rename(columns={abundance_col: "abundance"})
        logger.info(
            "Mapped %d abundance entries to %d FASTA ids (direct matching).",
            len(df),
            len(fasta_ids),
        )
        return df

    logger.info("Mapping abundances via STRING→UniProt mapping table.")
    merged = abund_df.merge(
        mapping_df,
        left_on=abundance_id_col,
        right_on=string_col,
        how="inner",
    )

    merged = merged.rename(columns={uniprot_col: "seq_id"})
    merged = merged[["seq_id", abundance_col]].copy()
    merged = merged[merged["seq_id"].isin(fasta_ids)]
    merged = merged.rename(columns={abundance_col: "abundance"})

    logger.info(
        "Mapped %d abundance entries to %d FASTA ids (via mapping table).",
        len(merged),
        len(fasta_ids),
    )
    return merged

./CV/02_Postdoc_CONICET/paxdb/src/analyze_amino_acids.py
# paxdb/scripts/analyze_amino_acids.py

from __future__ import annotations

"""
Main entry point for the PaxDB amino acid usage pipeline.

Typical usage:

    python scripts/analyze_amino_acids.py \
        --species-metadata data/metadata/species.tsv \
        --aa-costs data/metadata/amino_acid_costs.tsv \
        --outdir results

This script roughly corresponds to the workflow that used to be
spread across multiple files like:
- amino_acid_count*.py
- newdef_protein*.py
- aa_relationship.py
- protein_relationship.py
but in a cleaner, modular form.
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import List

import pandas as pd

# Make src/ importable when running this script directly
THIS_DIR = Path(__file__).resolve().parent
SRC_DIR = THIS_DIR.parent / "src"
sys.path.insert(0, str(SRC_DIR))

from utils import setup_logging, resolve_path  # type: ignore
from fasta_parser import load_fasta_as_dict  # type: ignore
from abundance_loader import (  # type: ignore
    load_abundance_table,
    load_mapping_table,
    map_abundances_to_fasta_ids,
)
from protein import Protein  # type: ignore
from aa_metrics import (  # type: ignore
    compute_proteome_aa_usage,
    load_amino_acid_costs,
    STANDARD_AA,
)
from relationships import (  # type: ignore
    correlate_usage_with_cost,
    results_to_dataframe,
)

logger = logging.getLogger(__name__)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Analyze amino acid usage under metabolic constraints (PaxDB)."
    )
    parser.add_argument(
        "--species-metadata",
        type=str,
        required=True,
        help="TSV file describing species and paths to FASTA/abundance/mapping.",
    )
    parser.add_argument(
        "--aa-costs",
        type=str,
        required=True,
        help="TSV file with amino acid costs (columns: aa, cost_atp).",
    )
    parser.add_argument(
        "--outdir",
        type=str,
        default="results",
        help="Output directory for results (default: results).",
    )
    parser.add_argument(
        "--logdir",
        type=str,
        default="results/logs",
        help="Directory for log file (default: results/logs).",
    )
    return parser.parse_args()


def load_species_metadata(path: Path) -> pd.DataFrame:
    """
    Load species metadata describing where to find inputs.

    Expected columns:
    - species_id
    - fasta_path          (relative to this file's directory)
    - abundance_path
    - mapping_path        (can be empty)
    """
    df = pd.read_csv(path, sep="	", comment="#", dtype=str)
    required = ["species_id", "fasta_path", "abundance_path", "mapping_path"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"Missing columns in species metadata: {missing}")
    return df


def build_protein_objects_for_species(
    species_id: str,
    fasta_path: Path,
    abundance_path: Path,
    mapping_path: Path | None,
) -> List[Protein]:
    """
    For one species, build Protein objects with sequence and abundance.

    Steps:
    1. Load FASTA (sequence_id → sequence).
    2. Load abundance table.
    3. Optional: load mapping table and map to FASTA ids.
    4. Build Protein instances for each mapped entry.
    """
    logger.info("=== Species %s ===", species_id)
    logger.info("FASTA: %s", fasta_path)
    logger.info("Abundance: %s", abundance_path)
    if mapping_path is not None:
        logger.info("Mapping: %s", mapping_path)

    seq_dict = load_fasta_as_dict(fasta_path)
    logger.info("Loaded %d protein sequences from FASTA.", len(seq_dict))

    abund_df = load_abundance_table(abundance_path)

    mapping_df = (
        load_mapping_table(mapping_path) if mapping_path is not None else None
    )

    mapped_abund = map_abundances_to_fasta_ids(
        abund_df=abund_df,
        fasta_ids=set(seq_dict.keys()),
        mapping_df=mapping_df,
    )

    proteins: List[Protein] = []
    for seq_id, abundance in zip(mapped_abund["seq_id"], mapped_abund["abundance"]):
        seq = seq_dict.get(seq_id)
        if not seq:
            continue
        proteins.append(Protein(seq_id=seq_id, sequence=seq, abundance=float(abundance)))

    logger.info("Built %d Protein objects for species %s", len(proteins), species_id)
    return proteins


def main() -> None:
    args = parse_args()

    outdir = Path(args.outdir).resolve()
    logdir = Path(args.logdir).resolve()
    outdir.mkdir(parents=True, exist_ok=True)

    setup_logging(logdir)
    logger.info("Starting PaxDB amino acid analysis pipeline.")
    logger.info("Output directory: %s", outdir)

    species_meta_path = Path(args.species_metadata).resolve()
    aa_costs_path = Path(args.aa_costs).resolve()

    species_df = load_species_metadata(species_meta_path)
    aa_costs = load_amino_acid_costs(aa_costs_path)

    # DataFrames to accumulate AA usage across species
    usage_unweighted_rows = []
    usage_weighted_rows = []
    relationship_results = []

    meta_base = species_meta_path.parent

    for _, row in species_df.iterrows():
        species_id = row["species_id"]

        fasta_path = resolve_path(meta_base, row["fasta_path"])
        abundance_path = resolve_path(meta_base, row["abundance_path"])
        mapping_path = row.get("mapping_path") or ""
        mapping_path_resolved = (
            resolve_path(meta_base, mapping_path) if mapping_path.strip() else None
        )

        proteins = build_protein_objects_for_species(
            species_id=species_id,
            fasta_path=fasta_path,
            abundance_path=abundance_path,
            mapping_path=mapping_path_resolved,
        )

        if not proteins:
            logger.warning(
                "No Protein objects generated for species %s; skipping.", species_id
            )
            continue

        # Amino acid usage (unweighted and abundance-weighted)
        freqs_unweighted = compute_proteome_aa_usage(
            proteins, weighted=False, allowed_aas=STANDARD_AA
        )
        freqs_weighted = compute_proteome_aa_usage(
            proteins, weighted=True, allowed_aas=STANDARD_AA
        )

        row_unweighted = {"species_id": species_id}
        row_unweighted.update(freqs_unweighted)
        usage_unweighted_rows.append(row_unweighted)

        row_weighted = {"species_id": species_id}
        row_weighted.update(freqs_weighted)
        usage_weighted_rows.append(row_weighted)

        # Relationship between usage and cost
        rel_result = correlate_usage_with_cost(
            species_id=species_id,
            freqs=freqs_weighted,
            costs=aa_costs,
        )
        relationship_results.append(rel_result)

    # Save tables
    if usage_unweighted_rows:
        df_unw = pd.DataFrame(usage_unweighted_rows).set_index("species_id")
        df_unw.to_csv(outdir / "amino_acid_usage_unweighted.tsv", sep="	")
        logger.info(
            "Saved unweighted AA usage table for %d species.",
            df_unw.shape[0],
        )

    if usage_weighted_rows:
        df_w = pd.DataFrame(usage_weighted_rows).set_index("species_id")
        df_w.to_csv(outdir / "amino_acid_usage_weighted.tsv", sep="	")
        logger.info(
            "Saved abundance-weighted AA usage table for %d species.",
            df_w.shape[0],
        )

    if relationship_results:
        df_rel = results_to_dataframe(relationship_results).set_index("species_id")
        df_rel.to_csv(outdir / "amino_acid_cost_correlations.tsv", sep="	")
        logger.info(
            "Saved amino-acid usage vs cost correlation table for %d species.",
            df_rel.shape[0],
        )

    logger.info("Pipeline finished.")


if __name__ == "__main__":
    main()

./CV/02_Postdoc_CONICET/paxdb/src/fasta_parser.py
# paxdb/src/fasta_parser.py

from __future__ import annotations

from pathlib import Path
from typing import Dict, Iterator, Tuple


def parse_fasta(fasta_path: Path) -> Iterator[Tuple[str, str]]:
    """
    Simple FASTA parser that yields (sequence_id, sequence) pairs.

    This avoids external dependencies (no Biopython needed) and is
    sufficient for proteome-scale amino acid counting.

    The sequence identifier is taken as the first token after '>'.

    Parameters
    ----------
    fasta_path : Path
        Path to the FASTA file.

    Yields
    ------
    (seq_id, sequence) : (str, str)
        Sequence identifier and sequence string (no whitespace).
    """
    with fasta_path.open("r", encoding="utf-8") as handle:
        seq_id = None
        chunks = []

        for line in handle:
            line = line.strip()
            if not line:
                continue
            if line.startswith(">"):
                # Finish previous sequence
                if seq_id is not None:
                    yield seq_id, "".join(chunks)
                # Start new sequence
                header = line[1:]
                seq_id = header.split()[0]
                chunks = []
            else:
                chunks.append(line)

        # Last sequence
        if seq_id is not None:
            yield seq_id, "".join(chunks)


def load_fasta_as_dict(fasta_path: Path) -> Dict[str, str]:
    """
    Load a FASTA file into a simple dict {seq_id: sequence}.

    Parameters
    ----------
    fasta_path : Path

    Returns
    -------
    dict
        Mapping from sequence ID to protein sequence.
    """
    return {seq_id: seq for seq_id, seq in parse_fasta(fasta_path)}

./CV/02_Postdoc_CONICET/paxdb/src/protein.py
# paxdb/src/protein.py

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict
from collections import Counter


@dataclass
class Protein:
    """
    Representation of a protein with sequence and abundance.

    This is a minimal, explicit version of what the original
    newdef_protein* scripts were doing.

    Attributes
    ----------
    seq_id : str
        Sequence identifier (e.g. UniProt id).
    sequence : str
        Amino acid sequence (one-letter codes).
    abundance : float
        Relative abundance (e.g. PaxDB score). Defaults to 1.0 for
        unweighted counts.
    """

    seq_id: str
    sequence: str
    abundance: float = 1.0

    def length(self) -> int:
        """Return the sequence length."""
        return len(self.sequence)

    def amino_acid_counts(self) -> Dict[str, int]:
        """
        Return a raw count of each amino acid in this protein.

        Ambiguous residues (e.g. B, Z, X) are counted separately and can
        optionally be discarded in downstream analyses.
        """
        return dict(Counter(self.sequence))

    def amino_acid_frequencies(self) -> Dict[str, float]:
        """
        Return fractional composition (per protein).

        Frequencies sum to 1.0 over all characters present in the sequence.
        """
        counts = self.amino_acid_counts()
        total = float(sum(counts.values())) or 1.0
        return {aa: c / total for aa, c in counts.items()}

    def weighted_amino_acid_counts(self) -> Dict[str, float]:
        """
        Return abundance-weighted amino acid counts.

        Each raw count is multiplied by the protein abundance.
        This is the key link between PaxDB abundances and proteome-level
        amino acid usage.
        """
        counts = self.amino_acid_counts()
        return {aa: c * self.abundance for aa, c in counts.items()}

./CV/02_Postdoc_CONICET/paxdb/src/relationships.py
# paxdb/src/relationships.py

from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import Dict, Iterable, List, Tuple

import numpy as np
import pandas as pd
from scipy.stats import pearsonr

from .aa_metrics import (
    STANDARD_AA,
    aa_vector_from_costs,
    aa_vector_from_freqs,
)

logger = logging.getLogger(__name__)


@dataclass
class AARelationshipResult:
    """
    Container for the relationship between amino-acid usage
    and amino-acid cost for a given species.
    """

    species_id: str
    r: float
    pvalue: float
    used_aas: List[str]

    def as_dict(self) -> Dict[str, object]:
        return {
            "species_id": self.species_id,
            "pearson_r": self.r,
            "pvalue": self.pvalue,
            "n_amino_acids": len(self.used_aas),
            "amino_acids": "".join(self.used_aas),
        }


def correlate_usage_with_cost(
    species_id: str,
    freqs: Dict[str, float],
    costs: Dict[str, float],
    amino_acids: Iterable[str] = STANDARD_AA,
    drop_nan_costs: bool = True,
) -> AARelationshipResult:
    """
    Compute Pearson correlation between amino-acid usage and cost.

    This corresponds conceptually to parts of the MBE paper where
    proteome-level frequencies are compared to energetic costs.

    Parameters
    ----------
    species_id : str
        Species identifier for reporting.
    freqs : dict
        AA -> relative frequency.
    costs : dict
        AA -> energetic cost (e.g. ATP units).
    amino_acids : iterable of str
        Amino acids to consider (default: 20 standard).
    drop_nan_costs : bool
        If True, ignore amino acids that have missing cost values.

    Returns
    -------
    AARelationshipResult
        Pearson r, p-value, and which AAs were used.
    """
    aa_list = list(amino_acids)

    x = aa_vector_from_freqs(freqs, aa_list)
    y = aa_vector_from_costs(costs, aa_list)

    if drop_nan_costs:
        mask = ~np.isnan(y)
        x = x[mask]
        y = y[mask]
        used_aas = [aa for aa, keep in zip(aa_list, mask) if keep]
    else:
        used_aas = aa_list

    if x.size < 3:
        raise ValueError("Not enough amino acids with defined cost for correlation.")

    r, p = pearsonr(x, y)
    logger.info(
        "Species %s: Pearson r(usage, cost) = %.3f (p=%.2e) over %d amino acids",
        species_id,
        r,
        p,
        len(used_aas),
    )
    return AARelationshipResult(species_id, float(r), float(p), used_aas)


def results_to_dataframe(results: Iterable[AARelationshipResult]) -> pd.DataFrame:
    """
    Convert a list of AARelationshipResult to a tidy pandas DataFrame.
    """
    rows = [res.as_dict() for res in results]
    return pd.DataFrame(rows)

./CV/02_Postdoc_CONICET/paxdb/src/utils.py
# paxdb/src/utils.py

from __future__ import annotations

import logging
from pathlib import Path
from typing import Optional


def setup_logging(log_dir: Path, verbose: bool = True) -> None:
    """
    Configure a basic logging setup (both console and file).

    Parameters
    ----------
    log_dir : Path
        Directory where the log file will be written.
    verbose : bool
        If True, log INFO to console; otherwise only WARNING+.
    """
    log_dir.mkdir(parents=True, exist_ok=True)
    log_file = log_dir / "paxdb_analysis.log"

    # Root logger
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)

    # Clear existing handlers (useful if re-running in the same process)
    if logger.handlers:
        for h in list(logger.handlers):
            logger.removeHandler(h)

    # File handler (always DEBUG)
    fh = logging.FileHandler(str(log_file), mode="w", encoding="utf-8")
    fh.setLevel(logging.DEBUG)
    fh_formatter = logging.Formatter(
        fmt="%(asctime)s [%(levelname)s] %(name)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    fh.setFormatter(fh_formatter)
    logger.addHandler(fh)

    # Console handler
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO if verbose else logging.WARNING)
    ch_formatter = logging.Formatter("%(levelname)s - %(message)s")
    ch.setFormatter(ch_formatter)
    logger.addHandler(ch)

    logging.getLogger(__name__).info("Logging initialized. Log file: %s", log_file)


def resolve_path(base: Path, relative: str) -> Path:
    """
    Resolve a path relative to some base directory.

    This is helpful when the species metadata gives paths relative
    to its own location.

    Parameters
    ----------
    base : Path
        Base directory (e.g. the directory containing species.tsv).
    relative : str
        Relative path string from that base.

    Returns
    -------
    Path
        Absolute path.
    """
    p = Path(relative)
    if not p.is_absolute():
        p = base / p
    return p.resolve()

./CV/02_Postdoc_CONICET/paxdb/src/__init__.py


./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/advanced_analysis/plots_advanced.py
# abm_pipeline/advanced_analysis/plots_advanced.py

from __future__ import annotations
from pathlib import Path
from typing import List, Optional

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

from abm_pipeline.advanced_analysis.utils import (
    load_pareto_dataframe,
    ensure_dir,
)
from abm_pipeline.config import ADVANCED_RESULTS_DIR
from abm_pipeline.parameter_exploration.utils import logger


def make_violinplot(
    df: pd.DataFrame,
    param: str,
    save_dir: Optional[str] = None,
    title: Optional[str] = None,
):
    """
    Produit le violin plot d’un paramètre.
    """
    if save_dir is None:
        save_path = ensure_dir(ADVANCED_RESULTS_DIR / "violin")
    else:
        save_path = ensure_dir(save_dir)

    fig, ax = plt.subplots(figsize=(6, 5))

    sns.violinplot(
        data=df,
        x=param,
        palette="Set2",  
        ax=ax,
    )

    if title:
        ax.set_title(title)

    out_path = save_path / f"violin_{param}.png"
    fig.savefig(out_path, dpi=300, bbox_inches="tight")
    logger.info(f"Saved violin plot: {out_path}")


def make_violinplots_all_parameters(
    pareto_file: str,
    save_dir: Optional[str] = None,
    exclude_cols: Optional[List[str]] = None,
):
    """
    Produit un violin plot pour chaque paramètre du fichier Pareto.
    Equivalent de parameter_violin_plots.py + all_pareto_df_to_violin.py
    """
    df = load_pareto_dataframe(pareto_file)

    if exclude_cols is None:
        # À adapter selon ton fichier Pareto exact
        exclude_cols = [
            "delta_fitness_via",
            "delta_fitness_conc",
        ]

    params = [col for col in df.columns if col not in exclude_cols]

    for param in params:
        make_violinplot(df, param, save_dir=save_dir, title=f"Distribution of {param}")




from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA


def run_pca_analysis(
    pareto_file: str,
    save_dir: Optional[str] = None,
):
    """
    Effectue une PCA sur les paramètres optimisés.
    Equivalent de pca_analysis.py
    (Seulement la partie ‘projection 2D’ + scree plot.)
    """
    df = load_pareto_dataframe(pareto_file)

    if save_dir is None:
        save_path = ensure_dir(ADVANCED_RESULTS_DIR / "pca")
    else:
        save_path = ensure_dir(save_dir)

    # On enlève fitness + éventuelles col. non-param
    df_params = df.drop(columns=["delta_fitness_via", "delta_fitness_conc"], errors="ignore")

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df_params)

    pca = PCA(n_components=2)
    comps = pca.fit_transform(X_scaled)

    fig, ax = plt.subplots(figsize=(6, 6))
    ax.scatter(comps[:, 0], comps[:, 1], s=30, alpha=0.7)
    ax.set_xlabel("PC1")
    ax.set_ylabel("PC2")
    ax.set_title("PCA of Optimized Parameters")

    fig.savefig(save_path / "pca_scatter.png", dpi=300, bbox_inches="tight")

    # Scree plot
    fig2, ax2 = plt.subplots(figsize=(6, 4))
    ax2.plot(pca.explained_variance_ratio_, marker="o")
    ax2.set_title("Explained Variance Ratio")
    ax2.set_xlabel("Principal Component")
    ax2.set_ylabel("Variance Ratio")

    fig2.savefig(save_path / "pca_scree.png", dpi=300, bbox_inches="tight")

    logger.info(f"PCA results saved to {save_path}")

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/advanced_analysis/stats_advanced.py
# abm_pipeline/advanced_analysis/stats_advanced.py

from __future__ import annotations
import pandas as pd
from pathlib import Path
from scipy.stats import mannwhitneyu

from abm_pipeline.advanced_analysis.utils import (
    load_pareto_dataframe,
    ensure_dir,
)
from abm_pipeline.config import ADVANCED_RESULTS_DIR
from abm_pipeline.parameter_exploration.utils import logger

def run_parameter_stats_tests(
    pareto_file_1: str,
    pareto_file_2: str,
    save_dir: str | None = None,
    exclude_cols=None,
):
    """
    Test statistique entre deux jeux de paramètres optimisés
    (ex: classe 1 vs classe 2)
    Equivalent de stat_test_paretos.py.

    Sauvegarde résultats dans TSV.
    """
    if save_dir is None:
        save_path = ensure_dir(ADVANCED_RESULTS_DIR / "stats")
    else:
        save_path = ensure_dir(save_dir)

    df1 = load_pareto_dataframe(pareto_file_1)
    df2 = load_pareto_dataframe(pareto_file_2)

    if exclude_cols is None:
        exclude_cols = ["delta_fitness_via", "delta_fitness_conc"]

    params = [c for c in df1.columns if c not in exclude_cols]

    results = []
    for param in params:
        stat, pval = mannwhitneyu(df1[param], df2[param], alternative="two-sided")
        results.append([param, stat, pval])

    out_df = pd.DataFrame(results, columns=["parameter", "MannWhitneyU", "pvalue"])
    out_df.to_csv(save_path / "stats_comparison.tsv", sep="	", index=False)

    logger.info(f"Statistical comparison written: {save_path / 'stats_comparison.tsv'}")

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/advanced_analysis/utils.py
# abm_pipeline/advanced_analysis/utils.py

from pathlib import Path
import pandas as pd


def load_pareto_dataframe(file_path: str) -> pd.DataFrame:
    """Charge un fichier Pareto_ABM_2D_patient.txt ou TSV."""
    path = Path(file_path)
    if path.suffix == ".txt":
        return pd.read_csv(path)
    return pd.read_csv(path, sep="	")


def ensure_dir(path: str | Path):
    path = Path(path)
    path.mkdir(parents=True, exist_ok=True)
    return path

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/advanced_analysis/__init__.py
from .plots_advanced import (
    make_violinplot,
    make_violinplots_all_parameters,
    run_pca_analysis,
)
from .stats_advanced import (
    run_parameter_stats_tests,
)

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/cli.py
# abm_pipeline/cli.py

import typer

from abm_pipeline.parameter_exploration.initial_ranges.aggregate_data import (
    aggregate_population_files,
)
from abm_pipeline.parameter_exploration.nsga2_analysis import (
    build_pareto_front,
    extract_best_param_sets_to_file,
    make_best_sets_all_patients,
    export_patient_data_for_git,
)
from abm_pipeline.parameter_exploration.instantiate_models import (
    make_behavior_space_file_for_patient,
)

app = typer.Typer(help="ABM parameter exploration pipeline")


@app.command()
def aggregate(
    rootdir: str = typer.Argument(..., help="Dossier contenant populationX.csv"),
    output_file: str = typer.Argument(..., help="Fichier de sortie agrégé"),
):
    """Concatène les fichiers populationX.csv (ancienne aggregateData.py)."""
    aggregate_population_files(rootdir, output_file)


@app.command()
def pareto(
    input_file: str = typer.Argument(..., help="CSV issu d'OpenMOLE"),
    output_prefix: str = typer.Argument(
        ..., help="Préfixe pour pareto_*.png et pareto_*.txt"
    ),
):
    """Construit le Pareto front + figure (copy_for_git_paretoFrontGenericStochastic)."""
    build_pareto_front(input_file, output_prefix)


@app.command()
def bestsets(
    pareto_file: str = typer.Argument(..., help="Fichier Pareto .csv ou .txt"),
    output_tsv: str = typer.Argument(
        ..., help="Fichier TSV best_param_sets_*.tsv"
    ),
):
    """Extrait best_via, knee_point, best_conc (extract_param_sets_from_pareto)."""
    extract_best_param_sets_to_file(pareto_file, output_tsv)


@app.command()
def bestsets_all(
    set_name: str = typer.Argument(
        ...,
        help="best_via_set | knee_point_set | best_conc_set",
    ),
    output_file: str = typer.Argument(..., help="TSV de sortie"),
):
    """Concatène les sets pour tous les patients."""
    make_best_sets_all_patients(set_name, output_file)


@app.command()
def export_git(
    output_root: str = typer.Argument(
        "patient_data_for_git", help="Dossier de sortie"
    ),
):
    """Prépare les fichiers pareto_* pour dépôt Git (make_files_for_git.py)."""
    export_patient_data_for_git(output_root)


@app.command()
def make_behaviorspace(
    best_sets_tsv: str = typer.Argument(
        ..., help="TSV best_param_sets_ABM_2D_patient.tsv"
    ),
    output_file: str = typer.Argument(
        ..., help="Fichier de paramètres pour NetLogo"
    ),
    patient_dict: str = typer.Option(
        "patient_dict.txt", help="Fichier patient_dict.txt"
    ),
):
    """Génère le fichier de paramètres NetLogo pour un patient."""
    make_behavior_space_file_for_patient(best_sets_tsv, output_file, patient_dict)


# dans abm_pipeline/cli.py (suite)

from abm_pipeline.parameter_exploration.shell_commands import (
    generate_patient_command_scripts,
    generate_kneepoint_scripts,
    generate_averaged_class_scripts,
    generate_sensitivity_shell_scripts,
)
from abm_pipeline.parameter_exploration.instantiate_models.xml_behavior_space import (
    make_sensitivity_experiment_xml,
)


@app.command()
def patient_shell(
    base_dir: str = typer.Argument(".", help="Dossier où écrire les scripts .sh"),
):
    """Génère patient_command_{PATIENT}.sh pour tous les patients."""
    generate_patient_command_scripts(base_dir)


@app.command()
def kneepoint_shell(
    setup_xml: str = typer.Argument(..., help="Fichier XML de setup (kneepoint*.xml)"),
    label: str = typer.Argument("0", help="0, 1_class1, 1_class2, 2"),
    out_dir: str = typer.Argument(".", help="Dossier de sortie"),
):
    """Génère un script .sh par patient pour les simulations kneepoint."""
    generate_kneepoint_scripts(setup_xml, label, out_dir)


@app.command()
def averaged_shell(
    xml_file: str = typer.Argument(..., help="XML class1_averaged.xml ou autre"),
    class_label: str = typer.Argument("class1", help="class1 ou class2"),
    out_dir: str = typer.Argument(".", help="Dossier de sortie"),
):
    """Génère un script .sh pour lancer les simulations averaged_* sur tous les patients."""
    generate_averaged_class_scripts(xml_file, class_label, out_dir)


@app.command()
def sensi_xml(
    best_sets_tsv: str = typer.Argument(...),
    output_xml: str = typer.Argument(...),
    mono_init: float = typer.Argument(..., help="gui-prop-mono-init"),
    apo_init: float = typer.Argument(..., help="gui-prop-apo-init"),
):
    """
    Génére un fichier XML BehaviorSpace pour l'analyse de sensibilité.
    (les ranges_dict sont à coder dans un module de config séparé).
    """
    from abm_pipeline.sensitivity_config import RANGES_DICT_CLASS1  # à créer

    make_sensitivity_experiment_xml(
        best_sets_tsv,
        output_xml,
        gui_prop_mono_init=mono_init,
        gui_prop_apo_init=apo_init,
        ranges_dict=RANGES_DICT_CLASS1,
    )


@app.command()
def sensi_shell(
    model_path: str = typer.Argument(..., help="Chemin complet vers ABM_2D_*.nlogo"),
    xml_file: str = typer.Argument(..., help="Fichier XML de sensibilité"),
    out_sh: str = typer.Argument("sensitivity_experiments.sh"),
):
    """Génère le script .sh de lancement des expériences de sensibilité."""
    from abm_pipeline.sensitivity_config import EXP_LIST  # ["perturb-gui-apo-mov", ...]

    generate_sensitivity_shell_scripts(
        model_path=model_path,
        xml_file=xml_file,
        out_sh=out_sh,
        exp_list=EXP_LIST,
    )


# abm_pipeline/cli.py (ajouts en bas)

from abm_pipeline.model_validation.plots import plot_sim_vs_exp_with_scores
from abm_pipeline.model_validation.validator import compute_metrics_all_patients


@app.command()
def plot_sim(
    patient: str = typer.Argument(..., help="ID du patient, ex. CAS1802"),
    param_set: str = typer.Argument(
        ..., help="Nom du set, ex. stocha_best_via / stocha_knee_point / ..."
    ),
    base_dir: str = typer.Argument(".", help="Racine du projet"),
    behaviorspace_dir: str = typer.Option(
        None, help="Sous-dossier BehaviorSpace (défaut: {patient}/BehaviorSpace)"
    ),
    save_dir: str = typer.Option(
        None, help="Dossier où sauver les figures (défaut: {patient}/figures)"
    ),
):
    """Trace simulation vs expérimental pour un patient et un set donné."""
    plot_sim_vs_exp_with_scores(
        patient=patient,
        param_set=param_set,
        base_dir=base_dir,
        behaviorspace_dir=behaviorspace_dir,
        save_dir=save_dir,
    )


@app.command()
def validate_all(
    base_dir: str = typer.Argument(".", help="Racine du projet"),
    output_prefix: str = typer.Argument("NRMSE", help="Préfixe des fichiers TSV"),
):
    """
    Calcule tous les NRMSE (via/conc/sum) pour tous les patients et param_sets
    et écrit les TSV (équivalent RMSE.py).
    """
    compute_metrics_all_patients(base_dir=base_dir, output_prefix=output_prefix)


# abm_pipeline/cli.py (suite)

from abm_pipeline.sensitivity.plots import (
    plot_sensitivity_for_param,
    plot_sensitivity_all_params,
)


@app.command()
def sensitivity_plot(
    exp_name: str = typer.Argument(..., help="Nom de l'expérience (ex: perturb-gui-apo-mov)"),
    csv_dir: str = typer.Argument(..., help="Dossier contenant ABM_2D_sensitivity_*.csv"),
    save_dir: str = typer.Argument(..., help="Dossier pour sauver les figures"),
):
    """Trace la sensibilité pour un seul paramètre."""
    plot_sensitivity_for_param(exp_name, csv_dir, save_dir)


@app.command()
def sensitivity_plot_all(
    csv_dir: str = typer.Argument(...),
    save_dir: str = typer.Argument(...),
):
    """
    Trace toutes les sensibilités (exp_list doit être définie dans sensitivity_config).
    """
    from abm_pipeline.sensitivity_config import EXP_LIST
    plot_sensitivity_all_params(EXP_LIST, csv_dir, save_dir)


# abm_pipeline/cli.py (ajouts)

from abm_pipeline.advanced_analysis.plots_advanced import (
    make_violinplots_all_parameters,
    run_pca_analysis,
)
from abm_pipeline.advanced_analysis.stats_advanced import (
    run_parameter_stats_tests,
)


@app.command()
def violin_all(
    pareto_file: str = typer.Argument(...),
    save_dir: str = typer.Option(None, help="Dossier pour sauvegarder les violins"),
):
    """Produit les violin plots pour tous les paramètres."""
    make_violinplots_all_parameters(pareto_file, save_dir=save_dir)


@app.command()
def pca(
    pareto_file: str = typer.Argument(...),
    save_dir: str = typer.Option(None),
):
    """Effectue la PCA sur les paramètres optimisés."""
    run_pca_analysis(pareto_file, save_dir=save_dir)


@app.command()
def stats_params(
    pareto_file_1: str = typer.Argument(...),
    pareto_file_2: str = typer.Argument(...),
    save_dir: str = typer.Option(None),
):
    """Tests statistiques (Mann-Whitney) entre deux ensembles de paramètres."""
    run_parameter_stats_tests(pareto_file_1, pareto_file_2, save_dir=save_dir)




def main():
    app()


if __name__ == "__main__":
    main()

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/config.py
# abm_pipeline/config.py

from pathlib import Path

# Racine du projet (à adapter si nécessaire)
PROJECT_ROOT = Path(__file__).resolve().parent.parent

# Liste patients + % monocytes 
PATIENTS_WITH_MONO = [
    "CRE1704-1.1%",
    "LAU1405-2.5%",
    "DES2105-1.25%",
    "ORE1706-0.68%",
    "CAS1802-1.04%",
    "GER160522-0.45%",
    "REI230522-0.95%",
    "PUJ240522-0.34%",
    "LAR300522-0.21%",
    "CAZ310522-3.48%",
]

PATIENT_IDS = [p.split("-")[0] for p in PATIENTS_WITH_MONO]

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/model_validation/metrics.py
# abm_pipeline/model_validation/metrics.py

from __future__ import annotations

import math
from typing import Sequence, Dict, Tuple

import numpy as np
import pandas as pd


def _to_array(x: Sequence[float]) -> np.ndarray:
    return np.array(list(x), dtype=float)


def rmse(y_true: Sequence[float], y_pred: Sequence[float]) -> float:
    """Root Mean Square Error."""
    y_t = _to_array(y_true)
    y_p = _to_array(y_pred)
    return float(np.sqrt(np.mean((y_p - y_t) ** 2)))


def nrmse_maxmin(y_true: Sequence[float], y_pred: Sequence[float]) -> float:
    """
    Normalized RMSE = RMSE / (max - min)
    """
    y_t = _to_array(y_true)
    denom = float(y_t.max() - y_t.min())
    if denom == 0:
        return float("nan")
    return rmse(y_t, y_pred) / denom


def nrmse_mean(y_true: Sequence[float], y_pred: Sequence[float]) -> float:
    """
    Normalized RMSE = RMSE / mean(y_true).
    """
    y_t = _to_array(y_true)
    denom = float(y_t.mean())
    if denom == 0:
        return float("nan")
    return rmse(y_t, y_pred) / denom


def nrmse_std(y_true: Sequence[float], y_pred: Sequence[float]) -> float:
    """
    Normalized RMSE = RMSE / std(y_true).
    """
    y_t = _to_array(y_true)
    denom = float(y_t.std(ddof=1))
    if denom == 0:
        return float("nan")
    return rmse(y_t, y_pred) / denom


def compute_viability_conc_nrmse(
    patient_data: pd.DataFrame,
    viability_sim: pd.Series,
    conc_sim: pd.Series,
    patient: str,
) -> Dict[str, Dict[str, float]]:
    """
    Calcule les 3 NRMSE pour la viabilité et la concentration pour un patient.

    patient_data :
        dataframe avec colonnes 'Day', '{patient}_viability', '{patient}_concentration'.
    viability_sim / conc_sim :
        séries avec les mêmes index (en heures ou en jours) que patient_data['Day'].
    """
    via_exp = patient_data[f"{patient}_viability"]
    conc_exp = patient_data[f"{patient}_concentration"]

    # s'assurer que les index sont alignés
    via_exp = via_exp.loc[viability_sim.index]
    conc_exp = conc_exp.loc[conc_sim.index]

    via = {
        "maxmin": nrmse_maxmin(via_exp, viability_sim),
        "mean": nrmse_mean(via_exp, viability_sim),
        "std": nrmse_std(via_exp, viability_sim),
    }
    conc = {
        "maxmin": nrmse_maxmin(conc_exp, conc_sim),
        "mean": nrmse_mean(conc_exp, conc_sim),
        "std": nrmse_std(conc_exp, conc_sim),
    }

    sums = {
        "maxmin": via["maxmin"] + conc["maxmin"],
        "mean": via["mean"] + conc["mean"],
        "std": via["std"] + conc["std"],
    }

    return {"viability": via, "concentration": conc, "sum": sums}

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/model_validation/plots.py
# abm_pipeline/model_validation/plots.py

from __future__ import annotations

from pathlib import Path
from typing import Tuple

import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import mean_squared_error, r2_score

from abm_pipeline.model_validation.metrics import compute_viability_conc_nrmse
from abm_pipeline.parameter_exploration.utils import logger


def _load_patient_exp_data(patient: str, base_dir: str = ".") -> pd.DataFrame:
    """
    Lit les données expérimentales du patient.
    On suppose un fichier: {base_dir}/{patient}/{patient}.csv
    avec colonnes: 'Day', f'{patient}_viability', f'{patient}_concentration'
    """
    path = Path(base_dir) / patient / f"{patient}.csv"
    df = pd.read_csv(path, index_col=0)
    # dans le script RMSE.py, Day est multiplié par 24 → heures
    if "Day" in df.columns:
        df["Day"] = df["Day"].apply(lambda x: x * 24)
    return df


def _load_behaviorspace_csv(simu_file_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Lit le CSV BehaviorSpace et reconstruit deux dataframes :
    - df_viability_simu : viability par step × run
    - df_remaining_simu : concentration par step × run

    On colle à la logique du script historique :
      - step = colonne 21
      - viability = colonne 23
      - remainingCellRatio = colonne 24
      - on saute les 7 premières lignes (header BehaviorSpace).
    """
    viability_dict = {}
    remaining_dict = {}

    with open(simu_file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    for line in lines[7:]:
        line = line.replace('"', "").strip().split(",")
        if len(line) < 25:
            continue
        run_number = int(line[0])
        step = int(line[21])
        viability = float(line[23])
        remaining = float(line[24])

        viability_dict.setdefault(run_number, {})[step] = viability
        remaining_dict.setdefault(run_number, {})[step] = remaining

    df_viability = pd.DataFrame.from_dict(viability_dict)
    df_remaining = pd.DataFrame.from_dict(remaining_dict)
    return df_viability, df_remaining


def plot_sim_vs_exp_with_scores(
    patient: str,
    param_set: str,
    base_dir: str = ".",
    behaviorspace_dir: str | None = None,
    save_dir: str | None = None,
) -> None:
    """
    Reprend la logique de plot_sim_vs_exp_with_scores.py :

    - patient: ex 'CAS1802'
    - param_set: ex 'stocha_best_via', 'stocha_knee_point', ...
    - base_dir: racine du projet (dossiers {patient}/...).
    - behaviorspace_dir: sous-dossier des CSV BehaviorSpace (par défaut {patient}/BehaviorSpace).
    - save_dir: dossier où sauver la figure (par défaut {patient}/figures).

    La figure contient:
      - courbe simulée + points exp pour viabilité
      - courbe simulée + points exp pour concentration
      - NRMSE et r² affichés dans le titre de chaque subplot.
    """
    base_path = Path(base_dir)
    if behaviorspace_dir is None:
        behaviorspace_path = base_path / patient / "BehaviorSpace"
    else:
        behaviorspace_path = base_path / behaviorspace_dir

    simu_file = behaviorspace_path / f"{param_set}.csv"
    logger.info(f"Loading simulations from {simu_file}")

    patient_data = _load_patient_exp_data(patient, base_dir=base_dir)
    df_viability_simu, df_remaining_simu = _load_behaviorspace_csv(str(simu_file))

    # On garde uniquement les time points présents dans les données exp :
    # dans plot_sim_vs_exp_with_scores, la colonne 'Day' est directement utilisée.
    time_points = list(patient_data["Day"])
    filtered_viability_simu = df_viability_simu[
        df_viability_simu.index.isin(time_points)
    ]
    filtered_remaining_simu = df_remaining_simu[
        df_remaining_simu.index.isin(time_points)
    ]

    # Moyennes (sur les runs)
    via_simu_mean = filtered_viability_simu.mean(axis=1)
    conc_simu_mean = filtered_remaining_simu.mean(axis=1)

    # Recalage index des séries sur les time_points
    via_simu_mean.index = time_points
    conc_simu_mean.index = time_points

    # NRMSE (max-min, mean, std) + somme
    metrics = compute_viability_conc_nrmse(
        patient_data=patient_data,
        viability_sim=via_simu_mean,
        conc_sim=conc_simu_mean,
        patient=patient,
    )

    # NRMSE supplémentaire type sklearn (comme dans plot_sim_vs_exp_with_scores.py)
    y_via_exp = patient_data[f"{patient}_viability"]
    y_conc_exp = patient_data[f"{patient}_concentration"]
    nrms_via_sklearn = mean_squared_error(
        y_via_exp, via_simu_mean, squared=False
    ) / (y_via_exp.max() - y_via_exp.min())
    nrms_conc_sklearn = mean_squared_error(
        y_conc_exp, conc_simu_mean, squared=False
    ) / (y_conc_exp.max() - y_conc_exp.min())
    nrms_via_sklearn = round(float(nrms_via_sklearn), 2)
    nrms_conc_sklearn = round(float(nrms_conc_sklearn), 2)

    # r²
    r2_via = round(float(r2_score(y_via_exp, via_simu_mean)), 2)
    r2_conc = round(float(r2_score(y_conc_exp, conc_simu_mean)), 2)

    # --- Figure -------------------------------------------------------------
    fig, axes = plt.subplots(2, 1, figsize=(6, 8), sharex=True)

    # Viabilité
    axes[0].plot(
        via_simu_mean.index / 24.0, via_simu_mean, label="Simulation", linewidth=2
    )
    axes[0].plot(
        patient_data["Day"] / 24.0,
        patient_data[f"{patient}_viability"],
        label="Experimental",
        color="black",
        marker="o",
        linestyle="--",
    )
    axes[0].set_ylabel("Viability (%)")
    axes[0].set_xticks(range(0, 16, 2))
    axes[0].set_title(
        f"{patient} – {param_set}
"
        f"NRMSE_via={nrms_via_sklearn}, R²_via={r2_via}"
    )
    axes[0].legend()

    # Concentration
    axes[1].plot(
        conc_simu_mean.index / 24.0,
        conc_simu_mean,
        label="Simulation",
        linewidth=2,
    )
    axes[1].plot(
        patient_data["Day"] / 24.0,
        patient_data[f"{patient}_concentration"],
        label="Experimental",
        color="black",
        marker="o",
        linestyle="--",
    )
    axes[1].set_xlabel("Day")
    axes[1].set_ylabel("Concentration Ratio (%)")
    axes[1].set_xticks(range(0, 16, 2))
    axes[1].set_title(
        f"NRMSE_conc={nrms_conc_sklearn}, R²_conc={r2_conc}"
    )
    axes[1].legend()

    fig.tight_layout()

    # savefig
    if save_dir is None:
        save_path = base_path / patient / "figures"
    else:
        save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)
    out_file = save_path / f"{patient}_{param_set}_sim_vs_exp.png"
    fig.savefig(out_file, dpi=300, bbox_inches="tight")

    logger.info(f"Figure saved to {out_file}")

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/model_validation/validator.py
# abm_pipeline/model_validation/validator.py

from __future__ import annotations

from pathlib import Path
from typing import Dict, List

import pandas as pd

from abm_pipeline.model_validation.metrics import compute_viability_conc_nrmse
from abm_pipeline.parameter_exploration.utils import (
    get_patient_ids,
    logger,
)
from abm_pipeline.model_validation.plots import _load_patient_exp_data, _load_behaviorspace_csv


DEFAULT_PARAM_SETS = ["stocha_best_via", "stocha_best_conc", "stocha_knee_point"]


def compute_metrics_for_patient(
    patient: str,
    param_sets: List[str] | None = None,
    base_dir: str = ".",
) -> Dict[str, Dict[str, Dict[str, float]]]:
    """
    Calcule tous les NRMSE (via/conc/sum, maxmin/mean/std) pour un patient
    et plusieurs param_sets, comme dans RMSE.py.

    Retourne un dict:
    {
      'stocha_best_via': {
         'viability': {...},
         'concentration': {...},
         'sum': {...},
      },
      ...
    }
    """
    if param_sets is None:
        param_sets = DEFAULT_PARAM_SETS

    base_path = Path(base_dir)
    patient_data = _load_patient_exp_data(patient, base_dir=base_dir)

    all_results: Dict[str, Dict[str, Dict[str, float]]] = {}

    for param_set in param_sets:
        logger.info(f"[{patient}] metrics for {param_set}")
        simu_file = base_path / patient / "BehaviorSpace" / f"{param_set}.csv"
        df_viability_simu, df_remaining_simu = _load_behaviorspace_csv(str(simu_file))

        time_points = list(patient_data["Day"])
        filtered_viability_simu = df_viability_simu[
            df_viability_simu.index.isin(time_points)
        ]
        filtered_remaining_simu = df_remaining_simu[
            df_remaining_simu.index.isin(time_points)
        ]

        via_simu_mean = filtered_viability_simu.mean(axis=1)
        conc_simu_mean = filtered_remaining_simu.mean(axis=1)

        via_simu_mean.index = time_points
        conc_simu_mean.index = time_points

        metrics = compute_viability_conc_nrmse(
            patient_data=patient_data,
            viability_sim=via_simu_mean,
            conc_sim=conc_simu_mean,
            patient=patient,
        )
        all_results[param_set] = metrics

    return all_results


def compute_metrics_all_patients(
    param_sets: List[str] | None = None,
    base_dir: str = ".",
    output_prefix: str = "NRMSE",
) -> None:
    """
    Reproduit la structure de sortie de RMSE.py :

    - NRMSE_via_max_min.tsv
    - NRMSE_via_mean.tsv
    - NRMSE_via_stdev.tsv
    - ... idem pour concentration et somme

    Chaque fichier : index = patients, colonnes = param_sets
    """
    if param_sets is None:
        param_sets = DEFAULT_PARAM_SETS

    patients = get_patient_ids()

    # DataFrames vides
    df_via_maxmin = pd.DataFrame(columns=param_sets, index=patients)
    df_via_mean = pd.DataFrame(columns=param_sets, index=patients)
    df_via_std = pd.DataFrame(columns=param_sets, index=patients)

    df_conc_maxmin = pd.DataFrame(columns=param_sets, index=patients)
    df_conc_mean = pd.DataFrame(columns=param_sets, index=patients)
    df_conc_std = pd.DataFrame(columns=param_sets, index=patients)

    df_sum_maxmin = pd.DataFrame(columns=param_sets, index=patients)
    df_sum_mean = pd.DataFrame(columns=param_sets, index=patients)
    df_sum_std = pd.DataFrame(columns=param_sets, index=patients)

    for patient in patients:
        results = compute_metrics_for_patient(
            patient=patient,
            param_sets=param_sets,
            base_dir=base_dir,
        )
        for param_set in param_sets:
            m = results[param_set]
            via = m["viability"]
            conc = m["concentration"]
            sm = m["sum"]

            df_via_maxmin.loc[patient, param_set] = via["maxmin"]
            df_via_mean.loc[patient, param_set] = via["mean"]
            df_via_std.loc[patient, param_set] = via["std"]

            df_conc_maxmin.loc[patient, param_set] = conc["maxmin"]
            df_conc_mean.loc[patient, param_set] = conc["mean"]
            df_conc_std.loc[patient, param_set] = conc["std"]

            df_sum_maxmin.loc[patient, param_set] = sm["maxmin"]
            df_sum_mean.loc[patient, param_set] = sm["mean"]
            df_sum_std.loc[patient, param_set] = sm["std"]

    # écriture des TSV (comme dans RMSE.py)
    out_prefix = Path(output_prefix)
    df_via_maxmin.to_csv(f"{out_prefix}_via_max_min.tsv", sep="	", index=True)
    df_via_mean.to_csv(f"{out_prefix}_via_mean.tsv", sep="	", index=True)
    df_via_std.to_csv(f"{out_prefix}_via_stdev.tsv", sep="	", index=True)

    df_conc_maxmin.to_csv(f"{out_prefix}_conc_max_min.tsv", sep="	", index=True)
    df_conc_mean.to_csv(f"{out_prefix}_conc_mean.tsv", sep="	", index=True)
    df_conc_std.to_csv(f"{out_prefix}_conc_stdev.tsv", sep="	", index=True)

    df_sum_maxmin.to_csv(f"{out_prefix}_sum_max_min.tsv", sep="	", index=True)
    df_sum_mean.to_csv(f"{out_prefix}_sum_mean.tsv", sep="	", index=True)
    df_sum_std.to_csv(f"{out_prefix}_sum_stdev.tsv", sep="	", index=True)

    logger.info("All NRMSE tables written.")

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/model_validation/__init__.py
# abm_pipeline/model_validation/__init__.py

from .metrics import (
    rmse,
    nrmse_maxmin,
    nrmse_mean,
    nrmse_std,
    compute_viability_conc_nrmse,
)
from .plots import plot_sim_vs_exp_with_scores
from .validator import (
    compute_metrics_for_patient,
    compute_metrics_all_patients,
)

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/initial_ranges/aggregate_data.py
# abm_pipeline/parameter_exploration/initial_ranges/aggregate_data.py

import os
from pathlib import Path

from abm_pipeline.parameter_exploration.utils import logger, debug


def aggregate_population_files(rootdir: str, output_file: str) -> None:
    """
    Concatène tous les fichiers CSV du dossier `rootdir` dans `output_file`.

    - 'population1.csv' est recopié avec son header.
    - tous les autres 'populationX.csv' sont recopiés en sautant la première ligne.

    Comportement équivalent à l'ancien aggregateData.py.
    """
    rootdir_path = Path(rootdir)
    output_path = Path(output_file)
    logger.info(f"Aggregating population files from {rootdir_path} → {output_path}")

    with output_path.open("w", encoding="utf-8") as file_write:
        for _, _, files in os.walk(rootdir_path):
            for fname in sorted(files):
                path_to_file = rootdir_path / fname
                debug(f"Reading {path_to_file}")
                with path_to_file.open("r", encoding="utf-8") as file_read:
                    lines = file_read.readlines()
                    if fname == "population1.csv":
                        file_write.writelines(lines)
                    else:
                        file_write.writelines(lines[1:])

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/initial_ranges/__init__.py
# abm_pipeline/parameter_exploration/initial_ranges/__init__.py

from .aggregate_data import aggregate_population_files

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/instantiate_models/behavior_space_files.py
# abm_pipeline/parameter_exploration/instantiate_models/behavior_space_files.py

from pathlib import Path

from abm_pipeline.parameter_exploration.utils import logger


def _read_patient_dict(patient_dict_file: str, patient: str):
    mono_init = None
    apo_init = None
    with open(patient_dict_file, "r", encoding="utf-8") as f:
        data = f.readlines()
    for line in data[1:]:
        line = line.replace("
", "").split(" ")
        patient_name = line[0]
        if patient_name == patient:
            mono_init = line[1]
            apo_init = line[2]
            break
    if mono_init is None or apo_init is None:
        raise ValueError(f"Patient {patient} non trouvé dans {patient_dict_file}")
    return mono_init, apo_init


def make_behavior_space_file_for_patient(
    best_sets_tsv: str,
    output_file: str,
    patient_dict_file: str = "patient_dict.txt",
) -> None:
    """
    Version "adapted" : ajoute les paramètres mono/apo init spécifiques au patient
    et écrit un fichier de paramètres lisible par NetLogo.
    """
    tsv_path = Path(best_sets_tsv)
    patient = tsv_path.parts[0]  # on suppose "PATIENT/..."
    mono_init, apo_init = _read_patient_dict(patient_dict_file, patient)

    out_path = Path(output_file)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with tsv_path.open("r", encoding="utf-8") as file_read, out_path.open(
        "w", encoding="utf-8"
    ) as file_write:
        data = file_read.readlines()
        n = 1
        for line in data[1:]:
            cols = line.replace("
", "").split("	")
            (
                _set_name,
                _delta_via,
                _delta_conc,
                gui_apo_mov,
                gui_need_sig_mov,
                gui_layers,
                gui_alpha,
                gui_mono_phago_eff,
                gui_NLC_phago_eff,
                gui_M_phago_eff,
                gui_M_kill_eff,
                gui_cll_sens_dist,
                gui_mono_sens_dist,
                gui_nlc_sens_dist,
                gui_macro_sens_dist,
                gui_nlc_threshold,
                gui_sig_init,
                gui_sig_init_std,
                gui_diff_mean,
                gui_diff_std,
                gui_life_init_gamma,
                gui_alpha_distrib,
            ) = cols

            if n == 1:
                file_write.write("Best_via_set
")
            elif n == 2:
                file_write.write("Knee_point_set
")
            elif n == 3:
                file_write.write("Best_conc_set
")

            file_write.write(f'["gui-prop-mono-init" {mono_init}]
')
            file_write.write(f'["gui-prop-apo-init" {apo_init}]
')
            file_write.write(f'["gui-apo-mov" {gui_apo_mov}]
')
            file_write.write(f'["gui-need-sig-mov" {gui_need_sig_mov}]
')
            file_write.write(f'["gui-layers" {gui_layers}]
')
            file_write.write(f'["gui-alpha" {gui_alpha}]
')
            file_write.write(f'["gui-mono-phago-eff" {gui_mono_phago_eff}]
')
            file_write.write(f'["gui-NLC-phago-eff" {gui_NLC_phago_eff}]
')
            file_write.write(f'["gui-M-phago-eff" {gui_M_phago_eff}]
')
            file_write.write(f'["gui-M-kill-eff" {gui_M_kill_eff}]
')
            file_write.write(f'["gui-cll-sens-dist" {gui_cll_sens_dist}]
')
            file_write.write(f'["gui-mono-sens-dist" {gui_mono_sens_dist}]
')
            file_write.write(f'["gui-nlc-sens-dist" {gui_nlc_sens_dist}]
')
            file_write.write(f'["gui-macro-sens-dist" {gui_macro_sens_dist}]
')
            file_write.write(f'["gui-nlc-threshold" {gui_nlc_threshold}]
')
            file_write.write(f'["gui-sig-init" {gui_sig_init}]
')
            file_write.write(f'["gui-sig-init-std" {gui_sig_init_std}]
')
            file_write.write(f'["gui-diff-mean" {gui_diff_mean}]
')
            file_write.write(f'["gui-diff-std" {gui_diff_std}]
')
            file_write.write(f'["gui-life-init-gamma" {gui_life_init_gamma}]
')
            file_write.write(f'["gui-alpha-distrib" {gui_alpha_distrib}]

')
            n += 1

    logger.info(f"BehaviorSpace param file written to {out_path}")


def make_behavior_space_file_generic(best_sets_tsv: str, output_file: str) -> None:
    """
    Version sans mono/apo init spécifiques (ancienne version générique).
    """
    tsv_path = Path(best_sets_tsv)
    out_path = Path(output_file)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with tsv_path.open("r", encoding="utf-8") as file_read, out_path.open(
        "w", encoding="utf-8"
    ) as file_write:
        data = file_read.readlines()
        n = 1
        for line in data[1:]:
            cols = line.replace("
", "").split("	")
            (
                _set_name,
                _delta_via,
                _delta_conc,
                gui_apo_mov,
                gui_need_sig_mov,
                gui_layers,
                gui_alpha,
                gui_mono_phago_eff,
                gui_NLC_phago_eff,
                gui_M_phago_eff,
                gui_M_kill_eff,
                gui_cll_sens_dist,
                gui_mono_sens_dist,
                gui_nlc_sens_dist,
                gui_macro_sens_dist,
                gui_nlc_threshold,
                gui_sig_init,
                gui_sig_init_std,
                gui_diff_mean,
                gui_diff_std,
                gui_life_init_gamma,
                gui_alpha_distrib,
            ) = cols

            if n == 1:
                file_write.write("Best_via_set
")
            elif n == 2:
                file_write.write("Knee_point_set
")
            elif n == 3:
                file_write.write("Best_conc_set
")

            file_write.write(f'["gui-apo-mov" {gui_apo_mov}]
')
            file_write.write(f'["gui-need-sig-mov" {gui_need_sig_mov}]
')
            file_write.write(f'["gui-layers" {gui_layers}]
')
            file_write.write(f'["gui-alpha" {gui_alpha}]
')
            file_write.write(f'["gui-mono-phago-eff" {gui_mono_phago_eff}]
')
            file_write.write(f'["gui-NLC-phago-eff" {gui_NLC_phago_eff}]
')
            file_write.write(f'["gui-M-phago-eff" {gui_M_phago_eff}]
')
            file_write.write(f'["gui-M-kill-eff" {gui_M_kill_eff}]
')
            file_write.write(f'["gui-cll-sens-dist" {gui_cll_sens_dist}]
')
            file_write.write(f'["gui-mono-sens-dist" {gui_mono_sens_dist}]
')
            file_write.write(f'["gui-nlc-sens-dist" {gui_nlc_sens_dist}]
')
            file_write.write(f'["gui-macro-sens-dist" {gui_macro_sens_dist}]
')
            file_write.write(f'["gui-nlc-threshold" {gui_nlc_threshold}]
')
            file_write.write(f'["gui-sig-init" {gui_sig_init}]
')
            file_write.write(f'["gui-sig-init-std" {gui_sig_init_std}]
')
            file_write.write(f'["gui-diff-mean" {gui_diff_mean}]
')
            file_write.write(f'["gui-diff-std" {gui_diff_std}]
')
            file_write.write(f'["gui-life-init-gamma" {gui_life_init_gamma}]
')
            file_write.write(f'["gui-alpha-distrib" {gui_alpha_distrib}]

')
            n += 1

    logger.info(f"Generic BehaviorSpace file written to {out_path}")

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/instantiate_models/xml_behavior_space.py
# abm_pipeline/parameter_exploration/instantiate_models/xml_behavior_space.py

from pathlib import Path
from typing import Dict

import pandas as pd

from abm_pipeline.parameter_exploration.utils import logger


def _load_knee_point_dict(best_sets_tsv: str) -> Dict[str, float]:
    """
    Lit le TSV best_param_sets_* et retourne un dict param_name -> valeur
    en prenant la ligne 'knee_point_set'.
    """
    df = pd.read_csv(best_sets_tsv, sep="	")
    knee_row = df[df["set"] == "knee_point_set"].iloc[0]
    # les colonnes de param commencent après (set, delta_fitness_via, delta_fitness_conc)
    param_cols = knee_row.index[3:]
    knee_dict = {name: float(knee_row[name]) for name in param_cols}
    return knee_dict


def make_sensitivity_experiment_xml(
    best_sets_tsv: str,
    output_xml: str,
    gui_prop_mono_init: float,
    gui_prop_apo_init: float,
    ranges_dict: Dict[str, list],
    repetitions: int = 3,
    time_limit: int = 312,
) -> None:
    """
    Génère un fichier XML de type BehaviorSpace pour l'analyse de sensibilité.

    Équivalent conceptuel aux scripts d'origine:
      - make_behavior_space_experiment_file_class1.py
      - make_behavior_space_experiment_file_class2.py
      
    Paramètres
    ----------
    best_sets_tsv : str
        Fichier TSV best_param_sets_... (contenant la ligne 'knee_point_set').
    output_xml : str
        Fichier XML de sortie.
    gui_prop_mono_init : float
        Proportion initiale de monocytes (valeur moyenne utilisée pour la classe).
    gui_prop_apo_init : float
        Proportion initiale apoptotique (valeur moyenne utilisée pour la classe).
    ranges_dict : dict
        Dictionnaire {param_name: [first, step, last]} pour la perturbation.
        C’est ici que sont recopiées les valeurs des scripts d’origine.
    repetitions : int
        Nombre de répétitions BehaviorSpace.
    time_limit : int
        Nombre de pas de temps.
    """
    knee_point_dict = _load_knee_point_dict(best_sets_tsv)
    out_path = Path(output_xml)

    logger.info(f"Writing sensitivity BehaviorSpace XML → {out_path}")
    with out_path.open("w", encoding="utf-8") as fw:
        fw.write("<experiments>
")

        for param_name in ranges_dict:
            remaining_params = list(ranges_dict.keys())
            remaining_params.remove(param_name)

            exp_name = f"perturb-{param_name}"
            fw.write(
                f'  <experiment name="{exp_name}" repetitions="{repetitions}" '
                'runMetricsEveryStep="true">
'
            )
            fw.write("    <setup>setup</setup>
")
            fw.write("    <go>go</go>
")
            fw.write(f'    <timeLimit steps="{time_limit}"/>
')
            fw.write("    <metric>getSeed</metric>
")
            fw.write("    <metric>getViability</metric>
")
            fw.write("    <metric>getRemainingCellRatio</metric>

")

            # paramètres globaux mono/apo init
            fw.write(
                '    <enumeratedValueSet variable="gui-prop-mono-init">'
                f'<value value="{gui_prop_mono_init}"/></enumeratedValueSet>
'
            )
            fw.write(
                '    <enumeratedValueSet variable="gui-prop-apo-init">'
                f'<value value="{gui_prop_apo_init}"/></enumeratedValueSet>
'
            )

            # les autres paramètres sont fixés sur la valeur du knee_point
            for other_param in remaining_params:
                val = knee_point_dict[other_param]
                fw.write(
                    f'    <enumeratedValueSet variable="{other_param}">'
                    f'<value value="{val}"/></enumeratedValueSet>
'
                )

            first, step, last = ranges_dict[param_name]
            fw.write(
                f'    <steppedValueSet variable="{param_name}" '
                f'first="{first}" step="{step}" last="{last}"/>
'
            )
            fw.write("  </experiment>

")

        fw.write("  </experiments>
")

    logger.info("Sensitivity XML generated.")

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/instantiate_models/__init__.py
# abm_pipeline/parameter_exploration/instantiate_models/__init__.py

from .behavior_space_files import (
    make_behavior_space_file_for_patient,
    make_behavior_space_file_generic,
)

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/nsga2_analysis/export_for_git.py
# abm_pipeline/parameter_exploration/nsga2_analysis/export_for_git.py

from pathlib import Path

from abm_pipeline.parameter_exploration.utils import (
    build_patient_mono_dict,
    get_patient_ids,
    logger,
)


def export_patient_data_for_git(output_root: str = "patient_data_for_git") -> None:
    """
    Reproduit le comportement du script legacy make_files_for_git.py :

    - crée patient_data_for_git/patient_n
    - copie pareto_ABM_2D_{ID}.txt en pareto_front_patient_n.txt
    - met un header "propre" dans le fichier cible
    """
    output_root_path = Path(output_root)
    output_root_path.mkdir(parents=True, exist_ok=True)

    patients = get_patient_ids()
    patient_dict = build_patient_mono_dict()

    for idx, patient in enumerate(patients, start=1):
        patient_dir = output_root_path / f"patient_{idx}"
        patient_dir.mkdir(exist_ok=True)

        logger.info(f"Exporting pareto for {patient} → patient_{idx}")
        src_file = Path(f"{patient}/pareto_ABM_2D_{patient}.txt")
        dest_file = patient_dir / f"pareto_front_patient_{idx}.txt"

        with src_file.open("r", encoding="utf-8") as file_read:
            data = file_read.readlines()

        data[0] = (
            "delta_fitness_via,delta_fitness_conc, apoCellsMovementProba,"
            " needSigCellsMvtProba, layersAroundNLC, antiApoBoost, monoPhagoEff,"
            " nlcPhagoEff, macroPhagoEff, macroKillEff, cllSensingDistance,"
            " monocyteSensingDistance, nlcSensingDistance, macrophageSensingDistance,"
            " nlcThreshold, signalInitMean, signalInitStd, monoDiffThreshold,"
            " monoDiffTimeStd, gammaLifeInitRate, gammaLifeInitShape
"
        )

        with dest_file.open("w", encoding="utf-8") as file_write:
            file_write.writelines(data)

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/nsga2_analysis/extract_best_sets.py
# abm_pipeline/parameter_exploration/nsga2_analysis/extract_best_sets.py

from pathlib import Path
from typing import Literal

import numpy as np
import pandas as pd

from abm_pipeline.parameter_exploration.utils import (
    logger,
    get_patient_ids,
)


def extract_best_param_sets(pareto_file: str) -> pd.DataFrame:
    """
    Depuis un fichier Pareto (CSV), renvoie un DataFrame avec 3 lignes :
      - best_via_set
      - knee_point_set
      - best_conc_set
    """
    pf = pd.read_csv(pareto_file)
    pf["distances"] = np.sqrt(
        pf["delta_fitness_via"] ** 2 + pf["delta_fitness_conc"] ** 2
    )

    best_via = pf[pf.delta_fitness_via == pf.delta_fitness_via.min()]
    best_conc = pf[pf.delta_fitness_conc == pf.delta_fitness_conc.min()]
    knee_point = pf[pf.distances == pf.distances.min()]

    best_params = pd.concat([best_via, knee_point, best_conc])
    best_params = best_params.drop(columns=["distances"])
    best_params.insert(
        loc=0,
        column="set",
        value=["best_via_set", "knee_point_set", "best_conc_set"],
    )
    return best_params


def extract_best_param_sets_to_file(pareto_file: str, output_tsv: str | None = None):
    pf_path = Path(pareto_file)
    if output_tsv is None:
        output_tsv = f"best_param_sets_{pf_path.stem}.tsv"

    df = extract_best_param_sets(pareto_file)
    df.to_csv(output_tsv, sep="	", index=False)
    logger.info(f"Best parameter sets written to {output_tsv}")


def make_best_sets_all_patients(
    set_name: Literal["best_via_set", "best_conc_set", "knee_point_set"],
    output_file: str,
) -> None:
    """
    Concatène, pour tous les patients, la ligne correspondante de
    best_param_sets_ABM_2D_{patient}.tsv dans un seul TSV.
    """
    patients = get_patient_ids()
    all_sets = pd.DataFrame(columns=patients)

    for patient in patients:
        path = Path(f"{patient}/best_param_sets_ABM_2D_{patient}.tsv")
        df = pd.read_csv(path, sep="	", header=0, index_col=0)
        best_set = df.loc[set_name]
        all_sets[patient] = best_set.T

    all_sets.to_csv(output_file, index=True, sep="	")
    logger.info(f"All patients {set_name} written to {output_file}")

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/nsga2_analysis/pareto_front.py
# abm_pipeline/parameter_exploration/nsga2_analysis/pareto_front.py

import math
from pathlib import Path
from typing import List, Tuple

import matplotlib.pyplot as plt

from abm_pipeline.parameter_exploration.utils import logger, debug


def _compute_all_points(lines: List[str]) -> List[List[float]]:
    points: List[List[float]] = []

    for line in lines[1:]:
        line = line.replace("
", "").split(",")
        apo = int(line[0])
        need_sig = int(line[1])
        layers = int(line[2])
        alpha = int(line[3])
        mono_phago = int(line[4])
        nlc_phago = int(line[5])
        m2_phago = int(line[6])
        m2_kill = int(line[7])
        cll_dist = int(line[8])
        mono_dist = int(line[9])
        nlc_dist = int(line[10])
        macro_dist = int(line[11])
        nlc_threshold = int(line[12])
        signal_init_mean = int(line[13])
        signal_init_std = int(line[14])
        diff_time = int(line[15])
        diff_init_std = int(line[16])
        gamma_life_init = int(line[17])
        alpha_distrib = float(line[18])
        delta_via = float(line[19])
        delta_conc = float(line[20])

        euclid = math.sqrt(delta_via ** 2 + delta_conc ** 2)

        points.append(
            [
                delta_via,
                delta_conc,
                euclid,
                1,  # marqueur Pareto ou non
                apo,
                need_sig,
                layers,
                alpha,
                mono_phago,
                nlc_phago,
                m2_phago,
                m2_kill,
                cll_dist,
                mono_dist,
                nlc_dist,
                macro_dist,
                nlc_threshold,
                signal_init_mean,
                signal_init_std,
                diff_time,
                diff_init_std,
                gamma_life_init,
                alpha_distrib,
            ]
        )
    return points


def _flag_pareto_points(points: List[List[float]]):
    # déduplication
    unique = list({tuple(p) for p in points})
    unique = [list(p) for p in unique]

    unique.sort(key=lambda x: x[2])  # par distance euclidienne

    for i in range(len(unique)):
        x1, y1 = unique[i][0], unique[i][1]
        for j in range(len(unique)):
            x2, y2 = unique[j][0], unique[j][1]
            if (x1, y1) != (x2, y2) and (x2 <= x1) and (y2 <= y1):
                unique[i][3] = 0
                break

    pareto_front = [
        (p[0], p[1], p[4:])
        for p in unique
        if p[3] == 1
    ]
    return pareto_front, unique


def build_pareto_front(input_file: str, output_prefix: str) -> None:
    """
    Construit le Pareto front à partir d'un CSV OpenMOLE.

    - input_file: CSV avec colonnes [apo, needSig, ..., delta_fitness_via, delta_fitness_conc]
    - output_prefix: préfixe pour PNG et TXT ('pareto_ABM_2D_patient' par ex.)
    """
    input_path = Path(input_file)
    logger.info(f"Building pareto front from {input_path}")

    with input_path.open("r", encoding="utf-8") as file_read:
        lines = file_read.readlines()

    points = _compute_all_points(lines)
    pareto_front, all_points = _flag_pareto_points(points)

    # Plot
    x_val = [p[0] for p in all_points]
    y_val = [p[1] for p in all_points]
    x_pareto = [p[0] for p in pareto_front]
    y_pareto = [p[1] for p in pareto_front]

    fig, ax = plt.subplots()
    ax.scatter(x_val, y_val, s=3)
    ax.scatter(x_pareto, y_pareto, s=5)
    ax.set_xlabel(r"$\Delta Viability$ fitness")
    ax.set_ylabel(r"$\Delta Concentration$ fitness")
    ax.set_title("Pareto front")
    ax.grid(True)
    fig.tight_layout()
    png_path = Path(f"{output_prefix}.png")
    fig.savefig(png_path, bbox_inches="tight")

    logger.info(f"len(pareto_front) = {len(pareto_front)}")

    pareto_sorted = sorted(pareto_front, key=lambda x: x[0])
    header = (
        "delta_fitness_via,delta_fitness_conc,apo,needSig,layers,alpha,"
        "monoPhago,NLCPhago,M2Phago,M2Kill,cllDist,MonoDist,nlcDist,"
        "macroDist,nlcThreshold,signalInitMean,signalInitStd,diffTime,"
        "diffInitStd,LifeInitGamma,alphaDistrib
"
    )

    txt_path = Path(f"{output_prefix}.txt")
    with txt_path.open("w", encoding="utf-8") as file_write:
        file_write.write(header)
        for sets in pareto_sorted:
            line = (",".join(str(x) for x in sets)).replace("[", "").replace("]", "")
            file_write.write(line + "
")

    logger.info(f"Pareto text written to {txt_path}")

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/nsga2_analysis/__init__.py
# abm_pipeline/parameter_exploration/nsga2_analysis/__init__.py

from .pareto_front import build_pareto_front
from .extract_best_sets import (
    extract_best_param_sets,
    extract_best_param_sets_to_file,
    make_best_sets_all_patients,
)
from .export_for_git import export_patient_data_for_git

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/shell_commands/averaged.py
# abm_pipeline/parameter_exploration/shell_commands/averaged.py

from pathlib import Path
from abm_pipeline.parameter_exploration.utils import get_patient_ids, logger
from .patients import NETLOGO_HEADLESS


def generate_averaged_class_scripts(
    xml_file: str,
    class_label: str,
    output_dir: str = ".",
) -> None:
    """
    Équivalent à averaged_simu_shell.py :
    crée un patient_command_averaged_classX_simu.sh
    qui lance 'averaged_simu_{PATIENT}' pour tous les patients.
    """
    patients = get_patient_ids()
    out_path = Path(output_dir) / f"patient_command_averaged_{class_label}_simu.sh"
    logger.info(f"Writing {out_path}")

    with out_path.open("w", encoding="utf-8") as fw:
        fw.write("#!/bin/bash

")
        for patient in patients:
            fw.write(
                f'"{NETLOGO_HEADLESS}" --model {patient}/ABM_2D_{patient}.nlogo '
                f"--setup-file {xml_file} "
                ff"--experiment averaged_simu_{patient} "
                f"--table {patient}/BehaviorSpace/averaged_{class_label}_simu_{patient}.csv "
                "--threads 4

"
            )

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/shell_commands/kneepoint.py
# abm_pipeline/parameter_exploration/shell_commands/kneepoint.py

from pathlib import Path
from typing import Literal

from abm_pipeline.parameter_exploration.utils import get_patient_ids, logger
from .patients import NETLOGO_HEADLESS


def generate_kneepoint_scripts(
    setup_xml: str,
    class_label: Literal["0", "1_class1", "1_class2", "2"],
    output_dir: str = ".",
) -> None:
    """
    Reprend la logique de:
      - kneepoint0_by_patient_simu_shell.py
      - kneepoint1_class1_by_patient_simu_shell.py
      - kneepoint1_class2_by_patient_simu_shell.py
      - kneepoint2_by_patient_simu_shell.py

    Un script .sh par patient, avec l'appel NetLogo correspondant.
    """
    patients = get_patient_ids()
    out_path = Path(output_dir)

    for patient in patients:
        sh_name = f"kneepoint{class_label}_by_patient_simu_{patient}.sh"
        sh_path = out_path / sh_name
        logger.info(f"Writing {sh_path}")

        exp_name = f"kneepoint{class_label}_simu_{patient}"
        behavior_csv = f"{patient}/BehaviorSpace/{exp_name}.csv"

        with sh_path.open("w", encoding="utf-8") as fw:
            fw.write("#!/bin/bash

")
            fw.write(
                f'"{NETLOGO_HEADLESS}" --model {patient}/ABM_2D_{patient}.nlogo '
                f"--setup-file {setup_xml} "
                f"--experiment {exp_name} "
                f"--table {behavior_csv} --threads 4
"
            )

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/shell_commands/patients.py
# abm_pipeline/parameter_exploration/shell_commands/patients.py

from pathlib import Path

from abm_pipeline.parameter_exploration.utils import (
    get_patient_ids,
    logger,
)


NETLOGO_HEADLESS = "/I/Program Files/NetLogo 6.1.0/netlogo-headless.bat"


def generate_patient_command_scripts(base_dir: str = ".") -> None:
    """
    Reprend la logique de list_of_commands.py :

    Pour chaque patient, crée un fichier patient_command_{PATIENT}.sh
    avec la chaîne d’appels :
      - aggregateData
      - remove_duplicates...
      - copy_for_git_paretoFront...
      - extract_param_sets_from_pareto...
      - parse_best_param_for_behavior_space_adapted...
      - make_behavior_space_experiment_file...
      - 3 simulations stocha_* via NetLogo
      - 3 plots plot_sim_vs_exp.py
    """
    patients = get_patient_ids()
    base_path = Path(base_dir)

    for patient in patients:
        sh_path = base_path / f"patient_command_{patient}.sh"
        logger.info(f"Writing {sh_path}")

        with sh_path.open("w", encoding="utf-8") as fw:
            fw.write("#!/bin/bash

")
            fw.write(f'echo "This is a shell script for patient {patient}"

')

            fw.write(
                f"python scripts/aggregateData.py {patient}/ABM_2D_{patient} "
                f"{patient}/outputs_ABM_2D_{patient}.txt

"
            )
            fw.write(
                "python scripts/"
                f"remove_duplicates_generic_with_filtering_keeping_only_samples.py "
                f"{patient}/outputs_ABM_2D_{patient}.txt fitnessVia 50

"
            )
            fw.write(
                "python scripts/copy_for_git_paretoFrontGenericStochastic.py "
                f"{patient}/outputs_ABM_2D_{patient}"
                "_duplicates_removed_filtered_only_samples_kept_50.0.txt "
                f"{patient}/pareto_ABM_2D_{patient}

"
            )
            fw.write(
                "python scripts/extract_param_sets_from_pareto_adapted.py "
                f"{patient}/pareto_ABM_2D_{patient}.txt "
                f"{patient}/best_param_sets_ABM_2D_{patient}.tsv

"
            )
            fw.write(
                "python scripts/parse_best_param_for_behavior_space_adapted.py "
                f"{patient}/best_param_sets_ABM_2D_{patient}.tsv "
                f"{patient}/netlogo_best_param_sets_ABM_2D_{patient}.txt

"
            )
            fw.write(
                "python scripts/make_behavior_space_experiment_file.py "
                f"{patient}/best_param_sets_ABM_2D_{patient}.tsv "
                f"{patient}/experiment_file.xml

"
            )

            for exp in ["stocha_best_via", "stocha_knee_point", "stocha_best_conc"]:
                fw.write(
                    f'"{NETLOGO_HEADLESS}" --model {patient}/ABM_2D_{patient}.nlogo '
                    f"--setup-file {patient}/experiment_file.xml "
                    f"--experiment {exp} "
                    f"--table {patient}/BehaviorSpace/{exp}.csv --threads 4

"
                )

            for exp in ["stocha_best_via", "stocha_knee_point", "stocha_best_conc"]:
                fw.write(
                    f"python scripts/plot_sim_vs_exp.py {patient} {exp}
"
                )

        # sur Git Bash : chmod +x patient_command_*.sh

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/shell_commands/sensitivity.py
# abm_pipeline/parameter_exploration/shell_commands/sensitivity.py

from pathlib import Path

from abm_pipeline.parameter_exploration.utils import logger

from .patients import NETLOGO_HEADLESS


def generate_sensitivity_shell_scripts(
    model_path: str,
    xml_file: str,
    out_sh: str,
    exp_list: list[str],
) -> None:
    """
    Reprend la logique de make_shell_commands*.py :

    - model_path : chemin complet vers le modèle NetLogo .nlogo
    - xml_file   : fichier XML pour l'analyse de sensibilité
    - out_sh     : nom du .sh (sensitivity_experiments_class1.sh, etc.)
    - exp_list   : ["perturb-gui-apo-mov", ...]
    """
    sh_path = Path(out_sh)
    logger.info(f"Writing {sh_path}")

    with sh_path.open("w", encoding="utf-8") as fw:
        fw.write("#!/bin/bash

")
        for exp in exp_list:
            fw.write(f'echo "This is a shell script for exp {exp}"
')
            fw.write(
                f'"{NETLOGO_HEADLESS}" --model {model_path} '
                f"--setup-file {xml_file} --experiment {exp} "
                f"--table ABM_2D_sensitivity_{exp}.csv --threads 4

"
            )

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/shell_commands/__init__.py
# abm_pipeline/parameter_exploration/shell_commands/__init__.py

from .patients import (
    generate_patient_command_scripts,
    generate_kneepoint_scripts,
    generate_averaged_class_scripts,
)
from .sensitivity import generate_sensitivity_shell_scripts

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/utils.py
# abm_pipeline/parameter_exploration/utils.py

import logging
from typing import List, Dict

from abm_pipeline.config import PATIENTS_WITH_MONO, PATIENT_IDS

# --- logging / debug -------------------------------------------------------

logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s | %(name)s | %(message)s",
)

logger = logging.getLogger("abm_pipeline")


def debug(msg: str):
    """Wrapper simple pour les messages de debug."""
    logger.debug(msg)


# --- patients --------------------------------------------------------------

def get_patient_ids() -> List[str]:
    """['CRE1704', 'LAU1405', ...]."""
    return PATIENT_IDS.copy()


def get_patients_with_mono() -> List[str]:
    """['CRE1704-1.1%', ...]."""
    return PATIENTS_WITH_MONO.copy()


def build_patient_mono_dict() -> Dict[str, float]:
    """
    {'CRE1704': 1.1, ...}
    Utile pour les scripts qui ont besoin du % mono.
    """
    patient_dict = {}
    for patient in PATIENTS_WITH_MONO:
        name = patient.split("-")[0]
        mono = float(patient.split("-")[1].rstrip("%"))
        patient_dict[name] = mono
    return patient_dict

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/parameter_exploration/__init__.py
# abm_pipeline/parameter_exploration/__init__.py

from . import initial_ranges
from . import nsga2_analysis
from . import instantiate_models
from .utils import logger, debug, get_patient_ids, get_patients_with_mono

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/sensitivity/plots.py
# abm_pipeline/sensitivity/plots.py

from __future__ import annotations
from pathlib import Path
from typing import List, Dict

import matplotlib.pyplot as plt
import pandas as pd

from abm_pipeline.sensitivity.utils import load_sensitivity_csv
from abm_pipeline.parameter_exploration.utils import logger


def plot_sensitivity_for_param(
    exp_name: str,
    csv_dir: str,
    save_dir: str,
) -> None:
    """
    Produit une figure (viabilité + concentration) pour UN paramètre perturbé.

    exp_name est du type 'perturb-gui-apo-mov',
    CSV attendu : {csv_dir}/ABM_2D_sensitivity_{exp_name}.csv
    """
    csv_path = Path(csv_dir) / f"ABM_2D_sensitivity_{exp_name}.csv"
    logger.info(f"Loading sensitivity CSV: {csv_path}")

    df_viability, df_remaining = load_sensitivity_csv(str(csv_path))

    # moyennes sur les runs
    via_mean = df_viability.mean(axis=1)
    conc_mean = df_remaining.mean(axis=1)

    # figure
    fig, axes = plt.subplots(2, 1, figsize=(6, 7), sharex=True)

    axes[0].plot(via_mean.index, via_mean, label="Simulation", linewidth=2)
    axes[0].set_ylabel("Viability (%)")
    axes[0].set_title(f"Perturbation: {exp_name}")

    axes[1].plot(conc_mean.index, conc_mean, label="Simulation", linewidth=2)
    axes[1].set_ylabel("Concentration Ratio (%)")
    axes[1].set_xlabel("Step")

    fig.tight_layout()

    save_dir_path = Path(save_dir)
    save_dir_path.mkdir(parents=True, exist_ok=True)
    out_path = save_dir_path / f"sensitivity_{exp_name}.png"
    fig.savefig(out_path, dpi=300, bbox_inches="tight")
    logger.info(f"Saved {out_path}")


def plot_sensitivity_all_params(
    exp_list: List[str],
    csv_dir: str,
    save_dir: str,
) -> None:
    """Produit une figure par paramètre dans exp_list."""
    for exp_name in exp_list:
        plot_sensitivity_for_param(exp_name, csv_dir, save_dir)

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/sensitivity/utils.py
# abm_pipeline/sensitivity/utils.py

from __future__ import annotations
from pathlib import Path
import pandas as pd
from typing import Dict, Tuple


def load_sensitivity_csv(csv_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Lecture d’un fichier BehaviorSpace de sensibilité.
    Retourne :
      - df_viability : DataFrame index=step, columns=run_number
      - df_concentration : pareil
    """
    viability_dict: Dict[int, Dict[int, float]] = {}
    remaining_dict: Dict[int, Dict[int, float]] = {}

    with open(csv_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    for line in lines[7:]:  # header BehaviorSpace
        row = line.replace('"', "").strip().split(",")
        if len(row) < 25:
            continue

        run_number = int(row[0])
        step = int(row[21])
        viability = float(row[23])
        remaining = float(row[24])

        viability_dict.setdefault(run_number, {})[step] = viability
        remaining_dict.setdefault(run_number, {})[step] = remaining

    df_viability = pd.DataFrame.from_dict(viability_dict)
    df_remaining = pd.DataFrame.from_dict(remaining_dict)

    return df_viability, df_remaining

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/sensitivity/__init__.py
# abm_pipeline/sensitivity/__init__.py

from .plots import plot_sensitivity_for_param, plot_sensitivity_all_params

./CV/04_Postdoc_INSERM/01_AgentBasedModel/abm_pipeline/__init__.py
# abm_pipeline/__init__.py

from . import config
from . import parameter_exploration

./CV/04_Postdoc_INSERM/02_BooleanModel/run_pipeline.py
import argparse
from src.pipeline import run_signatures, run_all


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--step", type=str, default="all")
    args = parser.parse_args()

    if args.step == "signatures":
        run_signatures()
    elif args.step == "all":
        run_all()
    else:
        print("Unknown step:", args.step)


if __name__ == "__main__":
    main()

./CV/04_Postdoc_INSERM/02_BooleanModel/src/io.py
from pathlib import Path


def read_tsv(filepath: Path):
    rows = []
    with open(filepath, "r", encoding="utf-8") as f:
        for line in f.readlines()[1:]:
            parts = line.strip().split("	")
            rows.append(parts)
    return rows


def write_lines(filepath: Path, lines):
    with open(filepath, "a+", encoding="utf-8") as f:
        for line in lines:
            f.write(line + "
")

./CV/04_Postdoc_INSERM/02_BooleanModel/src/pipeline.py
from pathlib import Path
from .signatures import compute_signatures
from .utils import Timer

DATA = Path("data")


def run_signatures():
    input_file = DATA / "input" / "Dorothea_TF_activity_scale.tsv"
    output_file = DATA / "output" / "scale_rescaled_output.txt"

    with Timer() as t:
        results = compute_signatures(input_file, output_file)

    print("M1_signature:", results["M1_signature"])
    print("M2_signature:", results["M2_signature"])
    print("NLC_signature:", results["NLC_signature"])
    print("NLC count:", len(results["NLC_signature"]))
    print("Time:", t.duration)


def run_all():
    run_signatures()

./CV/04_Postdoc_INSERM/02_BooleanModel/src/signatures.py
from pathlib import Path
from .io import read_tsv, write_lines

MODEL_TF = [
    "STAT1", "STAT5A", "STAT5B", "NFKB1", "NFKB2",
    "PPARG", "STAT6", "STAT3", "IRF3", "IRF5",
    "IRF4", "KLF4", "HIF1A",
]


def compute_signatures(input_file: Path, output_file: Path):
    rows = read_tsv(input_file)

    M1, M2, NLC = [], [], []
    output_lines = []

    for row in rows:
        TF = row[0]
        m1 = float(row[2])
        m2 = float(row[3])
        nlc = float(row[4])

        if nlc > m1 and nlc > m2:
            NLC.append(TF)
            output_lines.append(f"{TF}	{m1}	{m2}	{nlc}")
        elif m1 > nlc and m1 > m2:
            M1.append(TF)
        elif m2 > m1 and m2 > nlc:
            M2.append(TF)

    write_lines(output_file, output_lines)

    return {
        "M1_signature": set(M1),
        "M2_signature": set(M2),
        "NLC_signature": set(NLC),
    }

./CV/04_Postdoc_INSERM/02_BooleanModel/src/utils.py
import time


class Timer:
    def __enter__(self):
        self.start = time.time()
        return self

    def __exit__(self, *args):
        self.end = time.time()
        self.duration = self.end - self.start

./CV/04_Postdoc_INSERM/02_BooleanModel/src/__init__.py
# Package initialization for the Boolean Model pipeline.
# Exposes high-level functions for convenient imports.

from .pipeline import run_all, run_signatures
from .signatures import compute_signatures

__all__ = [
    "run_all",
    "run_signatures",
    "compute_signatures",
]

./CV/04_Postdoc_INSERM/03_RNASeqDeconvolution/pipeline/python/sample_parser.py


./CV/04_Postdoc_INSERM/03_RNASeqDeconvolution/revision/python/heatmap_ROC.py


./CV/04_Postdoc_INSERM/03_RNASeqDeconvolution/revision/python/predict_from_deconv_revision.py


./CV/04_Postdoc_INSERM/03_RNASeqDeconvolution/revision/python/predict_from_deconv_revision_last_miguel.py


./CV/04_Postdoc_INSERM/03_RNASeqDeconvolution/revision/python/predict_from_deconv_revision_LP_False.py


./CV/04_Postdoc_INSERM/03_RNASeqDeconvolution/revision/python/predict_from_deconv_revision_LP_False_output_dfcorr.py


./CV/04_Postdoc_INSERM/03_RNASeqDeconvolution/revision/python/predict_from_deconv_revision_master_dfcorr.py


./CV/04_Postdoc_INSERM/03_RNASeqDeconvolution/revision/python/predict_from_deconv_revision_miguel_january.py


./CV/04_Postdoc_INSERM/05_NetworkMedicine/01_multilayer_pipeline/compute_observed_degrees_clean.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Input: multilayer network + mock networks provided by M. De Domenico (CoMuNe Lab)

This module computes the OBSERVED structural metrics of CovMulNet19:
- degree / strength for each node of a given source type to a given target type
- supports directed or undirected mode
- exports:
    * observed degrees
    * observed relative degrees
    * observed withZeros tables
    * rankings

This clean version replaces the following historical scripts:
- protein-protein-degrees-rankings.py
- protein-drug-degrees-(directed|undirected)-rankings.py
- protein-disease-degrees-directed-rankings-*.py
- protein-symptom-degrees-directed-rankings-*.py
- protein-GO-degrees-directed-rankings-*.py
- protein-target-degrees-directed/undirected corrected generic scripts
- all “withZeros.tsv” generation blocks
- all degree ranking scripts

Usage (CLI example):
    python compute_observed_degrees_clean.py \
        --nodes COVID19_GDDS_nodes.csv \
        --edges COVID19_GDDS_edges.csv \
        --source_type protein \
        --target_type disease \
        --directed True \
        --output outdir/
"""

import argparse
import csv
import os
import networkx as nx


# -------------------------------------------------------------------------
# Verbose print
# -------------------------------------------------------------------------

VERBOSE = True

def vprint(*a, **k):
    if VERBOSE:
        print(*a, **k)


# -------------------------------------------------------------------------
# I/O Helpers
# -------------------------------------------------------------------------

def load_nodes(node_file):
    """
    Load node list from CovMulNet19 node file.
    Expected format: node_id, node_type, description, ...
    """
    nodes = {}
    with open(node_file, "r", encoding="utf-8") as f:
        reader = csv.reader(f)
        header = next(reader)
        for row in reader:
            node_id = row[0]
            node_type = row[1].strip()
            nodes[node_id] = node_type
    vprint(f"[load_nodes] Loaded {len(nodes)} nodes.")
    return nodes


def load_edges(edge_file):
    """
    Load edges from CovMulNet19 edge file.
    Expected format: source, target, ...
    """
    edges = []
    with open(edge_file, "r", encoding="utf-8") as f:
        reader = csv.reader(f)
        header = next(reader)
        for row in reader:
            src = row[0]
            tgt = row[1]
            edges.append((src, tgt))
    vprint(f"[load_edges] Loaded {len(edges)} edges.")
    return edges


# -------------------------------------------------------------------------
# Core Computation
# -------------------------------------------------------------------------

def compute_observed_degrees(
    nodes,
    edges,
    source_type="protein",
    target_type="disease",
    directed=True
):
    """
    Build graph, filter by source-target types, return degree mapping.

    Returns:
        observed: {source_node: count of edges to target nodes}
        all_source_nodes: list of all nodes of source type
        all_target_nodes: list of nodes of target type
    """
    # Select nodes of interest
    source_nodes = {n for n, t in nodes.items() if t.lower() == source_type.lower()}
    target_nodes = {n for n, t in nodes.items() if t.lower() == target_type.lower()}

    vprint(f"[compute_observed_degrees] {len(source_nodes)} source nodes ({source_type})")
    vprint(f"[compute_observed_degrees] {len(target_nodes)} target nodes ({target_type})")

    # Build graph
    if directed:
        G = nx.DiGraph()
    else:
        G = nx.Graph()

    G.add_nodes_from(nodes.keys())
    G.add_edges_from(edges)

    observed = {}

    # Count connections source -> target
    for s in source_nodes:
        count = 0
        if directed:
            neighbors = G.successors(s)
        else:
            neighbors = G.neighbors(s)

        for neigh in neighbors:
            if neigh in target_nodes:
                count += 1

        observed[s] = count

    return observed, sorted(source_nodes), sorted(target_nodes)


# -------------------------------------------------------------------------
# Helpers: relative degrees, zeros, ranking
# -------------------------------------------------------------------------

def compute_relative_degrees(observed_dict):
    total = sum(observed_dict.values())
    if total == 0:
        return {k: 0.0 for k in observed_dict}
    return {k: v / total for k, v in observed_dict.items()}


def write_table(path, header, rows):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f, delimiter="	")
        writer.writerow(header)
        writer.writerows(rows)
    vprint(f"[write] Wrote: {path}")


# -------------------------------------------------------------------------
# Export Full Results (observed, relative, withZeros, ranking)
# -------------------------------------------------------------------------

def export_observed_results(
    observed,
    source_nodes,
    source_type,
    target_type,
    outdir,
    directed
):
    mode = "directed" if directed else "undirected"

    # 1. observed degrees (only nodes w/ deg > 0)
    rows_obs = [(n, observed[n]) for n in source_nodes if observed[n] > 0]
    write_table(
        os.path.join(outdir, f"{source_type}_to_{target_type}_{mode}_observed.tsv"),
        ["source_node", "degree"],
        rows_obs
    )

    # 2. observed relative degrees
    rel = compute_relative_degrees(observed)
    rows_rel = [(n, rel[n]) for n in source_nodes if observed[n] > 0]
    write_table(
        os.path.join(outdir, f"{source_type}_to_{target_type}_{mode}_relative.tsv"),
        ["source_node", "relative_degree"],
        rows_rel
    )

    # 3. withZeros (all source nodes)
    rows_zero = [(n, observed[n], rel[n]) for n in source_nodes]
    write_table(
        os.path.join(outdir, f"{source_type}_to_{target_type}_{mode}_withZeros.tsv"),
        ["source_node", "degree", "relative_degree"],
        rows_zero
    )

    # 4. ranking (sorted descending)
    ranked = sorted(rows_obs, key=lambda x: x[1], reverse=True)
    write_table(
        os.path.join(outdir, f"{source_type}_to_{target_type}_{mode}_ranking.tsv"),
        ["source_node", "degree"],
        ranked
    )


# -------------------------------------------------------------------------
# Main (CLI)
# -------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser()

    parser.add_argument("--nodes", required=True, help="COVID19 nodes CSV")
    parser.add_argument("--edges", required=True, help="COVID19 edges CSV")

    parser.add_argument("--source_type", required=True,
                        help="Type of source nodes (e.g., protein)")
    parser.add_argument("--target_type", required=True,
                        help="Type of target nodes (e.g., disease)")

    parser.add_argument("--directed", default="True",
                        help="True/False")

    parser.add_argument("--output", required=True,
                        help="Output directory")

    parser.add_argument("--verbose", default="True",
                        help="True/False")

    args = parser.parse_args()

    global VERBOSE
    VERBOSE = (args.verbose.lower() == "true")
    directed = (args.directed.lower() == "true")

    nodes = load_nodes(args.nodes)
    edges = load_edges(args.edges)

    observed, source_nodes, target_nodes = compute_observed_degrees(
        nodes,
        edges,
        source_type=args.source_type,
        target_type=args.target_type,
        directed=directed
    )

    export_observed_results(
        observed,
        source_nodes,
        args.source_type,
        args.target_type,
        args.output,
        directed
    )


if __name__ == "__main__":
    main()

./CV/04_Postdoc_INSERM/05_NetworkMedicine/01_multilayer_pipeline/io_networks_clean.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Input: multilayer network + mock networks provided by M. De Domenico (CoMuNe Lab)

This module handles:
- discovery of mock network files (nodes_*.csv, edges_*.csv)
- consistency between node files and edge files
- filtering out corrupted or unwanted networks
- returning a clean, sorted list of available mock network IDs

Used in:
- compute_mock_distributions.py
- downstream bootstrap and z-score computations
"""

import os
import re

VERBOSE = True

def vprint(*args, **kwargs):
    if VERBOSE:
        print(*args, **kwargs)


# ----------------------------------------------------------------------
# Utilities
# ----------------------------------------------------------------------

def _extract_id(filename, prefix="nodes_"):
    """
    Extract numeric ID from filenames like: nodes_00342.csv
    """
    num = filename.replace(".csv", "").replace(prefix, "")
    if num.isdigit():
        return int(num)
    return None


def _detect_files(basepath):
    """
    Returns two dicts:
    - nodes: {id: filepath}
    - edges: {id: filepath}
    """
    nodes = {}
    edges = {}

    for fname in os.listdir(basepath):
        if fname.startswith("nodes_") and fname.endswith(".csv"):
            node_id = _extract_id(fname, prefix="nodes_")
            if node_id is not None:
                nodes[node_id] = os.path.join(basepath, fname)

        elif fname.startswith("edges_") and fname.endswith(".csv"):
            edge_id = _extract_id(fname, prefix="edges_")
            if edge_id is not None:
                edges[edge_id] = os.path.join(basepath, fname)

    return nodes, edges


def _detect_inconsistencies(nodes_dict, edges_dict):
    """
    Detect IDs present in one set and missing in the other.
    """
    node_ids = set(nodes_dict.keys())
    edge_ids = set(edges_dict.keys())
    diff_nodes = node_ids - edge_ids
    diff_edges = edge_ids - node_ids
    return diff_nodes, diff_edges


# ----------------------------------------------------------------------
# Main function
# ----------------------------------------------------------------------

def load_mock_network_ids(
    basepath,
    exclude_ids=None,
    verbose=True,
):
    """
    Detects all usable mock network IDs in a folder.

    Parameters
    ----------
    basepath : str
        Path containing nodes_*.csv and edges_*.csv.
    exclude_ids : list[int]
        List of network IDs to exclude (corrupted or invalid).
    verbose : bool

    Returns
    -------
    mock_ids : list[int]
        Sorted list of valid mock network IDs.
    nodes_dict : dict
        Mapping id -> node file path
    edges_dict : dict
        Mapping id -> edge file path
    """

    global VERBOSE
    VERBOSE = verbose

    vprint(f"[io_networks] Scanning folder: {basepath}")

    nodes_dict, edges_dict = _detect_files(basepath)
    vprint(f"[io_networks] Found {len(nodes_dict)} node files.")
    vprint(f"[io_networks] Found {len(edges_dict)} edge files.")

    # Check consistency
    diff_nodes, diff_edges = _detect_inconsistencies(nodes_dict, edges_dict)
    if diff_nodes:
        vprint(f"[io_networks][WARNING] IDs missing edges: {sorted(diff_nodes)}")
    if diff_edges:
        vprint(f"[io_networks][WARNING] IDs missing nodes: {sorted(diff_edges)}")

    # Intersection = usable
    usable_ids = set(nodes_dict.keys()).intersection(edges_dict.keys())
    vprint(f"[io_networks] IDs with both nodes & edges: {len(usable_ids)}")

    # Exclusions
    if exclude_ids:
        vprint(f"[io_networks] Excluding {len(exclude_ids)} IDs explicitly.")
        usable_ids = usable_ids - set(exclude_ids)

    mock_ids = sorted(list(usable_ids))
    vprint(f"[io_networks] Final usable mock networks: {len(mock_ids)}")

    return mock_ids, nodes_dict, edges_dict


# ----------------------------------------------------------------------
# Example usage (not executed when imported)
# ----------------------------------------------------------------------

if __name__ == "__main__":
    # Example provided path
    base = "A:/Downloads/Projects/workFromHome/Projects/Covid/random_networks/Mock_networks_21Apr"

    # Example known-bad networks (from your scripts)
    exclude = [494, 525, 532, 569, 592, 629, 658, 662, 674, 676, 
               701, 705, 712, 736, 757, 788, 796, 825, 827, 838, 
               873, 876, 908, 910, 916, 917, 918, 938, 958, 965, 
               1088, 1125, 1128, 1145, 1146, 1165, 1171, 1183, 1185, 
               1192, 1195, 1211, 1244, 1254, 1257, 1300, 1353, 1354, 
               1364, 1372, 1394, 1395, 1401, 1410, 1427, 1454, 1477, 
               1497, 1517, 1532, 1549, 1551, 1575, 1578, 1611, 1624, 
               1629, 1637, 1650, 1655, 1697, 1700, 1713, 1736, 1745, 
               1762, 1790, 1800, 1815, 1824, 1827, 1866, 1872, 1886, 
               1887, 2053, 2072, 2106, 2115, 2128, 2160, 2217, 2343, 
               2383, 2388, 2413, 2458, 2465]

    ids, nd, ed = load_mock_network_ids(base, exclude_ids=exclude, verbose=True)
    print(ids[:10])  # first 10 usable ids

./CV/04_Postdoc_INSERM/05_NetworkMedicine/02_bootstrap_pipeline/compute_mock_distributions_clean.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Input: multilayer network + mock networks provided by M. De Domenico (CoMuNe Lab)

Étape 3 du pipeline :
    - Parcourt l'ensemble des réseaux mock (nodes_XXXX.csv / edges_XXXX.csv)
    - Calcule, pour chaque nœud d'un type donné (par ex. GO, disease, drug, symptom),
      son degré vers un autre type de nœud (souvent protein), dans chaque mock.
    - Construit une distribution de degrés par nœud (liste de valeurs sur tous les mocks).
    - Calcule des statistiques descriptives (n, mean, sd, min, max, median).
    - Exporte un TSV de distributions pour usage ultérieur (Z-scores, tests de normalité, p-values).

Ce module remplace les anciens scripts de type :
    - get_structural_degrees_distributions_across_mock_networks.py
    - covid_*_degree_to_proteins_distribution_in_mock_networks.tsv
    - toutes les variantes par type d'entité (GO, disease, drug, symptom, etc.)

Usage (CLI example):
    python compute_mock_distributions_clean.py \
      --mock_basepath Mock_networks_21Apr/ \
      --node_template_file COVID19_GDDS_nodes.csv \
      --focal_type GO \
      --neighbor_type protein \
      --directed True \
      --outfile results/covid_GO_degree_to_proteins_distribution_in_mock_networks.tsv
"""

import argparse
import csv
import os
from collections import defaultdict

import numpy as np
import networkx as nx

from io_networks_clean import load_mock_network_ids   # même dossier


# -------------------------------------------------------------------------
# Verbose print
# -------------------------------------------------------------------------

VERBOSE = True

def vprint(*args, **kwargs):
    if VERBOSE:
        print(*args, **kwargs)


# -------------------------------------------------------------------------
# I/O helpers
# -------------------------------------------------------------------------

def load_node_types(node_file):
    """
    Lit un fichier de noeuds (format CovMulNet19) et renvoie :
        node_types : dict {node_id: type}
    On suppose une structure du style :
        node_id, type, description, ...
    """
    node_types = {}
    with open(node_file, "r", encoding="utf-8") as f:
        reader = csv.reader(f)
        header = next(reader)
        for row in reader:
            node_id = row[0]
            node_type = row[1].strip()
            node_types[node_id] = node_type
    vprint(f"[load_node_types] Loaded {len(node_types)} nodes from {node_file}")
    return node_types


def load_edges_csv(edge_file, weighted=False):
    """
    Lit un fichier d'arêtes CSV.
    Format attendu minimal : source, target [, weight, ...]
    Si weighted=True, on suppose que la 3e colonne est un poids numérique.
    """
    edges = []
    with open(edge_file, "r", encoding="utf-8") as f:
        reader = csv.reader(f)
        header = next(reader)
        for row in reader:
            src = row[0]
            tgt = row[1]
            if weighted and len(row) > 2:
                try:
                    w = float(row[2])
                except ValueError:
                    w = 1.0
                edges.append((src, tgt, w))
            else:
                edges.append((src, tgt))
    vprint(f"[load_edges_csv] Loaded {len(edges)} edges from {edge_file}")
    return edges


# -------------------------------------------------------------------------
# Core: degree / strength per mock
# -------------------------------------------------------------------------

def compute_metric_for_mock(
    node_types,
    edges,
    focal_type="GO",
    neighbor_type="protein",
    directed=True,
    weighted=False
):
    """
    Calcule pour un mock donné :
      - pour chaque nœud de type focal_type (ex: GO),
      - son degré (ou strength) vers des voisins de type neighbor_type (ex: protein).

    Retourne : dict {focal_node: valeur}

    Remarque :
    - weighted=True => utilise l'attribut 'weight' des arêtes (somme des poids)
    - weighted=False => simple degree (compte du nombre de voisins)
    """
    # Sélection des sets de noeuds
    focal_nodes = {n for n, t in node_types.items() if t.lower() == focal_type.lower()}
    neighbor_nodes = {n for n, t in node_types.items() if t.lower() == neighbor_type.lower()}

    if directed:
        G = nx.DiGraph()
    else:
        G = nx.Graph()

    # Ajout des noeuds & arêtes
    G.add_nodes_from(node_types.keys())
    if weighted:
        # edges: (src, tgt, weight)
        for src, tgt, w in edges:
            G.add_edge(src, tgt, weight=w)
    else:
        # edges: (src, tgt)
        G.add_edges_from(edges)

    metric = {n: 0.0 for n in focal_nodes}

    for node in focal_nodes:
        if directed:
            neigh_iter = G.successors(node)
        else:
            neigh_iter = G.neighbors(node)

        if weighted:
            total_weight = 0.0
            for neigh in neigh_iter:
                if neigh in neighbor_nodes:
                    data = G.get_edge_data(node, neigh)
                    if data is None:
                        continue
                    # si plusieurs arêtes, on les somme
                    if isinstance(data, dict) and "weight" in data:
                        total_weight += data["weight"]
                    elif isinstance(data, dict):
                        # multigraph-like, ou dict de dict
                        # on somme tous les 'weight' rencontrés
                        for sub in data.values():
                            if isinstance(sub, dict) and "weight" in sub:
                                total_weight += sub["weight"]
            metric[node] = total_weight
        else:
            count = 0
            for neigh in neigh_iter:
                if neigh in neighbor_nodes:
                    count += 1
            metric[node] = float(count)

    return metric


# -------------------------------------------------------------------------
# Distribution aggregator
# -------------------------------------------------------------------------

def aggregate_distributions_over_mocks(
    mock_basepath,
    node_template_file,
    focal_type="GO",
    neighbor_type="protein",
    directed=True,
    weighted=False,
    exclude_ids=None,
    verbose=True
):
    """
    Parcourt l'ensemble des mock networks dans mock_basepath
    (avec fichiers nodes_XXXX.csv / edges_XXXX.csv),
    calcule le degré/strength pour chaque noeud focal_type,
    et construit une distribution sur tous les mocks.

    Retourne :
        distributions : dict {node_id: [valeurs sur chaque mock]}
    """
    global VERBOSE
    VERBOSE = verbose

    # Types de nœuds (on les prend depuis un template, supposé commun à tous les mocks)
    node_types = load_node_types(node_template_file)

    # Liste des mocks
    mock_ids, nodes_dict, edges_dict = load_mock_network_ids(
        mock_basepath,
        exclude_ids=exclude_ids,
        verbose=verbose
    )

    distributions = defaultdict(list)

    vprint(f"[aggregate_distributions] Processing {len(mock_ids)} mock networks...")

    for mock_id in mock_ids:
        node_file = nodes_dict.get(mock_id, None)
        edge_file = edges_dict.get(mock_id, None)
        if node_file is None or edge_file is None:
            vprint(f"[aggregate_distributions][WARNING] Missing files for mock {mock_id}, skipping.")
            continue

        vprint(f"[aggregate_distributions] Mock {mock_id}...")

        edges = load_edges_csv(edge_file, weighted=weighted)

        metric = compute_metric_for_mock(
            node_types=node_types,
            edges=edges,
            focal_type=focal_type,
            neighbor_type=neighbor_type,
            directed=directed,
            weighted=weighted
        )

        for n, val in metric.items():
            distributions[n].append(val)

    vprint(f"[aggregate_distributions] Done. {len(distributions)} focal nodes with distributions.")
    return distributions


# -------------------------------------------------------------------------
# Export
# -------------------------------------------------------------------------

def export_distributions(distributions, outfile):
    """
    Exporte les distributions mock par nœud dans un TSV.

    Format de sortie :
        node_id, n_mocks, mean, sd, min, max, median, values

    Où "values" est une liste des valeurs séparées par des virgules.
    """
    rows = []
    for node, vals in distributions.items():
        arr = np.array(vals, dtype=float)
        n = len(arr)
        if n == 0:
            continue
        mean = float(np.mean(arr))
        # ddof=1 pour sd de type "sample", si n>1
        sd = float(np.std(arr, ddof=1)) if n > 1 else 0.0
        vmin = float(np.min(arr))
        vmax = float(np.max(arr))
        med = float(np.median(arr))
        values_str = ",".join(str(v) for v in vals)
        rows.append((node, n, mean, sd, vmin, vmax, med, values_str))

    # tri optionnel par mean décroissant (utile en debug)
    rows.sort(key=lambda r: r[2], reverse=True)

    os.makedirs(os.path.dirname(outfile), exist_ok=True)
    with open(outfile, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f, delimiter="	")
        writer.writerow(["node_id", "n_mocks", "mean", "sd", "min", "max", "median", "values"])
        writer.writerows(rows)

    vprint(f"[export_distributions] Wrote {len(rows)} rows to {outfile}")


# -------------------------------------------------------------------------
# CLI
# -------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        description="Compute mock degree distributions for focal nodes."
    )

    parser.add_argument("--mock_basepath", required=True,
                        help="Path to folder containing nodes_XXXX.csv and edges_XXXX.csv")

    parser.add_argument("--node_template_file", required=True,
                        help="Node file (CSV) with node types, used as template for mocks")

    parser.add_argument("--focal_type", required=True,
                        help="Node type whose distribution we compute (e.g., GO, disease, drug, symptom)")

    parser.add_argument("--neighbor_type", default="protein",
                        help="Neighbor node type used to define degree (default: protein)")

    parser.add_argument("--directed", default="True",
                        help="True/False for directed graphs")

    parser.add_argument("--weighted", default="False",
                        help="True/False: use edge weights if available (3rd column)")

    parser.add_argument("--exclude_ids", default="",
                        help="Comma-separated list of mock IDs to exclude")

    parser.add_argument("--outfile", required=True,
                        help="Output TSV file for distributions")

    parser.add_argument("--verbose", default="True",
                        help="True/False for verbose output")

    args = parser.parse_args()

    global VERBOSE
    VERBOSE = (args.verbose.lower() == "true")
    directed = (args.directed.lower() == "true")
    weighted = (args.weighted.lower() == "true")

    if args.exclude_ids.strip():
        exclude_ids = [int(x) for x in args.exclude_ids.split(",") if x.strip().isdigit()]
    else:
        exclude_ids = None

    dists = aggregate_distributions_over_mocks(
        mock_basepath=args.mock_basepath,
        node_template_file=args.node_template_file,
        focal_type=args.focal_type,
        neighbor_type=args.neighbor_type,
        directed=directed,
        weighted=weighted,
        exclude_ids=exclude_ids,
        verbose=VERBOSE
    )

    export_distributions(dists, args.outfile)


if __name__ == "__main__":
    main()

./CV/04_Postdoc_INSERM/05_NetworkMedicine/02_bootstrap_pipeline/compute_pvalues_clean.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Input: multilayer network + mock networks provided by M. De Domenico (CoMuNe Lab)

Étape 5 du pipeline :
    - Lit les distributions mock (µ, σ, valeurs) de l’Étape 3
    - Lit les Z-scores de l’Étape 4
    - Pour chaque entité (GO, disease, drug, symptom, protein) :
        * teste la normalité (Shapiro, D’Agostino)
        * si normal -> p = 1 - erf(|Z| / sqrt(2))
        * sinon -> p = (sd^2) / (n * |x_covid - µ|)  (borne de Chebyshev utilisée dans les scripts originaux)
    - Gère les cas particuliers :
        * sd = 0  -> z = 0, p = 0.5 si normal, 1.0 sinon
        * p > 1   -> p tronqué à 1
        * peu de points -> p = NaN, isNormal = 'NaN'

Le module remplace :
    - test_normal_distributions.py
    - transform_Zscore_in_pvalue.py
    - convert_pval_manlio.py
    - convert_pval_manlio_adjust.py
    (logique intégrée ici de façon propre et modulaire).

Usage (CLI example):
    python compute_pvalues_clean.py \
      --focal_types GO,drug \
      --zscores_files results/GO_zscores.tsv,results/drug_zscores.tsv \
      --distribution_files results/covid_GO_degree_to_proteins_distribution_in_mock_networks.tsv,results/covid_drug_degree_to_proteins_distribution_in_mock_networks.tsv \
      --outdir results/pvalues/
"""

import argparse
import csv
import os
import math

import numpy as np
from scipy.stats import shapiro, normaltest
from math import erf

from compute_zscores_clean import load_mock_distribution_tsv  # même dossier


# ---------------------------------------------------------------------
# Verbose print
# ---------------------------------------------------------------------

VERBOSE = True

def vprint(*args, **kwargs):
    if VERBOSE:
        print(*args, **kwargs)


# ---------------------------------------------------------------------
# Load Z-score TSV
# ---------------------------------------------------------------------

def load_zscore_table(path):
    """
    Lecture d'un TSV de Z-scores produit par compute_zscores_clean.py

    Colonnes attendues :
        node_id, observed, mean, sd, min, max, median, zscore

    Retourne :
        ztab[node_id] = {
            'observed': ...,
            'mean': ...,
            'sd': ...,
            'min': ...,
            'max': ...,
            'median': ...,
            'zscore': ...
        }
    """
    ztab = {}
    vprint(f"[load_zscore_table] Loading {path}")
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.reader(f, delimiter="	")
        header = next(reader)
        for row in reader:
            node = row[0]
            obs = float(row[1])
            mean = float(row[2]) if row[2] not in ("", "None") else None
            sd   = float(row[3]) if row[3] not in ("", "None") else None
            vmin = float(row[4]) if row[4] not in ("", "None") else None
            vmax = float(row[5]) if row[5] not in ("", "None") else None
            med  = float(row[6]) if row[6] not in ("", "None") else None
            z    = row[7]
            z    = float(z) if z not in ("", "None") else None

            ztab[node] = {
                "observed": obs,
                "mean": mean,
                "sd": sd,
                "min": vmin,
                "max": vmax,
                "median": med,
                "zscore": z
            }
    return ztab


# ---------------------------------------------------------------------
# p-value computation (Shapiro + D’Agostino)
# ---------------------------------------------------------------------

def _pvalue_from_z_chebyshev(sd, n, observed, mean):
    """
    Variante Chebyshev, cohérente avec les anciens scripts :
        p = (sd^2) / (n * |observed - mean|)
    avec traitement spécial deg_covid == mean -> p = 0.5
    (mais ce cas est géré à l'extérieur).
    """
    if n <= 0:
        return float("nan")
    if observed == mean:
        return 0.5
    return (sd * sd) / (n * abs(observed - mean))


def _pvalues_for_node(distribution, z, observed, mean, sd,
                      alpha=0.05):
    """
    distribution : liste de valeurs (mock)
    z           : Z-score (float ou None)
    observed    : valeur dans le réseau réel
    mean, sd    : µ et σ de la distribution mock

    Retourne :
        {
          'isNormalShapiro': <str>,
          'p_shapiro': float,
          'isNormalDagostino': <str>,
          'p_dagostino': float
        }
    suivant la logique originale :
        - test Shapiro si n > 3
        - test D’Agostino si n > 8
        - normal -> erf
        - non-normal -> Chebyshev
    """
    n = len(distribution)

    # Valeurs par défaut
    isN_S = "NaN"
    p_S = float("nan")
    isN_D = "NaN"
    p_D = float("nan")

    # Shapiro
    if n > 3:
        stat, p = shapiro(distribution)
        if p < alpha:
            isNormalShapiro = False
        else:
            isNormalShapiro = True
        isN_S = str(isNormalShapiro)

        if isNormalShapiro:
            # cas normal
            if z is None or math.isnan(z):
                p_S = 0.5
            else:
                p_S = 1.0 - erf(abs(z) / math.sqrt(2.0))
        else:
            # Chebyshev
            if (observed == mean):
                p_S = 0.5
            else:
                p_S = _pvalue_from_z_chebyshev(sd, n, observed, mean)
    else:
        p_S = float("nan")
        isN_S = "NaN"

    # D’Agostino
    if n > 8:
        stat, p = normaltest(distribution)
        if p < alpha:
            isNormalDagostino = False
        else:
            isNormalDagostino = True
        isN_D = str(isNormalDagostino)

        if isNormalDagostino:
            if z is None or math.isnan(z):
                p_D = 0.5
            else:
                p_D = 1.0 - erf(abs(z) / math.sqrt(2.0))
        else:
            if (observed == mean):
                p_D = 0.5
            else:
                p_D = _pvalue_from_z_chebyshev(sd, n, observed, mean)
    else:
        p_D = float("nan")
        isN_D = "NaN"

    return {
        "isNormalShapiro": isN_S,
        "p_shapiro": p_S,
        "isNormalDagostino": isN_D,
        "p_dagostino": p_D
    }


def _adjust_special_cases(sd, z, isN_S, p_S, isN_D, p_D):
    """
    Implémente la logique de convert_pval_manlio_adjust.py :
    - si sd == 0 → z = 0, p_shap/p_dago = 0.5 (si normal) ou 1 (si non-normal)
    - p > 1 → tronqué à 1
    """
    # sd == 0
    if sd == 0.0:
        z = 0.0
        # Shapiro
        if isN_S == "True":
            p_S = 0.5
        elif isN_S == "False":
            p_S = 1.0
        # D’Agostino
        if isN_D == "True":
            p_D = 0.5
        elif isN_D == "False":
            p_D = 1.0

    # p > 1 → tronquer
    if p_S is not None and not math.isnan(p_S) and p_S > 1.0:
        p_S = 1.0
    if p_D is not None and not math.isnan(p_D) and p_D > 1.0:
        p_D = 1.0

    return z, p_S, p_D


# ---------------------------------------------------------------------
# Process focal type
# ---------------------------------------------------------------------

def compute_pvalues_for_focal_type(zscore_file, distribution_file, outfile, alpha=0.05):
    """
    Un focal_type (GO, disease, drug, symptom, etc.) :

    - lit distributions mock (mean, sd, val list,...)
    - lit observed + zscores
    - calcule p_shapiro & p_dagostino
    - applique l'ajustement sd==0 & p>1
    - écrit outfile TSV
    """
    ztab = load_zscore_table(zscore_file)
    dist_stats = load_mock_distribution_tsv(distribution_file)

    rows = []

    for node, zd in ztab.items():
        obs = zd["observed"]
        mean = zd["mean"]
        sd = zd["sd"]
        z = zd["zscore"]

        if node not in dist_stats or mean is None or sd is None:
            # manque d’info mock → tout NaN
            sample_size = dist_stats.get(node, {}).get("n", 0)
            vmin = zd["min"]
            vmax = zd["max"]
            med = zd["median"]
            isN_S = "NaN"
            p_S = float("nan")
            isN_D = "NaN"
            p_D = float("nan")
        else:
            dstat = dist_stats[node]
            sample_size = dstat["n"]
            vmin = dstat["min"]
            vmax = dstat["max"]
            med = dstat["median"]
            distribution = dstat["values"]

            res = _pvalues_for_node(
                distribution=distribution,
                z=z,
                observed=obs,
                mean=mean,
                sd=sd,
                alpha=alpha
            )

            isN_S = res["isNormalShapiro"]
            p_S = res["p_shapiro"]
            isN_D = res["isNormalDagostino"]
            p_D = res["p_dagostino"]

            z, p_S, p_D = _adjust_special_cases(sd, z, isN_S, p_S, isN_D, p_D)

        # ligne finale
        rows.append([
            node,
            sample_size,
            vmin,
            vmax,
            mean,
            med,
            sd,
            z,
            obs,
            isN_S,
            p_S,
            isN_D,
            p_D
        ])

    # tri par p_shapiro croissant (les plus significatifs en haut)
    def sort_key(row):
        pS = row[10]
        if pS is None or (isinstance(pS, float) and math.isnan(pS)):
            return 1e9
        return pS

    rows.sort(key=sort_key)

    os.makedirs(os.path.dirname(outfile), exist_ok=True)
    with open(outfile, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f, delimiter="	")
        writer.writerow([
            "node_id",
            "sample_size",
            "min",
            "max",
            "mean",
            "median",
            "sd",
            "zscore",
            "observed",
            "isNormalShapiro",
            "p_shapiro",
            "isNormalDagostino",
            "p_dagostino"
        ])
        writer.writerows(rows)

    vprint(f"[compute_pvalues_for_focal_type] Wrote {len(rows)} rows to {outfile}")


# ---------------------------------------------------------------------
# Multi-focal CLI
# ---------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="Compute p-values from Z-scores + mock distributions (Shapiro + D’Agostino, erf/Chebyshev).")

    parser.add_argument("--focal_types",
                        required=True,
                        help="Comma-separated list of focal types (e.g. GO,disease,drug)")

    parser.add_argument("--zscores_files",
                        required=True,
                        help="Comma-separated list of Z-score TSV files, "
                             "same order as focal_types")

    parser.add_argument("--distribution_files",
                        required=True,
                        help="Comma-separated list of mock-distribution TSVs, "
                             "same order as focal_types")

    parser.add_argument("--outdir", required=True,
                        help="Output folder for p-value tables")

    parser.add_argument("--alpha", default="0.05",
                        help="Alpha for normality tests (default 0.05)")

    parser.add_argument("--verbose", default="True",
                        help="True/False")

    args = parser.parse_args()

    global VERBOSE
    VERBOSE = (args.verbose.lower() == "true")
    alpha = float(args.alpha)

    focal_types = [x.strip() for x in args.focal_types.split(",")]
    z_paths = [x.strip() for x in args.zscores_files.split(",")]
    dist_paths = [x.strip() for x in args.distribution_files.split(",")]

    if len(focal_types) != len(z_paths) or len(focal_types) != len(dist_paths):
        raise ValueError("Mismatch in lengths: focal_types, zscores_files, distribution_files")

    z_dict = dict(zip(focal_types, z_paths))
    dist_dict = dict(zip(focal_types, dist_paths))

    for ftype in focal_types:
        vprint("
===================================================")
        vprint(f"[main] Focal type: {ftype}")
        vprint("===================================================
")

        zfile = z_dict[ftype]
        dfile = dist_dict[ftype]

        outfile = os.path.join(args.outdir, f"{ftype}_pvalues.tsv")
        compute_pvalues_for_focal_type(zfile, dfile, outfile, alpha=alpha)


if __name__ == "__main__":
    main()

./CV/04_Postdoc_INSERM/05_NetworkMedicine/02_bootstrap_pipeline/compute_zscores_clean.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Input: multilayer network + mock networks provided by M. De Domenico (CoMuNe Lab)

Étape 4 du pipeline :
    - Lit les distributions mock (µ, σ, …) générées à l’Étape 3
    - Lit les valeurs observées (de l’Étape 2 : *withZeros.tsv*)
    - Pour chaque entité (GO, disease, drug, symptom, protein):
           Z = (observé - µ) / σ
    - Supporte n types d’entités dans un même run (multi-focal)
    - Exporte un TSV propre contenant :
           node_id, observed, mean, sd, min, max, median, zscore
    - Laisse Étape 5 traiter la conversion en p-values

Le module remplace :
    - toutes les variantes de fichiers *_NormTest_alpha_0.05.tsv
    - les anciens scripts de calcul de Z-scores dans /bootstrap/
"""

import argparse
import csv
import os
import numpy as np

# ---------------------------------------------------------------------
# Verbose print
# ---------------------------------------------------------------------

VERBOSE = True
def vprint(*args, **kwargs):
    if VERBOSE:
        print(*args, **kwargs)


# ---------------------------------------------------------------------
# Reading helpers : distributions & observed
# ---------------------------------------------------------------------

def load_mock_distribution_tsv(path):
    """
    Lecture du TSV créé par compute_mock_distributions_clean.py

    Colonnes attendues :
        node_id, n_mocks, mean, sd, min, max, median, values

    Retourne :
        stats[node_id] = {
            'mean': ...,
            'sd': ...,
            'min': ...,
            'max': ...,
            'median': ...,
            'n': ...,
            'values': [...]
        }
    """
    stats = {}
    vprint(f"[load_mock_distribution] Loading {path}")
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.reader(f, delimiter="	")
        header = next(reader)

        for row in reader:
            node = row[0]
            n = int(row[1])
            mean = float(row[2])
            sd = float(row[3])
            vmin = float(row[4])
            vmax = float(row[5])
            med = float(row[6])
            vals = [float(x) for x in row[7].split(",")]

            stats[node] = {
                "n": n,
                "mean": mean,
                "sd": sd,
                "min": vmin,
                "max": vmax,
                "median": med,
                "values": vals
            }
    return stats


def load_observed_withzeros(path):
    """
    Lecture du observed_withZeros TSV créé à l’Étape 2.
    Format attendu :
        source_node, degree, relative_degree

    On lit seulement "degree" comme valeur observée brute.
    """
    obs = {}
    vprint(f"[load_observed] Loading {path}")
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.reader(f, delimiter="	")
        header = next(reader)
        for row in reader:
            node = row[0]
            deg = float(row[1])
            obs[node] = deg
    return obs


# ---------------------------------------------------------------------
# Z-score computation
# ---------------------------------------------------------------------

def compute_zscores_for_focal_type(
    observed_dict,
    dist_stats_dict
):
    """
    observed_dict :  {node: observed_degree}
    dist_stats_dict : {node: {'mean':..., 'sd':..., ...}}

    Retourne :
        result[node] = {
            'observed': ...,
            'mean': ...,
            'sd': ...,
            'min': ...,
            'max': ...,
            'median': ...,
            'zscore': ...
        }
    """

    result = {}

    for node, obs_val in observed_dict.items():
        if node not in dist_stats_dict:
            # le noeud n'existe pas dans les mocks → sd=0 → pas de Z
            result[node] = {
                "observed": obs_val,
                "mean": None,
                "sd": None,
                "min": None,
                "max": None,
                "median": None,
                "zscore": None
            }
            continue

        stats = dist_stats_dict[node]
        mu = stats["mean"]
        sd = stats["sd"]

        if sd == 0 or sd is None:
            z = None
        else:
            z = (obs_val - mu) / sd

        result[node] = {
            "observed": obs_val,
            "mean": mu,
            "sd": sd,
            "min": stats["min"],
            "max": stats["max"],
            "median": stats["median"],
            "zscore": z
        }

    return result


# ---------------------------------------------------------------------
# Export
# ---------------------------------------------------------------------

def export_zscore_table(zdict, outfile):
    """
    Exporte un TSV propre :
        node_id, observed, mean, sd, min, max, median, zscore
    """
    rows = []
    for node, d in zdict.items():
        rows.append([
            node,
            d["observed"],
            d["mean"],
            d["sd"],
            d["min"],
            d["max"],
            d["median"],
            d["zscore"]
        ])

    # tri optionnel par z-score décroissant (zscore None en bas)
    def sort_key(row):
        z = row[7]
        if z is None:
            return -999999
        return z

    rows.sort(key=sort_key, reverse=True)

    os.makedirs(os.path.dirname(outfile), exist_ok=True)
    with open(outfile, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f, delimiter="	")
        writer.writerow(["node_id", "observed", "mean", "sd", "min", "max", "median", "zscore"])
        writer.writerows(rows)

    vprint(f"[export_zscores] Wrote {len(rows)} rows to {outfile}")


# ---------------------------------------------------------------------
# Multiple focal types in one run
# ---------------------------------------------------------------------

def process_multiple_focal_types(focal_types, observed_files, distribution_files, outdir):
    """
    focal_types : ["GO", "disease", "drug", ...]
    observed_files :  {focal_type: path_to_observed_withZeros}
    distribution_files : {focal_type: path_to_mock_distribution}

    outdir : dossier où écrire les Z-scores.

    Pour chaque type, on calcule et exporte :
        outdir/{focal_type}_zscores.tsv
    """

    for ftype in focal_types:
        vprint("
===================================================")
        vprint(f"[process] Focal type: {ftype}")
        vprint("===================================================
")

        obs_file = observed_files.get(ftype)
        dist_file = distribution_files.get(ftype)

        if obs_file is None or dist_file is None:
            vprint(f"[WARNING] Missing file for {ftype}, skipping.")
            continue

        obs = load_observed_withzeros(obs_file)
        dstat = load_mock_distribution_tsv(dist_file)

        zdict = compute_zscores_for_focal_type(obs, dstat)

        outfile = os.path.join(outdir, f"{ftype}_zscores.tsv")
        export_zscore_table(zdict, outfile)


# ---------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="Compute Z-scores from observed + mock distributions.")

    parser.add_argument("--focal_types",
                        required=True,
                        help="Comma-separated list of focal types (e.g. GO,disease,drug)")

    parser.add_argument("--observed_files",
                        required=True,
                        help="Comma-separated list of observed_withZeros files, "
                             "same order as focal_types")

    parser.add_argument("--distribution_files",
                        required=True,
                        help="Comma-separated list of mock-distribution TSVs, "
                             "same order as focal_types")

    parser.add_argument("--outdir", required=True,
                        help="Output folder for Z-score tables")

    parser.add_argument("--verbose", default="True",
                        help="True/False")

    args = parser.parse_args()

    global VERBOSE
    VERBOSE = (args.verbose.lower() == "true")

    focal_types = [x.strip() for x in args.focal_types.split(",")]
    obs_paths = [x.strip() for x in args.observed_files.split(",")]
    dist_paths = [x.strip() for x in args.distribution_files.split(",")]

    if len(focal_types) != len(obs_paths) or len(focal_types) != len(dist_paths):
        raise ValueError("Mismatch in lengths: focal_types, observed_files, distribution_files")

    observed_dict = dict(zip(focal_types, obs_paths))
    dist_dict = dict(zip(focal_types, dist_paths))

    process_multiple_focal_types(
        focal_types=focal_types,
        observed_files=observed_dict,
        distribution_files=dist_dict,
        outdir=args.outdir
    )


if __name__ == "__main__":
    main()

./CV/04_Postdoc_INSERM/05_NetworkMedicine/02_bootstrap_pipeline/ranking_and_plots_clean.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Input: multilayer network + mock networks provided by M. De Domenico (CoMuNe Lab)

Étape 6 du pipeline :
    - Lit les fichiers p-values (un par focal_type)
    - Classe les entités selon :
          * p_shapiro
          * p_dagostino
          * zscore (descendant)
    - Génère :
          * un fichier TSV "topN"
          * un fichier TSV complet trié
    - Génère des barplots horizontaux (png)
          * abs values: absolute degrees / observed
          * rel values: zscore ou -log10(pvalue), selon option

Ce module remplace entièrement :
    - random-network-analysis-*-ranking.py
    - random-network-analysis-*-filtering.py
    - pulling_results.py / pulling_results_undirected.py
    - plots.py
"""

import argparse
import csv
import os
import math

import numpy as np
import matplotlib.pyplot as plt


# ---------------------------------------------------------------------
# Verbose print
# ---------------------------------------------------------------------

VERBOSE = True
def vprint(*args, **kwargs):
    if VERBOSE:
        print(*args, **kwargs)



# ---------------------------------------------------------------------
# I/O helpers
# ---------------------------------------------------------------------

def load_pvalues(path):
    """
    Charge le TSV produit par compute_pvalues_clean.py.
    Colonnes :
        node_id, sample_size, min, max, mean, median, sd,
        zscore, observed, isNormalShapiro, p_shapiro,
        isNormalDagostino, p_dagostino
    """
    vprint(f"[load_pvalues] Loading {path}")
    table = {}
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.reader(f, delimiter="	")
        header = next(reader)
        for row in reader:
            node = row[0]
            sample_size = int(row[1])
            vmin = float(row[2]) if row[2] != "None" else None
            vmax = float(row[3]) if row[3] != "None" else None
            mean = float(row[4]) if row[4] != "None" else None
            med  = float(row[5]) if row[5] != "None" else None
            sd   = float(row[6]) if row[6] != "None" else None
            zscore = float(row[7]) if row[7] != "None" else None
            observed = float(row[8]) if row[8] != "None" else None
            isN_S = row[9]
            p_S = float(row[10]) if row[10] not in ("None", "") else None
            isN_D = row[11]
            p_D = float(row[12]) if row[12] not in ("None", "") else None

            table[node] = {
                "sample_size": sample_size,
                "min": vmin,
                "max": vmax,
                "mean": mean,
                "median": med,
                "sd": sd,
                "observed": observed,
                "zscore": zscore,
                "isNormalShapiro": isN_S,
                "p_shapiro": p_S,
                "isNormalDagostino": isN_D,
                "p_dagostino": p_D
            }
    return table



# ---------------------------------------------------------------------
# Ranking
# ---------------------------------------------------------------------

def rank_table(table, metric="p_shapiro"):
    """
    Classe les entités selon un metric donné (p_shapiro / p_dagostino / zscore).
    """
    vprint(f"[rank_table] Ranking by {metric}")

    def sortkey(node):
        val = table[node][metric]
        if val is None or (isinstance(val, float) and math.isnan(val)):
            return float("inf")   # p-values None = en bas
        return val

    reverse = False  # p-values → tri ascendant
    if metric == "zscore":
        reverse = True   # zscore → tri descendant

    ranked = sorted(table.keys(), key=sortkey, reverse=reverse)
    return ranked



def write_ranked_tsv(table, ranked_nodes, outfile):
    """
    Construit un TSV trié.
    """
    vprint(f"[write_ranked_tsv] Writing {outfile}")

    os.makedirs(os.path.dirname(outfile), exist_ok=True)
    with open(outfile, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f, delimiter="	")
        writer.writerow([
            "node_id", "observed", "mean", "sd", "median",
            "zscore", "p_shapiro", "p_dagostino",
            "isNormalShapiro", "isNormalDagostino"
        ])
        for node in ranked_nodes:
            d = table[node]
            writer.writerow([
                node,
                d["observed"],
                d["mean"],
                d["sd"],
                d["median"],
                d["zscore"],
                d["p_shapiro"],
                d["p_dagostino"],
                d["isNormalShapiro"],
                d["isNormalDagostino"]
            ])



def write_topN(ranked_nodes, table, N, outfile):
    """
    Exporte le top N sous forme TSV.
    """
    vprint(f"[write_topN] Writing top{N}: {outfile}")

    os.makedirs(os.path.dirname(outfile), exist_ok=True)
    with open(outfile, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f, delimiter="	")
        writer.writerow([
            "rank", "node_id", "observed", "zscore", "p_shapiro", "p_dagostino"
        ])
        for i, node in enumerate(ranked_nodes[:N], start=1):
            d = table[node]
            writer.writerow([
                i,
                node,
                d["observed"],
                d["zscore"],
                d["p_shapiro"],
                d["p_dagostino"]
            ])



# ---------------------------------------------------------------------
# Plots
# ---------------------------------------------------------------------

def plot_topN(
    table,
    ranked_nodes,
    N,
    outfile,
    value_field="zscore",   # "zscore", "observed", "-log10(p_shapiro)"
    title="Top entities"
):
    """
    Génère un barplot horizontal simple.
    Compatible avec :
      - zscore
      - observed
      - -log10(p_shapiro)
    """
    vprint(f"[plot_topN] Plot to {outfile}")

    nodes = ranked_nodes[:N]
    values = []

    if value_field == "-log10(p_shapiro)":
        for n in nodes:
            p = table[n]["p_shapiro"]
            if p is None or p <= 0:
                values.append(0)
            else:
                values.append(-math.log10(p))
    else:
        for n in nodes:
            v = table[n][value_field]
            values.append(v if v is not None else 0)

    plt.figure(figsize=(8, max(4, N * 0.35)))
    y = np.arange(len(nodes))
    plt.barh(y, values, color="grey", edgecolor="black")

    plt.yticks(y, nodes)
    plt.gca().invert_yaxis()
    plt.title(title)
    plt.xlabel(value_field)
    plt.tight_layout()

    os.makedirs(os.path.dirname(outfile), exist_ok=True)
    plt.savefig(outfile, dpi=300)
    plt.close()



# ---------------------------------------------------------------------
# Multi-focal orchestrator
# ---------------------------------------------------------------------

def process_focal_types(
    focal_types,
    pvalue_files,
    outdir,
    topN=20,
    ranking_metric="p_shapiro",
    plot_metric="zscore"
):
    for ftype in focal_types:
        vprint("
===================================================")
        vprint(f"[process_focal_types] {ftype}")
        vprint("===================================================
")

        pfile = pvalue_files[ftype]
        table = load_pvalues(pfile)

        ranked_nodes = rank_table(table, metric=ranking_metric)

        # fichiers TSV
        ranked_out = os.path.join(outdir, f"{ftype}_ranking.tsv")
        write_ranked_tsv(table, ranked_nodes, ranked_out)

        top_out = os.path.join(outdir, f"{ftype}_top{topN}.tsv")
        write_topN(ranked_nodes, table, topN, top_out)

        # plot
        plot_out = os.path.join(outdir, f"{ftype}_top{topN}_{plot_metric}.png")
        plot_title = f"Top {topN} {ftype} by {plot_metric}"
        plot_topN(table, ranked_nodes, topN, plot_out, value_field=plot_metric, title=plot_title)



# ---------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="Ranking + plots from p-values.")

    parser.add_argument("--focal_types",
                        required=True,
                        help="Comma-separated list of focal types (GO,disease,drug,...)")

    parser.add_argument("--pvalue_files",
                        required=True,
                        help="Comma-separated list of p-value TSVs (same order as focal_types)")

    parser.add_argument("--outdir", required=True,
                        help="Directory for ranking TSVs + plots")

    parser.add_argument("--topN", default="20", help="Top N entities")
    parser.add_argument("--ranking_metric", default="p_shapiro",
                        help="Metric: p_shapiro | p_dagostino | zscore")
    parser.add_argument("--plot_metric", default="zscore",
                        help="Metric for plots: zscore | observed | -log10(p_shapiro)")
    parser.add_argument("--verbose", default="True")

    args = parser.parse_args()

    global VERBOSE
    VERBOSE = (args.verbose.lower() == "true")

    focal_types = [x.strip() for x in args.focal_types.split(",")]
    pfiles = [x.strip() for x in args.pvalue_files.split(",")]

    if len(focal_types) != len(pfiles):
        raise ValueError("Mismatch: focal_types and pvalue_files length differ.")

    p_dict = dict(zip(focal_types, pfiles))

    process_focal_types(
        focal_types=focal_types,
        pvalue_files=p_dict,
        outdir=args.outdir,
        topN=int(args.topN),
        ranking_metric=args.ranking_metric,
        plot_metric=args.plot_metric
    )


if __name__ == "__main__":
    main()

./CV/04_Postdoc_INSERM/06 _Sarscov2Interactome/analysis.py
#!/usr/bin/env python
from __future__ import annotations

import argparse
import logging

from src.logging_utils import setup_logging
from src.interactors import compute_all_orders_for_all_viruses
from src.tables import build_gene_lists_from_interactors, build_gene_virus_table
from src.enrichment_wrapper import run_r_enrichment


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Systemic effects of SARS-CoV-2 on host cellular functions – pipeline"
    )

    subparsers = parser.add_subparsers(dest="command", required=True)

    # 1. network → interactors
    p_net = subparsers.add_parser("compute_interactors", help="Compute order-k interactors")
    p_net.add_argument("--max-order", type=int, default=2)

    # 2. gene lists for R
    p_genelist = subparsers.add_parser("gene_lists", help="Build gene lists for enrichment")
    p_genelist.add_argument("--range-mode", type=str, default="direct_and_second")

    # 3. gene-virus table
    p_table = subparsers.add_parser("gene_virus_table", help="Build gene-virus table")
    p_table.add_argument("--range-mode", type=str, default="direct_and_second")

    # 4. enrichment via R
    p_enrich = subparsers.add_parser("enrich_reactome", help="Run Reactome enrichment in R")
    p_enrich.add_argument("--range-mode", type=str, default="direct_and_second")
    p_enrich.add_argument("--script-name", type=str, default="enrich_reactome_compareCluster.R")

    return parser.parse_args()


def main() -> None:
    setup_logging()
    logger = logging.getLogger("covid_pipeline")

    args = parse_args()

    if args.command == "compute_interactors":
        logger.info("Step: compute_interactors (max_order=%d)", args.max_order)
        compute_all_orders_for_all_viruses(max_order=args.max_order)

    elif args.command == "gene_lists":
        logger.info("Step: gene_lists (range_mode=%s)", args.range_mode)
        build_gene_lists_from_interactors(range_mode=args.range_mode)

    elif args.command == "gene_virus_table":
        logger.info("Step: gene_virus_table (range_mode=%s)", args.range_mode)
        build_gene_virus_table(range_mode=args.range_mode)

    elif args.command == "enrich_reactome":
        logger.info(
            "Step: enrich_reactome (range_mode=%s, script=%s)",
            args.range_mode, args.script_name
        )
        gene_list_file = build_gene_lists_from_interactors(range_mode=args.range_mode)
        run_r_enrichment(gene_list_file, script_name=args.script_name)

    else:
        raise RuntimeError(f"Unknown command: {args.command}")


if __name__ == "__main__":
    main()

./CV/04_Postdoc_INSERM/06 _Sarscov2Interactome/src/config.py
from pathlib import Path

# Racine du projet (fichier analysis.py à la racine)
PROJECT_ROOT = Path(__file__).resolve().parents[1]

DATA_RAW = PROJECT_ROOT / "data" / "raw" / "Virus_host_interactomes_thresh25" / "thresh_0.25"
DATA_INTERMEDIATE = PROJECT_ROOT / "data" / "intermediate"
DATA_RESULTS = PROJECT_ROOT / "data" / "results"
R_SCRIPTS_DIR = PROJECT_ROOT / "r"

INTERACTORS_DIR = DATA_INTERMEDIATE / "interactors"
GENE_LISTS_DIR = DATA_INTERMEDIATE / "gene_lists"
ENRICHMENT_DIR = DATA_INTERMEDIATE / "enrichment"

TABLES_DIR = DATA_RESULTS / "tables"
FIGURES_DIR = DATA_RESULTS / "figures"

# Création des dossiers au besoin
for p in [
    DATA_RAW,
    DATA_INTERMEDIATE,
    DATA_RESULTS,
    INTERACTORS_DIR,
    GENE_LISTS_DIR,
    ENRICHMENT_DIR,
    TABLES_DIR,
    FIGURES_DIR,
]:
    p.mkdir(parents=True, exist_ok=True)

./CV/04_Postdoc_INSERM/06 _Sarscov2Interactome/src/enrichment_wrapper.py
from __future__ import annotations
from pathlib import Path
import subprocess
import logging

from .config import R_SCRIPTS_DIR, ENRICHMENT_DIR

logger = logging.getLogger(__name__)


def run_r_enrichment(
    gene_list_file: Path,
    script_name: str = "enrich_reactome_compareCluster.R",
    output_prefix: str = "enrichPathway",
) -> Path:
    """
    Appelle un script R qui lit un fichier genes_list_*.txt,
    fait les mappings + compareCluster, et écrit un TSV dans ENRICHMENT_DIR.
    """
    script_path = R_SCRIPTS_DIR / script_name
    out_tsv = ENRICHMENT_DIR / f"{output_prefix}.tsv"

    cmd = [
        "Rscript",
        str(script_path),
        str(gene_list_file),
        str(out_tsv),
    ]
    logger.info("Running Rscript: %s", " ".join(cmd))
    subprocess.run(cmd, check=True)

    logger.info("R enrichment results written to %s", out_tsv)
    return out_tsv

./CV/04_Postdoc_INSERM/06 _Sarscov2Interactome/src/interactors.py
from pathlib import Path
from typing import Dict, List
import logging

from .config import DATA_RAW, INTERACTORS_DIR
from .io_utils import list_virus_dirs, load_nodes_edges
from .network import build_graph, compute_order_interactors

logger = logging.getLogger(__name__)


def compute_all_orders_for_all_viruses(
    max_order: int = 2,
    nodes_filename: str = "nodes.csv",
    edges_filename: str = "edges.csv",
) -> None:
    """
    Pour chaque virus:
      - construit le graphe
      - calcule les interacteurs d'ordres 1..max_order
      - écrit des fichiers texte dans INTERACTORS_DIR/<virus>/ :
           direct_interactors.txt
           only_second_range_interactors.txt
           only_Third_range_interactors.txt
           ...
    """
    virus_dirs = list_virus_dirs(DATA_RAW)
    logger.info("Found %d virus folders", len(virus_dirs))

    for virus_dir in virus_dirs:
        virus_name = virus_dir.name
        logger.info("Processing virus %s", virus_name)

        descr, host_nodes, virus_nodes, edges = load_nodes_edges(
            virus_dir,
            nodes_filename=nodes_filename,
            edges_filename=edges_filename,
            delimiter_nodes=",",
            delimiter_edges=",",
            has_header=False,
        )

        G = build_graph(descr.keys(), edges)
        logger.info(
            "Graph for %s: n_nodes=%d, n_edges=%d",
            virus_name, G.number_of_nodes(), G.number_of_edges()
        )

        order_to_nodes = compute_order_interactors(
            G,
            virus_nodes=virus_nodes.keys(),
            host_nodes=set(host_nodes.keys()),
            max_order=max_order,
        )

        out_dir = INTERACTORS_DIR / virus_name
        out_dir.mkdir(parents=True, exist_ok=True)

        # ordre 1 : direct_interactors
        direct_path = out_dir / "direct_interactors.txt"
        _write_interactors_file(
            direct_path,
            virus_name,
            order_to_nodes.get(1, set()),
            descr,
            header_label="Direct interactors (order 1)",
        )

        # ordres suivants : only_second_range_interactors, etc.
        for order in range(2, max_order + 1):
            filename = {
                2: "only_second_range_interactors.txt",
                3: "only_Third_range_interactors.txt",
                4: "only_Fourth_range_interactors.txt",
                5: "only_Fifth_range_interactors.txt",
            }.get(order, f"only_{order}th_range_interactors.txt")
            out_path = out_dir / filename
            _write_interactors_file(
                out_path,
                virus_name,
                order_to_nodes.get(order, set()),
                descr,
                header_label=f"Only order {order} interactors",
            )


def _write_interactors_file(
    path: Path,
    virus_name: str,
    protein_ids: set[str],
    descr_dict: Dict[str, str],
    header_label: str,
) -> None:
    """
    Format inspiré des scripts legacy :
     - première ligne = virus_name
     - lignes suivantes = "protein_id<TAB>gene_symbol"
    """
    with path.open("w", encoding="utf-8") as f:
        f.write(f"{virus_name}
")
        f.write(f"# {header_label}
")
        for pid in sorted(protein_ids):
            gene = descr_dict.get(pid, "NA")
            f.write(f"{pid}	{gene}
")
    logger.info("Wrote %d interactors to %s", len(protein_ids), path)

./CV/04_Postdoc_INSERM/06 _Sarscov2Interactome/src/io_utils.py
from pathlib import Path
from typing import Dict, List, Tuple
import csv

def list_virus_dirs(rootdir: Path) -> List[Path]:
    """Retourne la liste des sous-dossiers (un par virus)."""
    return [
        d for d in rootdir.iterdir()
        if d.is_dir()
    ]

def load_nodes_edges(
    virus_dir: Path,
    nodes_filename: str = "nodes.csv",
    edges_filename: str = "edges.csv",
    delimiter_nodes: str = ",",
    delimiter_edges: str = ",",
    has_header: bool = False,
) -> Tuple[Dict[str, str], Dict[str, str], Dict[str, str], list[tuple[str, str]]]:
    """
    Lit nodes.csv et edges.csv et renvoie :
      - descr_dict_nodes: id -> nom
      - host_nodes: id -> nom (type 1)
      - virus_nodes: id -> nom (type 0)
      - edges: liste de tuples (u, v)
    """
    nodes_path = virus_dir / nodes_filename
    edges_path = virus_dir / edges_filename

    descr_dict_nodes: Dict[str, str] = {}
    host_nodes: Dict[str, str] = {}
    virus_nodes: Dict[str, str] = {}

    with nodes_path.open() as f:
        reader = csv.reader(f, delimiter=delimiter_nodes)
        if has_header:
            next(reader)
        for row in reader:
            if not row:
                continue
            node_id = row[0]
            node_name = row[1]
            node_type = row[2].strip()
            descr_dict_nodes[node_id] = node_name
            if node_type in {"1", "1.0"}:
                host_nodes[node_id] = node_name
            elif node_type in {"0", "0.0"}:
                virus_nodes[node_id] = node_name
            else:
                # on ignore les noeuds inconnus
                continue

    edges: list[tuple[str, str]] = []
    with edges_path.open() as f:
        reader = csv.reader(f, delimiter=delimiter_edges)
        if has_header:
            next(reader)
        for row in reader:
            if len(row) < 2:
                continue
            u, v = row[0], row[1]
            edges.append((u, v))

    return descr_dict_nodes, host_nodes, virus_nodes, edges

./CV/04_Postdoc_INSERM/06 _Sarscov2Interactome/src/logging_utils.py
import logging
from pathlib import Path

def setup_logging(log_file: Path | None = None) -> None:
    log_format = "%(asctime)s - %(levelname)s - %(name)s - %(message)s"
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[logging.StreamHandler()]
    )
    if log_file is not None:
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(logging.Formatter(log_format))
        logging.getLogger().addHandler(file_handler)

./CV/04_Postdoc_INSERM/06 _Sarscov2Interactome/src/network.py
from collections import deque
from typing import Dict, Iterable, Set
import networkx as nx

def build_graph(
    node_ids: Iterable[str],
    edges: Iterable[tuple[str, str]],
) -> nx.Graph:
    G = nx.Graph()
    G.add_nodes_from(node_ids)
    G.add_edges_from(edges)
    return G

def compute_order_interactors(
    G: nx.Graph,
    virus_nodes: Iterable[str],
    host_nodes: set[str],
    max_order: int,
) -> Dict[int, Set[str]]:
    """
    Calcule les interacteurs d'ordre 1..max_order autour des virus_nodes.
    Retourne un dict: order -> set(node_ids).
    """
    virus_nodes = set(virus_nodes)
    visited = set(virus_nodes)
    frontier = set(virus_nodes)

    results: Dict[int, Set[str]] = {}

    for order in range(1, max_order + 1):
        next_frontier: set[str] = set()
        for u in frontier:
            for v in G.neighbors(u):
                if v not in visited:
                    visited.add(v)
                    next_frontier.add(v)
        # on ne garde que les noeuds humains pour le reporting
        results[order] = {n for n in next_frontier if n in host_nodes}
        frontier = next_frontier

    return results

./CV/04_Postdoc_INSERM/06 _Sarscov2Interactome/src/tables.py
from __future__ import annotations
from pathlib import Path
from typing import Dict, List
import logging

import pandas as pd

from .config import INTERACTORS_DIR, GENE_LISTS_DIR, TABLES_DIR
from .io_utils import list_virus_dirs

logger = logging.getLogger(__name__)


def _read_interactor_file(path: Path) -> List[str]:
    """
    Lit un fichier d'interacteurs au format :
      1: nom du virus
      2-5: éventuellement metadata/commentaires
      6+: "<protein_id>	<gene_symbol>"
    Retourne la liste de gene_symbol.
    """
    genes: List[str] = []
    with path.open() as f:
        lines = f.readlines()
    for line in lines[1:]:
        if line.startswith("#") or not line.strip():
            continue
        parts = line.strip().split("	")
        if len(parts) < 2:
            continue
        gene = parts[1]
        genes.append(gene)
    return genes


def build_gene_lists_from_interactors(
    range_mode: str = "direct_and_second",
) -> Path:
    """
    Construit un fichier genes_list_*.txt au format attendu par les scripts R d'analyse en aval.
    range_mode peut être :
      - "direct"                  → direct_interactors.txt
      - "only_second"             → only_second_range_interactors.txt
      - "direct_and_second"       → direct_interactors + only_second_range
      - "orders_1_to_3"           → union des ordres 1,2,3
      - etc.
    """
    pattern_by_mode = {
        "direct": ["direct_interactors.txt"],
        "only_second": ["only_second_range_interactors.txt"],
        "only_third": ["only_Third_range_interactors.txt"],
        "only_fourth": ["only_Fourth_range_interactors.txt"],
        "orders_1_to_3": [
            "direct_interactors.txt",
            "only_second_range_interactors.txt",
            "only_Third_range_interactors.txt",
        ],
        "direct_and_second": [
            "direct_interactors.txt",
            "only_second_range_interactors.txt",
        ],
    }

    if range_mode not in pattern_by_mode:
        raise ValueError(f"Unknown range_mode: {range_mode}")

    logger.info("Building gene list for range_mode=%s", range_mode)

    gene_lists: Dict[str, List[str]] = {}

    for virus_dir in list_virus_dirs(INTERACTORS_DIR):
        virus_name = virus_dir.name
        genes_set: set[str] = set()
        for filename in pattern_by_mode[range_mode]:
            fpath = virus_dir / filename
            if not fpath.exists():
                continue
            genes = _read_interactor_file(fpath)
            genes_set.update(genes)
        gene_lists[virus_name] = sorted(genes_set)

    out_path = GENE_LISTS_DIR / f"genes_list_{range_mode}.txt"
    with out_path.open("w", encoding="utf-8") as f:
        for virus, genes in sorted(gene_lists.items()):
            if not genes:
                continue
            line = "	".join([virus] + genes)
            f.write(line + "
")

    logger.info("Wrote gene list file: %s", out_path)
    return out_path


def build_gene_virus_table(range_mode: str = "direct_and_second") -> Path:
    """
    Construit une matrice binaire (genes x virus) pour un range donné,
    à partir des fichiers dans INTERACTORS_DIR.
    """
    pattern_by_mode = {
        "direct": ["direct_interactors.txt"],
        "direct_and_second": [
            "direct_interactors.txt",
            "only_second_range_interactors.txt",
        ],
        "only_second": ["only_second_range_interactors.txt"],
        "only_third": ["only_Third_range_interactors.txt"],
    }

    if range_mode not in pattern_by_mode:
        raise ValueError(f"Unknown range_mode: {range_mode}")

    virus_to_genes: Dict[str, set[str]] = {}
    all_genes: set[str] = set()

    for virus_dir in list_virus_dirs(INTERACTORS_DIR):
        virus_name = virus_dir.name
        genes_set: set[str] = set()
        for filename in pattern_by_mode[range_mode]:
            fpath = virus_dir / filename
            if fpath.exists():
                genes_set.update(_read_interactor_file(fpath))
        if genes_set:
            virus_to_genes[virus_name] = genes_set
            all_genes.update(genes_set)

    df = pd.DataFrame(
        0,
        index=sorted(all_genes),
        columns=sorted(virus_to_genes.keys()),
        dtype=int,
    )
    for virus, genes in virus_to_genes.items():
        df.loc[list(genes), virus] = 1

    out_path = TABLES_DIR / f"gene_virus_table_{range_mode}.tsv"
    df.to_csv(out_path, sep="	")
    logger.info("Wrote gene-virus table: %s", out_path)
    return out_path

./CV/04_Postdoc_INSERM/07_BarcodesDrugScreening/notebooks/01_visualize_barcode_signatures.py
#!/usr/bin/env python
# coding: utf-8

# # 01 – Visualize barcode drug signatures
# 
# Notebook for downstream visualization of the barcode-based drug screen.
# 
# Input:
# - `merged_logfc_pval_filtered_deseq2_2023_fillna.csv`: matrix of log2 fold-changes (one DESeq2 result per condition) already filtered on p-value and with missing values imputed.
# - `colnames_annotated_2023.csv`: annotation of conditions (drug, class, etc.).
# 
# Outputs (saved in the `figures/` folder):
# - Heatmap of barcode signatures (barcodes × conditions).
# - Clustered correlation heatmap of conditions.
# - Drug similarity network (conditions as nodes, correlation-based edges).
# 

# In[31]:


import os
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from scipy.spatial.distance import squareform
from scipy.cluster.hierarchy import linkage

sns.set(context="notebook", style="white")
plt.rcParams["figure.dpi"] = 120

# Ensure output directory exists
FIG_DIR = Path("figures")
FIG_DIR.mkdir(parents=True, exist_ok=True)


# ## 1. Config – paths and parameters
# 
# Adjust paths below if needed. They are written assuming:
# - this notebook lives in `scripts/2023/` (or similar),
# - data files live in `../data/` relative to the notebook.
# 

# In[32]:


# ---- Paths (edit if needed) ----
DATA_DIR = Path("../data")
SCRIPT_DIR = Path(".")

LOGFC_FILE = Path("./merged_logfc_pval_filtered_deseq2_2023_fillna.csv")
ANNOT_FILE = Path("./colnames_annotated_2023.csv")  # condition annotations

# ---- Core parameters ----
# Correlation threshold for network edges
CORR_THRESHOLD = 0.8

# Number of top variable barcodes to plot in Fig.3 (None = all)
# You can set this to e.g. 3000 if the full matrix is too heavy to display.
N_TOP_VARIABLE_BARCODES = None  # or an int

print("LOGFC_FILE:", LOGFC_FILE)
print("ANNOT_FILE:", ANNOT_FILE)
print("Figures will be saved to:", FIG_DIR.resolve())


# ## 2. Load log2FC matrix and annotation
# 
# The log2FC file is expected to have:
# - rows: barcodes (or genomic features),
# - columns: conditions (CtrlMs_000u, Temps0_000u, drugs, doses, experiments, etc.),
# - all values: numeric log2 fold changes (after p-value filtering and imputation).
# 
# The annotation file is expected to contain per-condition information (e.g. drug class).
# 

# In[39]:


# Load log2FC matrix
logfc_df = pd.read_csv(LOGFC_FILE, sep=";", header=0, index_col=0)
print("Original logFC shape (rows × cols):", logfc_df.shape)

# Ensure numeric, coerce non-numeric to NaN then fill with 0 (should already be numeric & imputed)
logfc_df = logfc_df.apply(pd.to_numeric, errors="coerce")
logfc_df = logfc_df.fillna(0.0)

# Sort columns case-insensitively, like in your correlation script
logfc_df = logfc_df.reindex(sorted(logfc_df.columns, key=lambda x: x.lower()), axis=1)

print("LogFC matrix after cleaning:")
display(logfc_df.iloc[:5, :8])

# Load annotation of conditions (optional but highly recommended for nice plots)
if ANNOT_FILE.exists():
    annot_df = pd.read_csv(ANNOT_FILE, sep=";")
    # First column is condition name
    annot_df = annot_df.set_index(annot_df.columns[0])
    print("Annotation columns:", list(annot_df.columns))
    # Reindex to match conditions in logfc_df (safe subset)
    annot_df = annot_df.reindex(logfc_df.columns)
else:
    annot_df = None
    print("Warning: annotation file not found – plots will be unannotated.")


# ## 3. Optional: restrict to top variable barcodes
# 
# To avoid a gigantic heatmap, you can restrict to the most variable barcodes across conditions.
# If `N_TOP_VARIABLE_BARCODES` is `None`, all barcodes are used.
# 

# In[40]:


if N_TOP_VARIABLE_BARCODES is not None and N_TOP_VARIABLE_BARCODES < logfc_df.shape[0]:
    variances = logfc_df.var(axis=1)
    top_idx = variances.sort_values(ascending=False).head(N_TOP_VARIABLE_BARCODES).index
    logfc_sub = logfc_df.loc[top_idx].copy()
    print(f"Using top {N_TOP_VARIABLE_BARCODES} most variable barcodes out of {logfc_df.shape[0]}.")
else:
    logfc_sub = logfc_df.copy()
    print("Using all barcodes:", logfc_sub.shape[0])


# ## 4. Fig.3 – Heatmap of barcode signatures
# 
# We reproduce the idea of your Fig.3:
# - rows: barcodes,
# - columns: conditions,
# - values: column-wise normalized log2FC (z-score per condition),
# - optional column annotation (drug classes, MOA, etc.).
# 

# In[43]:


# Column-wise z-score (normalize each condition)
z_logfc = logfc_sub.copy()
z_logfc = (z_logfc - z_logfc.mean(axis=0)) / (z_logfc.std(axis=0, ddof=0) + 1e-9)

print("Z-scored logFC shape:", z_logfc.shape)

# Build annotation for seaborn, if available
col_colors = None
if annot_df is not None:
    # Choose one annotation column for coloring (modify if needed)
    # Example: a column named 'Class' or 'Mechanism' or similar.
    # If several exist, you can change this variable.
    annot_col_name = None
    for candidate in ["colname"]:
        if candidate in annot_df.columns:
            annot_col_name = candidate
            break

    if annot_col_name is not None:
        cond_to_class = annot_df[annot_col_name].astype(str)
        unique_classes = cond_to_class.unique()
        palette = sns.color_palette("tab20", n_colors=len(unique_classes))
        class_to_color = {c: palette[i] for i, c in enumerate(unique_classes)}
        col_colors = cond_to_class.map(class_to_color)
        print("Using annotation column for colors:", annot_col_name)
    else:
        print("No suitable annotation column found for color mapping.")

# Plot heatmap (rows not clustered, columns clustered or not as you prefer)
plt.figure(figsize=(14, 8))
sns.heatmap(
    z_logfc,
    cmap="RdBu_r",
    center=0,
    cbar_kws={"label": "normalized log2FC (z-score)"},
    xticklabels=False,
    yticklabels=False
)
plt.title("Fig.3 – Barcode signatures (z-scored log2FC)")
plt.tight_layout()
plt.savefig(FIG_DIR / "fig3_barcode_signatures_heatmap.png", dpi=300, bbox_inches="tight")
plt.savefig(FIG_DIR / "fig3_barcode_signatures_heatmap.pdf", bbox_inches="tight")
plt.show()


# ## 5. Fig.4 – Drug–drug correlation matrix
# 
# We compute the Pearson correlation between conditions using the log2FC values and plot a clustered heatmap.
# 
# - each column = one condition (drug/dose/experiment),
# - each cell = correlation between two conditions,
# - clustering on rows/columns gives the 2D drug map.
# 

# In[44]:


# Correlation between conditions (columns)
corr_mat = logfc_df.corr(method="pearson", min_periods=1)
print("Correlation matrix shape:", corr_mat.shape)

# Clustered heatmap of correlations
g = sns.clustermap(
    corr_mat,
    cmap="RdBu_r",
    vmin=-1,
    vmax=1,
    center=0,
    figsize=(12, 12),
    xticklabels=False,
    yticklabels=False,
    cbar_kws={"label": "Pearson correlation"},
    method="complete"
)
g.fig.suptitle("Fig.4 – Drug–drug correlation clustered heatmap", y=1.02)
plt.savefig(FIG_DIR / "fig4_drug_drug_correlation_clustered.png", dpi=300, bbox_inches="tight")
plt.savefig(FIG_DIR / "fig4_drug_drug_correlation_clustered.pdf", bbox_inches="tight")
plt.show()


# ## 6. Fig.5 – Drug network (correlation ≥ threshold)
# 
# We build an undirected graph:
# - nodes = conditions,
# - edge between two nodes if their correlation ≥ `CORR_THRESHOLD` (default 0.8),
# - node color = drug class (if annotation available),
# - layout = Fruchterman–Reingold (spring layout), like in your original description.
# 

# In[45]:


# Build graph from correlation matrix
G = nx.Graph()

# Add nodes with optional class attribute
for cond in corr_mat.columns:
    node_attrs = {}
    if annot_df is not None:
        # copy entire annotation row as attributes
        if cond in annot_df.index:
            for col in annot_df.columns:
                node_attrs[col] = annot_df.loc[cond, col]
    G.add_node(cond, **node_attrs)

# Add edges based on correlation threshold
for i, cond_i in enumerate(corr_mat.columns):
    for j, cond_j in enumerate(corr_mat.columns):
        if j <= i:
            continue
        r = corr_mat.loc[cond_i, cond_j]
        if r >= CORR_THRESHOLD:
            G.add_edge(cond_i, cond_j, weight=float(r))

print("Number of nodes:", G.number_of_nodes())
print("Number of edges (|r| >=", CORR_THRESHOLD, "):", G.number_of_edges())

# Choose node colors from a class attribute if available
node_classes = None
if annot_df is not None:
    for candidate in ["Class", "class", "DrugClass", "drug_class", "MOA", "moa"]:
        if candidate in annot_df.columns:
            node_classes = candidate
            break

if node_classes is not None:
    class_values = []
    for n in G.nodes:
        if n in annot_df.index:
            class_values.append(str(annot_df.loc[n, node_classes]))
        else:
            class_values.append("NA")
    unique_classes = sorted(set(class_values))
    palette = sns.color_palette("tab20", n_colors=len(unique_classes))
    class_to_color = {c: palette[i] for i, c in enumerate(unique_classes)}
    node_colors = [class_to_color[c] for c in class_values]
else:
    node_colors = "tab:blue"
    unique_classes = None
    class_to_color = None
    print("No class annotation found for node coloring.")

# Spring layout (Fruchterman–Reingold)
pos = nx.spring_layout(G, seed=42)

plt.figure(figsize=(10, 10))
nx.draw_networkx_edges(G, pos, alpha=0.3, width=0.5)
nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=80, linewidths=0.5, edgecolors="black")
nx.draw_networkx_labels(G, pos, font_size=4)

plt.title(f"Fig.5 – Drug similarity network (r ≥ {CORR_THRESHOLD})")
plt.axis("off")
plt.tight_layout()
plt.savefig(FIG_DIR / "fig5_drug_network.png", dpi=300, bbox_inches="tight")
plt.savefig(FIG_DIR / "fig5_drug_network.pdf", bbox_inches="tight")
plt.show()

# Optional: legend for classes
if unique_classes is not None and class_to_color is not None:
    fig, ax = plt.subplots(figsize=(4, 4))
    for c in unique_classes:
        ax.scatter([], [], color=class_to_color[c], label=c, s=40)
    ax.legend(title=node_classes, frameon=False, fontsize=6)
    ax.set_axis_off()
    plt.tight_layout()
    plt.savefig(FIG_DIR / "fig5_drug_network_legend.png", dpi=300, bbox_inches="tight")
    plt.savefig(FIG_DIR / "fig5_drug_network_legend.pdf", bbox_inches="tight")
    plt.show()


# ## 7. Quick summary
# 
# This notebook:
# - loads `merged_logfc_pval_filtered_deseq2_2023_fillna.csv`,
# - optionally restricts to the most variable barcodes,
# - generates three main figures:
#   1. **Fig.3** – barcode signatures heatmap,
#   2. **Fig.4** – clustered correlation heatmap of conditions,
#   3. **Fig.5** – correlation-based drug similarity network.
# 
# All figures are saved in the `figures/` folder as both `.png` and `.pdf`.
# 

# In[ ]:





# In[ ]:





# In[ ]:

./CV/04_Postdoc_INSERM/07_BarcodesDrugScreening/scripts/01_preprocess_barcodes.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import argparse
import glob
import os
import pandas as pd


def parse_args():
    p = argparse.ArgumentParser(
        description=(
            "Fusionne les fichiers *.csv de comptage barcodes, "
            "filtre les barcodes peu détectés, et produit les matrices filtrées "
            "en counts bruts et normalisés à 1e6 par échantillon."
        )
    )
    p.add_argument(
        "--input_dir",
        default="data/raw",
        help="Dossier contenant les fichiers *.csv",
    )
    p.add_argument(
        "--pattern",
        default="*.csv",
        help="Pattern des fichiers de comptage (par défaut *.csv)",
    )
    p.add_argument(
        "--output_prefix",
        default="data/processed/barcodes",
        help="Préfixe des fichiers de sortie (sans extension)",
    )
    p.add_argument(
        "--sep",
        default=";",
        help="Séparateur des CSV d'entrée (par défaut ;) ",
    )
    p.add_argument(
        "--min_reads",
        type=float,
        default=1.0,
        help="Seuil minimal de reads pour considérer qu'un barcode est détecté dans un échantillon",
    )
    p.add_argument(
        "--min_controls",
        type=int,
        default=5,
        help="Nombre minimal d'échantillons contrôle où le barcode doit être détecté",
    )
    p.add_argument(
        "--min_timezeros",
        type=int,
        default=5,
        help="Nombre minimal d'échantillons Temps0 où le barcode doit être détecté",
    )
    return p.parse_args()


def load_and_merge_runs(input_dir, pattern, sep):
    pattern_path = os.path.join(input_dir, pattern)
    files = sorted(glob.glob(pattern_path))
    if not files:
        raise FileNotFoundError(f"Aucun fichier ne matche {pattern_path}")

    dfs = []
    for f in files:
        print(f"[INFO] Lecture de {f}")
        df = pd.read_csv(f, sep=sep, header=0, index_col=0)
        dfs.append(df)

    # jointure sur les barcodes (index)
    merged = pd.concat(dfs, axis=1)
    merged = merged.fillna(0)
    # s'assurer que c'est bien numérique
    merged = merged.apply(pd.to_numeric, errors="coerce").fillna(0)
    print(f"[INFO] Matrice fusionnée : {merged.shape[0]} barcodes x {merged.shape[1]} échantillons")
    return merged


def filter_barcodes(df_raw, min_reads, min_controls, min_timezeros):
    cols_control = [c for c in df_raw.columns if "Contro" in c]
    cols_time0 = [c for c in df_raw.columns if "Temps" in c]

    if not cols_control:
        print("[WARN] Aucun échantillon contrôle trouvé (cols contenant 'Contro').")
    if not cols_time0:
        print("[WARN] Aucun échantillon Temps0 trouvé (cols contenant 'Temps').")

    print(f"[INFO] # contrôles : {len(cols_control)}, # temps0 : {len(cols_time0)}")

    detected_controls = (df_raw[cols_control] >= min_reads).sum(axis=1) if cols_control else 0
    detected_time0 = (df_raw[cols_time0] >= min_reads).sum(axis=1) if cols_time0 else 0

    mask = (detected_controls >= min_controls) & (detected_time0 >= min_timezeros)
    df_filt = df_raw.loc[mask].copy()

    print(f"[INFO] Après filtrage : {df_filt.shape[0]} barcodes conservés.")
    return df_filt


def normalize_cpm(df_counts):
    # normalisation par colonne (échantillon)
    lib_sizes = df_counts.sum(axis=0)
    cpm = df_counts.div(lib_sizes, axis=1) * 1_000_000
    return cpm


def main():
    args = parse_args()

    os.makedirs(os.path.dirname(args.output_prefix), exist_ok=True)

    df_raw = load_and_merge_runs(args.input_dir, args.pattern, args.sep)
    out_raw = f"{args.output_prefix}_combined_raw_counts.csv"
    df_raw.to_csv(out_raw, sep=";")
    print(f"[INFO] Sauvegardé : {out_raw}")

    df_filt = filter_barcodes(
        df_raw,
        min_reads=args.min_reads,
        min_controls=args.min_controls,
        min_timezeros=args.min_timezeros,
    )

    out_filt = f"{args.output_prefix}_filtered_counts.csv"
    df_filt.to_csv(out_filt, sep=";")
    print(f"[INFO] Sauvegardé : {out_filt}")

    df_cpm = normalize_cpm(df_filt)
    out_cpm = f"{args.output_prefix}_filtered_cpm.csv"
    df_cpm.to_csv(out_cpm, sep=";")
    print(f"[INFO] Sauvegardé : {out_cpm}")

    print("[DONE] 01_preprocess_barcodes terminé.")


if __name__ == "__main__":
    main()

./CV/04_Postdoc_INSERM/07_BarcodesDrugScreening/scripts/02_qc_controls_variability.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import argparse
import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


def parse_args():
    p = argparse.ArgumentParser(
        description=(
            "Calcule la variabilité des barcodes dans les contrôles par expérience, "
            "et génère un TSV + un violonplot global."
        )
    )
    p.add_argument(
        "--counts",
        default="data/processed/barcodes_filtered_counts.csv",
        help="Matrice de counts filtrée (sortie de 01_preprocess_barcodes.py)",
    )
    p.add_argument(
        "--output_prefix",
        default="results/qc_controls/controls_variability",
        help="Préfixe des fichiers de sortie",
    )
    p.add_argument(
        "--sep",
        default=";",
        help="Séparateur du fichier de counts (par défaut ;) ",
    )
    return p.parse_args()


def extract_experiment(colname):
    parts = colname.split("_")
    if len(parts) >= 3:
        return parts[2]
    return "unknown"


def compute_controls_stats(df):
    records = []
    for col in df.columns:
        if "Contro" not in col:
            continue
        exp = extract_experiment(col)
        # on groupera par exp + barcode ensuite
    # On re-boucle de manière structurée
    experiments = sorted({extract_experiment(c) for c in df.columns if "Contro" in c})

    for exp in experiments:
        ctrl_cols = [c for c in df.columns
                    if ("Contro" in c) and (extract_experiment(c) == exp)]
        if not ctrl_cols:
            continue
        sub = df[ctrl_cols]

        for barcode, row in sub.iterrows():
            values = row.values.astype(float)
            avg = float(np.mean(values))
            std = float(np.std(values, ddof=1)) if len(values) > 1 else 0.0
            if avg > 0:
                max_min_ratio = float((values.max() - values.min()) / avg)
            else:
                max_min_ratio = 0.0
            records.append(
                {
                    "experiment": exp,
                    "barcode": barcode,
                    "n_controls": len(values),
                    "mean_reads": avg,
                    "std_reads": std,
                    "max_min_over_mean": max_min_ratio,
                }
            )

    df_stats = pd.DataFrame.from_records(records)
    return df_stats


def plot_violin(df_stats, output_png):
    sns.set(style="whitegrid")
    plt.figure(figsize=(8, 4))
    ax = sns.violinplot(x="max_min_over_mean", data=df_stats, bw=0.05)
    ax.set_xlabel("(max - min) / moyenne des reads (contrôles)")
    ax.set_title("Variabilité des barcodes dans les contrôles (tous expériences confondues)")
    plt.tight_layout()
    plt.savefig(output_png, dpi=300)
    plt.close()
    print(f"[INFO] Violonplot sauvegardé : {output_png}")


def main():
    args = parse_args()
    os.makedirs(os.path.dirname(args.output_prefix), exist_ok=True)

    df = pd.read_csv(args.counts, sep=args.sep, header=0, index_col=0)
    print(f"[INFO] Matrice counts : {df.shape[0]} barcodes x {df.shape[1]} échantillons")

    df_stats = compute_controls_stats(df)

    out_tsv = f"{args.output_prefix}_per_barcode.tsv"
    df_stats.to_csv(out_tsv, sep="	", index=False)
    print(f"[INFO] Stats sauvegardées : {out_tsv}")

    out_png = f"{args.output_prefix}_violin.png"
    plot_violin(df_stats, out_png)

    print("[DONE] 02_qc_controls_variability terminé.")


if __name__ == "__main__":
    main()

./CV/04_Postdoc_INSERM/07_BarcodesDrugScreening/scripts/03_build_deseq2_inputs.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import argparse
import os
import pandas as pd


def parse_args():
    p = argparse.ArgumentParser(
        description=(
            "Construit les fichiers d'entrée pour DESeq2 : "
            "matrice de counts filtrée + table de design."
        )
    )
    p.add_argument(
        "--counts",
        default="data/processed/barcodes_filtered_counts.csv",
        help="Matrice de counts filtrée (sortie de 01_preprocess_barcodes.py)",
    )
    p.add_argument(
        "--output_dir",
        default="results/deseq2_inputs",
        help="Dossier de sortie pour counts + design",
    )
    p.add_argument(
        "--sep",
        default=";",
        help="Séparateur du fichier de counts (par défaut ;) ",
    )
    return p.parse_args()


def parse_sample_name(sample):
    """
    Suppose des noms du type:
    DrugX1_006u_exp200921_run1_xxx
    Contro1_000u_exp200921_run1_xxx
    Temps01_000u_exp200921_run1_xxx
    Adapte ce parsing si besoin.
    """
    parts = sample.split("_")
    drug_repl = parts[0] if len(parts) > 0 else ""
    dose = parts[1] if len(parts) > 1 else ""
    exp = parts[2] if len(parts) > 2 else ""
    run = parts[3] if len(parts) > 3 else ""

    replicate = drug_repl[-1] if len(drug_repl) > 0 else ""
    drug = drug_repl[:-1] if len(drug_repl) > 1 else drug_repl

    is_control = "Contro" in drug_repl
    is_timezero = ("Temps0" in sample) or ("Temps" in sample)

    if is_control:
        condition = "control"
    elif is_timezero:
        condition = "time0"
    else:
        condition = f"{drug}_{dose}"

    return {
        "sample": sample,
        "drug_raw": drug_repl,
        "drug": drug,
        "dose": dose,
        "exp": exp,
        "run": run,
        "replicate": replicate,
        "condition": condition,
        "is_control": is_control,
        "is_timezero": is_timezero,
    }


def main():
    args = parse_args()
    os.makedirs(args.output_dir, exist_ok=True)

    df_counts = pd.read_csv(args.counts, sep=args.sep, header=0, index_col=0)
    print(f"[INFO] Matrice counts filtrée : {df_counts.shape}")

    # On réécrit une version TSV pour DESeq2
    counts_path = os.path.join(args.output_dir, "counts_for_deseq2.tsv")
    df_counts.to_csv(counts_path, sep="	")
    print(f"[INFO] Counts DESeq2 : {counts_path}")

    # Table de design
    meta_records = []
    for sample in df_counts.columns:
        meta_records.append(parse_sample_name(sample))

    df_design = pd.DataFrame.from_records(meta_records)
    design_path = os.path.join(args.output_dir, "design_for_deseq2.tsv")
    df_design.to_csv(design_path, sep="	", index=False)
    print(f"[INFO] Design DESeq2 : {design_path}")

    print("[DONE] 03_build_deseq2_inputs terminé.")


if __name__ == "__main__":
    main()

./CV/04_Postdoc_INSERM/07_BarcodesDrugScreening/scripts/04_correlations_and_networks.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import argparse
import glob
import os

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import networkx as nx


def parse_args():
    p = argparse.ArgumentParser(
        description=(
            "Fusionne les résultats DESeq2 (log2FC) en une matrice, "
            "calcule une matrice de corrélation condition-condition, "
            "et génère une heatmap + un réseau."
        )
    )
    p.add_argument(
        "--deseq2_dir",
        default="results/deseq2",
        help="Dossier contenant les sorties DESeq2 (fichiers .tsv)",
    )
    p.add_argument(
        "--pattern",
        default="*.tsv",
        help="Pattern des fichiers DESeq2 (par défaut *.tsv)",
    )
    p.add_argument(
        "--output_prefix",
        default="results/networks/drug_signatures",
        help="Préfixe des fichiers de sortie",
    )
    p.add_argument(
        "--corr_threshold",
        type=float,
        default=0.8,
        help="Seuil de corrélation pour dessiner une arête dans le réseau",
    )
    return p.parse_args()


def build_logfc_matrix(deseq2_dir, pattern):
    pattern_path = os.path.join(deseq2_dir, pattern)
    files = sorted(glob.glob(pattern_path))
    if not files:
        raise FileNotFoundError(f"Aucun fichier ne matche {pattern_path}")

    dfs = []
    colnames = []

    for f in files:
        print(f"[INFO] Lecture DESeq2 : {f}")
        # nom de colonne = nom du fichier sans extension
        base = os.path.basename(f)
        cond_name = os.path.splitext(base)[0]

        df = pd.read_csv(f, sep="	", header=0, index_col=0)
        if "log2FoldChange" not in df.columns:
            print(f"[WARN] Pas de colonne 'log2FoldChange' dans {f}, ignoré.")
            continue

        logfc = df["log2FoldChange"]
        dfs.append(logfc)
        colnames.append(cond_name)

    if not dfs:
        raise RuntimeError("Aucun fichier n'a fourni de log2FoldChange exploitable.")

    merged = pd.concat(dfs, axis=1)
    merged.columns = colnames
    merged = merged.astype(float)
    print(f"[INFO] Matrice log2FC : {merged.shape[0]} barcodes x {merged.shape[1]} conditions")
    return merged


def plot_correlation_heatmap(corr, out_png):
    sns.set(style="white")
    plt.figure(figsize=(10, 8))
    g = sns.clustermap(
        corr,
        cmap="RdBu_r",
        vmin=-1,
        vmax=1,
        linewidths=0,
        xticklabels=True,
        yticklabels=True,
    )
    plt.setp(g.ax_heatmap.get_xticklabels(), rotation=90, fontsize=6)
    plt.setp(g.ax_heatmap.get_yticklabels(), fontsize=6)
    g.fig.suptitle("Drug-drug Pearson correlation (log2FC signatures)", y=1.02)
    g.savefig(out_png, dpi=300)
    plt.close()
    print(f"[INFO] Heatmap de corrélation sauvegardée : {out_png}")


def build_and_plot_network(corr, threshold, out_png):
    links = corr.stack().reset_index()
    links.columns = ["source", "target", "correlation"]

    links_filtered = links[
        (links["source"] != links["target"]) & (links["correlation"] >= threshold)
    ]

    print(f"[INFO] # arêtes avec corr >= {threshold} : {links_filtered.shape[0]}")

    G = nx.from_pandas_edgelist(
        links_filtered, "source", "target", edge_attr="correlation"
    )

    plt.figure(figsize=(10, 10))
    pos = nx.spring_layout(G, k=0.2, seed=42)

    # couleur des noeuds en fonction du degré (juste pour avoir un gradient)
    degrees = dict(G.degree())
    deg_vals = np.array(list(degrees.values()), dtype=float)
    if len(deg_vals) > 0:
        deg_norm = (deg_vals - deg_vals.min()) / (deg_vals.ptp() + 1e-9)
    else:
        deg_norm = deg_vals

    node_colors = deg_norm

    nx.draw_networkx_nodes(G, pos, node_size=300, node_color=node_colors, cmap="viridis")
    nx.draw_networkx_edges(G, pos, width=0.5, alpha=0.7)
    nx.draw_networkx_labels(G, pos, font_size=6)

    plt.axis("off")
    plt.title(f"Drug network (corr >= {threshold})", fontsize=12)
    plt.tight_layout()
    plt.savefig(out_png, dpi=300)
    plt.close()
    print(f"[INFO] Réseau sauvegardé : {out_png}")


def main():
    args = parse_args()
    os.makedirs(os.path.dirname(args.output_prefix), exist_ok=True)

    logfc = build_logfc_matrix(args.deseq2_dir, args.pattern)

    out_logfc = f"{args.output_prefix}_logfc_matrix.csv"
    logfc.to_csv(out_logfc, sep=";")
    print(f"[INFO] Matrice log2FC sauvegardée : {out_logfc}")

    corr = logfc.corr(method="pearson", min_periods=1)

    out_corr = f"{args.output_prefix}_correlation_matrix.csv"
    corr.to_csv(out_corr, sep=";")
    print(f"[INFO] Matrice de corrélation sauvegardée : {out_corr}")

    out_heatmap = f"{args.output_prefix}_correlation_heatmap.png"
    plot_correlation_heatmap(corr, out_heatmap)

    out_network = f"{args.output_prefix}_network.png"
    build_and_plot_network(corr, args.corr_threshold, out_network)

    print("[DONE] 04_correlations_and_networks terminé.")


if __name__ == "__main__":
    main()

./CV/04_Postdoc_INSERM/07_BarcodesDrugScreening/scripts/05_generate_figures_for_paper.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Generate paper figures for the barcoding analysis:
- Fig.3: drug clustering based on barcode signatures (log2FC)
- Fig.4: drug–drug correlation clustered heatmap
- Fig.5: drug network (correlation > threshold)
"""

import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import networkx as nx

# -----------------------------
# Configuration
# -----------------------------

DATA_DIR = os.path.join("data")
OUT_DIR = os.path.join("results", "paper_figures")
os.makedirs(OUT_DIR, exist_ok=True)

LOGFC_FILE = os.path.join(DATA_DIR, "merged_pval_filtered_deseq2_fillna.csv")
CORR_FILE  = os.path.join(DATA_DIR, "correl_matrix1_merged_pval_filtered_deseq2.csv")
COLOR_MAP_FILE = os.path.join(DATA_DIR, "color_mapping.tsv")

CORR_THRESHOLD = 0.8  # for the network edges

# -----------------------------
# Helpers
# -----------------------------

def load_logfc_matrix(path: str) -> pd.DataFrame:
    df = pd.read_csv(path, sep=";", header=0, index_col=0)
    # should already be log2FC from DESeq2
    # enforce float + drop all-NaN cols/rows
    df = df.astype(float)
    df = df.dropna(axis=0, how="all").dropna(axis=1, how="all")
    # assert no duplicate cols
    if df.columns.duplicated().any():
        dup = df.columns[df.columns.duplicated()].tolist()
        raise ValueError(f"Duplicate condition columns in logFC matrix: {dup}")
    return df

def load_corr_matrix(logfc_df: pd.DataFrame, corr_file: str | None = None) -> pd.DataFrame:
    if corr_file and os.path.exists(corr_file):
        corr = pd.read_csv(corr_file, sep=";", header=0, index_col=0)
        return corr.astype(float)
    # else compute from logfc
    corr = logfc_df.corr(method="pearson", min_periods=1)
    return corr

def load_color_mapping(path: str) -> dict:
    """
    color_mapping.tsv:
       <int_color_code>	<drug_label>
    Example: 1  Gefitinib_006u
    Returned mapping: drug_name (without replicate) -> color float in [0,1]
    """
    cmap = {}
    if not os.path.exists(path):
        return cmap

    with open(path, "r") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            parts = line.split("	")
            if len(parts) != 2:
                continue
            color_int, drug_label = parts
            color_int = int(color_int)

            # harmonize some special cases as in networks.py
            if drug_label == "Fluor_006u":
                drug_key = "5Fluor_006u"
            elif drug_label == "Azacyt_1,5u":
                drug_key = "Azacyt_1.5u"
            elif drug_label == "Bafilo_1,2n":
                drug_key = "Bafilo_1.2n"
            else:
                drug_key = drug_label

            # scale to [0, 1] for colormap
            cmap[drug_key] = (color_int + 1) / 100.0

    return cmap

def condition_to_drug_label(cond_name: str) -> str:
    """
    Reduce a condition name like 'GefitinibA_006u_exp200921_run1_sample1'
    to a drug label useful for coloring:
      - keep drug + dose, ignore exp/run/sample/replicate
    """
    parts = cond_name.split("_")
    if len(parts) < 2:
        return cond_name

    head = parts[0]
    dose = parts[1]

    # Drop replicate letter for drugs (last char)
    if head.startswith("Ctrl") or "Contro" in head:
        drug = "CtrlMs"
    elif head.startswith("Temps0") or "Temps0" in head:
        drug = "Temps0"
    else:
        # e.g. GefitinibA → Gefitinib
        drug = head[:-1]

    return f"{drug}_{dose}"

# -----------------------------
# Figure 3 – log2FC heatmap (drug signatures)
# -----------------------------

def figure3_drug_signature_heatmap(logfc_df: pd.DataFrame, out_dir: str):
    """
    Clustering of conditions based on barcode log2FC signatures.
    """
    # Optionally, center by barcode (row-wise) to normalize
    # Here: we keep raw log2FC, but clip extreme values for visual clarity.
    clipped = logfc_df.clip(lower=-4, upper=4)

    # Column order = clustered; row order = clustered
    sns.set(style="white")
    g = sns.clustermap(
        clipped,
        method="complete",
        metric="euclidean",
        cmap="RdBu_r",
        center=0.0,
        xticklabels=False,
        yticklabels=False,
        robust=False,
        figsize=(10, 12),
    )
    g.fig.suptitle("Annotated drug clustering based on barcode signatures (log2FC)", y=1.02)

    pdf_path = os.path.join(out_dir, "Fig3_drug_signature_heatmap.pdf")
    png_path = os.path.join(out_dir, "Fig3_drug_signature_heatmap.png")
    g.fig.savefig(pdf_path, bbox_inches="tight")
    g.fig.savefig(png_path, dpi=300, bbox_inches="tight")
    plt.close(g.fig)

# -----------------------------
# Figure 4 – Correlation heatmap
# -----------------------------

def figure4_correlation_heatmap(corr: pd.DataFrame, out_dir: str):
    sns.set(style="white")
    # make sure diagonal is exactly 1
    np.fill_diagonal(corr.values, 1.0)

    g = sns.clustermap(
        corr,
        method="complete",
        metric="euclidean",
        cmap="RdBu_r",
        vmin=-1.0,
        vmax=1.0,
        center=0.0,
        xticklabels=False,
        yticklabels=False,
        figsize=(10, 10),
    )
    g.fig.suptitle("Drug–drug correlation clustered heatmap (Pearson r)", y=1.02)

    pdf_path = os.path.join(out_dir, "Fig4_drug_correlation_heatmap.pdf")
    png_path = os.path.join(out_dir, "Fig4_drug_correlation_heatmap.png")
    g.fig.savefig(pdf_path, bbox_inches="tight")
    g.fig.savefig(png_path, dpi=300, bbox_inches="tight")
    plt.close(g.fig)

# -----------------------------
# Figure 5 – Correlation network
# -----------------------------

def figure5_correlation_network(
    corr: pd.DataFrame,
    color_mapping: dict,
    out_dir: str,
    threshold: float = 0.8,
):
    # Build edge list from upper triangle
    edges = []
    conds = corr.index.tolist()
    for i, s in enumerate(conds):
        for j in range(i + 1, len(conds)):
            t = conds[j]
            r = corr.iloc[i, j]
            if np.isnan(r):
                continue
            if r >= threshold:
                edges.append((s, t, float(r)))

    G = nx.Graph()
    for s, t, r in edges:
        G.add_edge(s, t, weight=r)

    # Node colors based on drug classes
    node_colors = []
    for n in G.nodes():
        drug_label = condition_to_drug_label(n)
        c = color_mapping.get(drug_label, 0.5)  # default mid-grey
        node_colors.append(c)

    # Positions – Fruchterman–Reingold (spring layout)
    pos = nx.spring_layout(G, k=0.15, iterations=200, seed=42)

    plt.figure(figsize=(10, 10))
    nx.draw_networkx_edges(
        G,
        pos,
        edgelist=G.edges(),
        width=0.5,
        alpha=0.6,
        edge_color="black",
    )

    nx.draw_networkx_nodes(
        G,
        pos,
        node_size=300,
        node_color=node_colors,
        cmap="viridis",
        alpha=0.9,
    )

    # Labels: drug only (no exp/run/sample)
    labels = {n: condition_to_drug_label(n).split("_")[0] for n in G.nodes()}
    nx.draw_networkx_labels(G, pos, labels=labels, font_size=6)

    plt.title(f"Drug network (Pearson r ≥ {threshold})")
    plt.axis("off")

    pdf_path = os.path.join(out_dir, "Fig5_drug_network.pdf")
    png_path = os.path.join(out_dir, "Fig5_drug_network.png")
    plt.savefig(pdf_path, bbox_inches="tight")
    plt.savefig(png_path, dpi=300, bbox_inches="tight")
    plt.close()

# -----------------------------
# Main
# -----------------------------

def main():
    print("Loading log2FC matrix…")
    logfc = load_logfc_matrix(LOGFC_FILE)

    print("Loading / computing correlation matrix…")
    corr = load_corr_matrix(logfc, CORR_FILE)

    print("Loading color mapping…")
    cmap = load_color_mapping(COLOR_MAP_FILE)

    print("Generating Fig.3 (heatmap of barcode signatures)…")
    figure3_drug_signature_heatmap(logfc, OUT_DIR)

    print("Generating Fig.4 (correlation clustered heatmap)…")
    figure4_correlation_heatmap(corr, OUT_DIR)

    print("Generating Fig.5 (correlation network)…")
    figure5_correlation_network(corr, cmap, OUT_DIR, threshold=CORR_THRESHOLD)

    print("Done. Figures saved in:", OUT_DIR)

if __name__ == "__main__":
    main()
